{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This experiment was done to make sure that the model Model-1 982K achives 92 %\n",
    "\n",
    "#  It is seen that Model-1 982K params achieves 92.36 with Adam()\n",
    "\n",
    "- 4 Denseblocks with layers [8,6,20,12], growth rate= 12, , compression = 0.5, dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K70hAckqg0EA",
    "outputId": "a0dba4c3-f402-4223-ea89-3be41ca275fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "#!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "756bdmamG6f1",
    "outputId": "2f1c4951-b47b-40a0-ab02-1c97fd7c1a95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight \n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhIk-iu4G6f-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time, pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBoa2F25G6gE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "#l = 40\n",
    "#num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWRR6JyzG6gT"
   },
   "outputs": [],
   "source": [
    "do_sub_sampling_of_input = False\n",
    "do_data_augmentation = True\n",
    "do_data_append       = False   #2X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "16a221e4-1075-49d1-9d76-8091d240f280"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_hYUAbsG6ge"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "class_name = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "00iNYJLkG6gj",
    "outputId": "4911e408-9a52-4009-eefd-44e5242fb219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6], dtype=uint8), 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], img_height, img_width, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS5X5srxG6gp"
   },
   "outputs": [],
   "source": [
    "def draw_img(i, x_train, y_train, class_name):\n",
    "    im = x_train[i]\n",
    "    c = y_train[i]\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"Class %d (%s)\" % (int(c), class_name[int(c)]))\n",
    "    plt.axis('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "mjF2KvIQG6gw",
    "outputId": "6c7a984d-4db5-4d5b-d110-b740edd41a02"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_img(7, x_train, y_train, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to dispaly and analyze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions\n",
    "\n",
    "plot_confusion_matrix(): helps us to plot the confusion matrix. \n",
    "It is taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html. \n",
    "The same page has some sample examples on how to use this function \n",
    "cm: Confusion matrix calcualted using confusion_matrix() from sklearn.metrics <br//> classes: a list of labels for the classes we are plotting\n",
    "normalize=False: True means we will plot nomalized values \n",
    "title='Confusion matrix': set the tiltle of the plot \n",
    "cmap : leave it as it is \n",
    "Example Usage:plot_confusion_matrix(cm, classes=Facial_Expressions, normalize=True, title='Test Data - Using Simple Average Ensembling ')\n",
    "\n",
    "plot_histogram(): helps to plot the histogram of a list \n",
    "lst_data: the list whose histogtam we want to plot , \n",
    "class_labels: a list of labels for the classes we are plotting \n",
    "ylabel='None': set the y label of the plot, x label is always frequency \n",
    "title='None': set the tiltle of the plot -lst_data, class_labels, ylabel='None', title='None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm) #to print in text if needed\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_histogram(lst_data, class_labels, ylabel='None', title='None'):\n",
    "    data = pd.Series(lst_data)\n",
    "    distribution = data.value_counts(sort=False)\n",
    "    y_pos = np.arange(len(class_labels))\n",
    "    \n",
    "    plt.bar(y_pos, distribution, align='center', alpha=0.8)\n",
    "    plt.xticks(y_pos, class_labels)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = ['airplane', 'automobile', 'bird','cat', 'deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(list(y_train), y_classes, ylabel='Frequency',title='CIFAR 10 Class Distribution in Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(list(y_test), y_classes, ylabel='Frequency',title='CIFAR 10 Class Distribution in Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmdwlu8pG6g3"
   },
   "outputs": [],
   "source": [
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JGd4ezgG6g7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "z_arXgOSG6hA",
    "outputId": "8960acf7-335b-4c96-9ae9-7fe9b9ad56fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  62,  63],\n",
       "       [ 43,  46,  45],\n",
       "       [ 50,  48,  43],\n",
       "       [ 68,  54,  42],\n",
       "       [ 98,  73,  52],\n",
       "       [119,  91,  63],\n",
       "       [139, 107,  75],\n",
       "       [145, 110,  80],\n",
       "       [149, 117,  89],\n",
       "       [149, 120,  93],\n",
       "       [131, 103,  77],\n",
       "       [125,  99,  76],\n",
       "       [142, 115,  91],\n",
       "       [144, 112,  86],\n",
       "       [137, 105,  79],\n",
       "       [129,  97,  71],\n",
       "       [137, 106,  79],\n",
       "       [134, 106,  76],\n",
       "       [124,  97,  64],\n",
       "       [139, 113,  78],\n",
       "       [139, 112,  75],\n",
       "       [133, 105,  69],\n",
       "       [136, 105,  74],\n",
       "       [139, 108,  77],\n",
       "       [152, 120,  89],\n",
       "       [163, 131, 100],\n",
       "       [168, 136, 108],\n",
       "       [159, 129, 102],\n",
       "       [158, 130, 104],\n",
       "       [158, 132, 108],\n",
       "       [152, 125, 102],\n",
       "       [148, 124, 103]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train[0][0:32][0:32][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "jd2usuXDG6hH",
    "outputId": "a0716d82-94e5-4f1b-ba28-39b1cdced01b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "T372AU6RG6hN",
    "outputId": "4739d668-5818-4f70-e6d3-3491edcb274c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Mean :  [[[[125.3069  122.95015 113.866  ]]]]\n",
      "Channel Std :  [[[[62.99325  62.088604 66.70501 ]]]]\n",
      "Channel Mean1 :  [[[[126.02428 123.70843 114.85442]]]]\n",
      "Channel Std1 :  [[[[62.896416 61.937508 66.70607 ]]]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "mean  = np.mean(x_train, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "std   = np.std(x_train, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "\n",
    "mean1  = np.mean(x_test, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "std1   = np.std(x_test, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "\n",
    "\n",
    "print(\"Channel Mean : \", mean)\n",
    "print(\"Channel Std : \", std)\n",
    "print(\"Channel Mean1 : \", mean1)\n",
    "print(\"Channel Std1 : \", std1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = (x_train - mean) / (std)\n",
    "x_test  = (x_test - mean1) / (std1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOCMAYM1G6hT"
   },
   "outputs": [],
   "source": [
    "#x_train = (x_train - 127.5)/255.0\n",
    "#x_test  = (x_test  - 127.5)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZF74U6zcG6hX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.0526032e+00, -9.8166406e-01, -7.6255137e-01],\n",
       "        [-1.3065987e+00, -1.2393603e+00, -1.0323962e+00],\n",
       "        [-1.1954757e+00, -1.2071482e+00, -1.0623789e+00],\n",
       "        ...,\n",
       "        [ 5.1899368e-01,  1.4575703e-01, -8.7939382e-02],\n",
       "        [ 4.2374539e-01,  3.3014923e-02, -1.7788765e-01],\n",
       "        [ 3.6024651e-01,  1.6908908e-02, -1.6289628e-01]],\n",
       "\n",
       "       [[-1.7352160e+00, -1.6581167e+00, -1.4071807e+00],\n",
       "        [-1.9892114e+00, -1.9802370e+00, -1.7070082e+00],\n",
       "        [-1.7034665e+00, -1.8513888e+00, -1.7070082e+00],\n",
       "        ...,\n",
       "        [-3.6621384e-02, -5.6290764e-01, -8.8248241e-01],\n",
       "        [-1.0012025e-01, -6.4343774e-01, -9.5743930e-01],\n",
       "        [-5.2496098e-02, -5.7901365e-01, -8.5249966e-01]],\n",
       "\n",
       "       [[-1.5923436e+00, -1.5936927e+00, -1.3921893e+00],\n",
       "        [-1.7352160e+00, -1.8674948e+00, -1.7070082e+00],\n",
       "        [-1.2113504e+00, -1.5453745e+00, -1.5870771e+00],\n",
       "        ...,\n",
       "        [-1.1599497e-01, -6.2733167e-01, -9.5743930e-01],\n",
       "        [-8.4245533e-02, -6.2733167e-01, -9.5743930e-01],\n",
       "        [-2.5886741e-01, -8.0449790e-01, -1.0773703e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.3127295e+00,  7.5778562e-01, -2.6783592e-01],\n",
       "        [ 1.2016065e+00,  4.8398334e-01, -1.1973014e+00],\n",
       "        [ 1.1539823e+00,  6.1283147e-01, -1.3172324e+00],\n",
       "        ...,\n",
       "        [ 5.5074310e-01,  1.6186304e-01, -6.5761173e-01],\n",
       "        [-1.1002274e+00, -1.4809505e+00, -1.6020685e+00],\n",
       "        [-1.1478515e+00, -1.4326324e+00, -1.4071807e+00]],\n",
       "\n",
       "       [[ 8.6823744e-01,  2.5849915e-01, -2.6783592e-01],\n",
       "        [ 7.5711441e-01,  8.0289232e-04, -1.0773703e+00],\n",
       "        [ 9.6348572e-01,  3.3902922e-01, -1.2572669e+00],\n",
       "        ...,\n",
       "        [ 9.3173629e-01,  4.0345326e-01, -2.9781866e-01],\n",
       "        [-4.4936401e-01, -9.8166406e-01, -1.1973014e+00],\n",
       "        [-6.7161006e-01, -1.1266181e+00, -1.1973014e+00]],\n",
       "\n",
       "       [[ 8.2061332e-01,  3.3902922e-01,  3.1991642e-02],\n",
       "        [ 6.7774087e-01,  9.7438984e-02, -2.9781866e-01],\n",
       "        [ 8.5236275e-01,  3.0681717e-01, -4.0275833e-01],\n",
       "        ...,\n",
       "        [ 1.4397272e+00,  9.8326981e-01,  3.9178470e-01],\n",
       "        [ 4.0787068e-01, -7.9727180e-02, -4.4773245e-01],\n",
       "        [-3.6621384e-02, -4.9848357e-01, -6.2762898e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "acEl5EjAG6hd",
    "outputId": "9efce3d3-3074-4f2e-b660-4c6b2f0caf20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XuGiXp6rG6hk",
    "outputId": "acc28bb5-470e-48c3-8e6c-354db61c19f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb0EH5H_G6hq"
   },
   "outputs": [],
   "source": [
    "if do_sub_sampling_of_input:\n",
    "    x_, x_train, x_, y_train    = cross_validation.train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
    "    x_, x_test,  y_, y_test    = cross_validation.train_test_split(x_test, y_test, test_size=0.25, random_state=0)\n",
    "    print(\"After SubSampling\")\n",
    "    print(x_train.shape, x_test.shape)\n",
    "    print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jz7L_trG6hv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if do_data_append :\n",
    "    print(\"Doing Data Appending\")\n",
    "    x_train = np.append(x_train, x_train,axis=0)\n",
    "    y_train = np.append(y_train, y_train,axis=0)\n",
    "#print(np.append(x_train, x_train,axis=0).shape)\n",
    "#print(np.append(y_train, y_train,axis=0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MkHX4hNLG6h0",
    "outputId": "25cf1428-1767-461c-ced2-577e40a2dec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "(50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMuvM67DG6h8"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npxonLr5G6h-"
   },
   "outputs": [],
   "source": [
    "keras.utils.Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,    # randomly flip images\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Tk4q4iYG6iD"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SeparableConv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction densenet -BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, growth_rate, dropout_rate = 0.2, l = 0):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        #Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        #Conv2D_1_1 = Conv2D(int(num_filter*4*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "        Conv2D_1_1 = Conv2D(int(growth_rate*4), (1,1), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_1_1 = Dropout(dropout_rate)(Conv2D_1_1)\n",
    "        BatchNorm_1_1 = BatchNormalization()(Conv2D_1_1)\n",
    "        relu_1_1 = Activation('relu')(BatchNorm_1_1)\n",
    "        \n",
    "        Conv2D_3_3 = Conv2D(int(growth_rate), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        #Conv2D_3_3 = SeparableConv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eEu8gikG6iP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    num_channels = int(input.shape[-1]) #assuming it is tensor\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_channels*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    \n",
    "    #Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    #flat = Dropout(0.25)(flat)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbzC-GOZG6ie"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "#num_filter = 12\n",
    "num_filter = 12\n",
    "growth_rate = 12\n",
    "dropout_rate = 0.2\n",
    "#compression = 0.751\n",
    "compression = 0.5\n",
    "l = 16\n",
    "dense_l= [8, 16, 20, 12]\n",
    "\n",
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(2*num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, growth_rate, dropout_rate, dense_l[0])\n",
    "First_Transition = add_transition(First_Block, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, growth_rate, dropout_rate, dense_l[1])\n",
    "Second_Transition = add_transition(Second_Block, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate, dense_l[2])\n",
    "Third_Transition = add_transition(Third_Block, dropout_rate)\n",
    "\n",
    "Last_Block = add_denseblock(Third_Transition,  growth_rate, dropout_rate, dense_l[3])\n",
    "output = output_layer(Last_Block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing and Verifying the Densenet-BC configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "p9uA_iR8G6io",
    "outputId": "87a7b724-7e86-416d-e7e9-6526b4bc860b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(?, 32, 32, 24)\n",
      "(?, 32, 32, 120) 120\n",
      "(?, 16, 16, 60)\n",
      "(?, 16, 16, 252)\n",
      "(?, 8, 8, 126)\n",
      "(?, 8, 8, 366)\n",
      "(?, 4, 4, 183)\n",
      "(?, 4, 4, 327)\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "print(First_Conv2D.shape)\n",
    "print(First_Block.shape, First_Block.shape[-1])\n",
    "print(First_Transition.shape)\n",
    "\n",
    "print(Second_Block.shape)\n",
    "print(Second_Transition.shape)\n",
    "\n",
    "print(Third_Block.shape)\n",
    "print(Third_Transition.shape)\n",
    "\n",
    "print(Last_Block.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 19618
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "7bc989ab-6517-466c-8120-25c63379a4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_396 (Conv2D)             (None, 32, 32, 24)   648         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 32, 32, 24)   96          conv2d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 32, 32, 24)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_397 (Conv2D)             (None, 32, 32, 48)   1152        activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_392 (Dropout)           (None, 32, 32, 48)   0           conv2d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 32, 32, 48)   192         dropout_392[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 32, 32, 48)   0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_398 (Conv2D)             (None, 32, 32, 12)   5184        activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_393 (Dropout)           (None, 32, 32, 12)   0           conv2d_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_191 (Concatenate)   (None, 32, 32, 36)   0           conv2d_396[0][0]                 \n",
      "                                                                 dropout_393[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 32, 32, 36)   144         concatenate_191[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 32, 32, 36)   0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_399 (Conv2D)             (None, 32, 32, 48)   1728        activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_394 (Dropout)           (None, 32, 32, 48)   0           conv2d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 32, 32, 48)   192         dropout_394[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 32, 32, 48)   0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_400 (Conv2D)             (None, 32, 32, 12)   5184        activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_395 (Dropout)           (None, 32, 32, 12)   0           conv2d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_192 (Concatenate)   (None, 32, 32, 48)   0           concatenate_191[0][0]            \n",
      "                                                                 dropout_395[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 32, 32, 48)   192         concatenate_192[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 32, 32, 48)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_401 (Conv2D)             (None, 32, 32, 48)   2304        activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_396 (Dropout)           (None, 32, 32, 48)   0           conv2d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 32, 32, 48)   192         dropout_396[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 32, 32, 48)   0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_402 (Conv2D)             (None, 32, 32, 12)   5184        activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_397 (Dropout)           (None, 32, 32, 12)   0           conv2d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_193 (Concatenate)   (None, 32, 32, 60)   0           concatenate_192[0][0]            \n",
      "                                                                 dropout_397[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 32, 32, 60)   240         concatenate_193[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 32, 32, 60)   0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_403 (Conv2D)             (None, 32, 32, 48)   2880        activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_398 (Dropout)           (None, 32, 32, 48)   0           conv2d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 32, 32, 48)   192         dropout_398[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 32, 32, 48)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_404 (Conv2D)             (None, 32, 32, 12)   5184        activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_399 (Dropout)           (None, 32, 32, 12)   0           conv2d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_194 (Concatenate)   (None, 32, 32, 72)   0           concatenate_193[0][0]            \n",
      "                                                                 dropout_399[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 32, 32, 72)   288         concatenate_194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 32, 32, 72)   0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 32, 32, 48)   3456        activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_400 (Dropout)           (None, 32, 32, 48)   0           conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 32, 32, 48)   192         dropout_400[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 32, 32, 48)   0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 32, 32, 12)   5184        activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_401 (Dropout)           (None, 32, 32, 12)   0           conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_195 (Concatenate)   (None, 32, 32, 84)   0           concatenate_194[0][0]            \n",
      "                                                                 dropout_401[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 32, 32, 84)   336         concatenate_195[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 32, 32, 84)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 32, 32, 48)   4032        activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_402 (Dropout)           (None, 32, 32, 48)   0           conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 32, 32, 48)   192         dropout_402[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 32, 32, 48)   0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 32, 32, 12)   5184        activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_403 (Dropout)           (None, 32, 32, 12)   0           conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_196 (Concatenate)   (None, 32, 32, 96)   0           concatenate_195[0][0]            \n",
      "                                                                 dropout_403[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 32, 32, 96)   384         concatenate_196[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 32, 32, 96)   0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 32, 32, 48)   4608        activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_404 (Dropout)           (None, 32, 32, 48)   0           conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 32, 32, 48)   192         dropout_404[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 32, 32, 48)   0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 32, 32, 12)   5184        activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_405 (Dropout)           (None, 32, 32, 12)   0           conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_197 (Concatenate)   (None, 32, 32, 108)  0           concatenate_196[0][0]            \n",
      "                                                                 dropout_405[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 32, 32, 108)  432         concatenate_197[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 32, 32, 108)  0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 32, 32, 48)   5184        activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_406 (Dropout)           (None, 32, 32, 48)   0           conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 32, 32, 48)   192         dropout_406[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 32, 32, 48)   0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 32, 32, 12)   5184        activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_407 (Dropout)           (None, 32, 32, 12)   0           conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_198 (Concatenate)   (None, 32, 32, 120)  0           concatenate_197[0][0]            \n",
      "                                                                 dropout_407[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 32, 32, 120)  480         concatenate_198[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 32, 32, 120)  0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 32, 32, 60)   7200        activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_408 (Dropout)           (None, 32, 32, 60)   0           conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 16, 16, 60)   0           dropout_408[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 16, 16, 60)   240         average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 16, 16, 60)   0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 16, 16, 48)   2880        activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_409 (Dropout)           (None, 16, 16, 48)   0           conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 16, 16, 48)   192         dropout_409[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 16, 16, 48)   0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 16, 16, 12)   5184        activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_410 (Dropout)           (None, 16, 16, 12)   0           conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_199 (Concatenate)   (None, 16, 16, 72)   0           average_pooling2d_15[0][0]       \n",
      "                                                                 dropout_410[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 16, 16, 72)   288         concatenate_199[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 16, 16, 72)   0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 16, 16, 48)   3456        activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_411 (Dropout)           (None, 16, 16, 48)   0           conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 16, 16, 48)   192         dropout_411[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 16, 16, 48)   0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 16, 16, 12)   5184        activation_415[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_412 (Dropout)           (None, 16, 16, 12)   0           conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_200 (Concatenate)   (None, 16, 16, 84)   0           concatenate_199[0][0]            \n",
      "                                                                 dropout_412[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 16, 16, 84)   336         concatenate_200[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 16, 16, 84)   0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 16, 16, 48)   4032        activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_413 (Dropout)           (None, 16, 16, 48)   0           conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 16, 16, 48)   192         dropout_413[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 16, 16, 48)   0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 16, 16, 12)   5184        activation_417[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_414 (Dropout)           (None, 16, 16, 12)   0           conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_201 (Concatenate)   (None, 16, 16, 96)   0           concatenate_200[0][0]            \n",
      "                                                                 dropout_414[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 16, 16, 96)   384         concatenate_201[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 16, 16, 96)   0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 16, 16, 48)   4608        activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_415 (Dropout)           (None, 16, 16, 48)   0           conv2d_420[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 16, 16, 48)   192         dropout_415[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 16, 16, 48)   0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 16, 16, 12)   5184        activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_416 (Dropout)           (None, 16, 16, 12)   0           conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_202 (Concatenate)   (None, 16, 16, 108)  0           concatenate_201[0][0]            \n",
      "                                                                 dropout_416[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 16, 16, 108)  432         concatenate_202[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 16, 16, 108)  0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 16, 16, 48)   5184        activation_420[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_417 (Dropout)           (None, 16, 16, 48)   0           conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 16, 16, 48)   192         dropout_417[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 16, 16, 48)   0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 16, 16, 12)   5184        activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_418 (Dropout)           (None, 16, 16, 12)   0           conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_203 (Concatenate)   (None, 16, 16, 120)  0           concatenate_202[0][0]            \n",
      "                                                                 dropout_418[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 16, 16, 120)  480         concatenate_203[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 16, 16, 120)  0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 16, 16, 48)   5760        activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_419 (Dropout)           (None, 16, 16, 48)   0           conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 16, 16, 48)   192         dropout_419[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 16, 16, 48)   0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 16, 16, 12)   5184        activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_420 (Dropout)           (None, 16, 16, 12)   0           conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_204 (Concatenate)   (None, 16, 16, 132)  0           concatenate_203[0][0]            \n",
      "                                                                 dropout_420[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 16, 16, 132)  528         concatenate_204[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 16, 16, 132)  0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 16, 16, 48)   6336        activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_421 (Dropout)           (None, 16, 16, 48)   0           conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 16, 16, 48)   192         dropout_421[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 16, 16, 48)   0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_427 (Conv2D)             (None, 16, 16, 12)   5184        activation_425[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_422 (Dropout)           (None, 16, 16, 12)   0           conv2d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_205 (Concatenate)   (None, 16, 16, 144)  0           concatenate_204[0][0]            \n",
      "                                                                 dropout_422[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_426 (BatchN (None, 16, 16, 144)  576         concatenate_205[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 16, 16, 144)  0           batch_normalization_426[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_428 (Conv2D)             (None, 16, 16, 48)   6912        activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_423 (Dropout)           (None, 16, 16, 48)   0           conv2d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_427 (BatchN (None, 16, 16, 48)   192         dropout_423[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 16, 16, 48)   0           batch_normalization_427[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_429 (Conv2D)             (None, 16, 16, 12)   5184        activation_427[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_424 (Dropout)           (None, 16, 16, 12)   0           conv2d_429[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_206 (Concatenate)   (None, 16, 16, 156)  0           concatenate_205[0][0]            \n",
      "                                                                 dropout_424[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_428 (BatchN (None, 16, 16, 156)  624         concatenate_206[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 16, 16, 156)  0           batch_normalization_428[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_430 (Conv2D)             (None, 16, 16, 48)   7488        activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_425 (Dropout)           (None, 16, 16, 48)   0           conv2d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_429 (BatchN (None, 16, 16, 48)   192         dropout_425[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 16, 16, 48)   0           batch_normalization_429[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_431 (Conv2D)             (None, 16, 16, 12)   5184        activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_426 (Dropout)           (None, 16, 16, 12)   0           conv2d_431[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_207 (Concatenate)   (None, 16, 16, 168)  0           concatenate_206[0][0]            \n",
      "                                                                 dropout_426[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_430 (BatchN (None, 16, 16, 168)  672         concatenate_207[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 16, 16, 168)  0           batch_normalization_430[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_432 (Conv2D)             (None, 16, 16, 48)   8064        activation_430[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_427 (Dropout)           (None, 16, 16, 48)   0           conv2d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_431 (BatchN (None, 16, 16, 48)   192         dropout_427[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 16, 16, 48)   0           batch_normalization_431[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_433 (Conv2D)             (None, 16, 16, 12)   5184        activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_428 (Dropout)           (None, 16, 16, 12)   0           conv2d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_208 (Concatenate)   (None, 16, 16, 180)  0           concatenate_207[0][0]            \n",
      "                                                                 dropout_428[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_432 (BatchN (None, 16, 16, 180)  720         concatenate_208[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 16, 16, 180)  0           batch_normalization_432[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_434 (Conv2D)             (None, 16, 16, 48)   8640        activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_429 (Dropout)           (None, 16, 16, 48)   0           conv2d_434[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_433 (BatchN (None, 16, 16, 48)   192         dropout_429[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 16, 16, 48)   0           batch_normalization_433[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_435 (Conv2D)             (None, 16, 16, 12)   5184        activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_430 (Dropout)           (None, 16, 16, 12)   0           conv2d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_209 (Concatenate)   (None, 16, 16, 192)  0           concatenate_208[0][0]            \n",
      "                                                                 dropout_430[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_434 (BatchN (None, 16, 16, 192)  768         concatenate_209[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 16, 16, 192)  0           batch_normalization_434[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_436 (Conv2D)             (None, 16, 16, 48)   9216        activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_431 (Dropout)           (None, 16, 16, 48)   0           conv2d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_435 (BatchN (None, 16, 16, 48)   192         dropout_431[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 16, 16, 48)   0           batch_normalization_435[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_437 (Conv2D)             (None, 16, 16, 12)   5184        activation_435[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_432 (Dropout)           (None, 16, 16, 12)   0           conv2d_437[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_210 (Concatenate)   (None, 16, 16, 204)  0           concatenate_209[0][0]            \n",
      "                                                                 dropout_432[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_436 (BatchN (None, 16, 16, 204)  816         concatenate_210[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 16, 16, 204)  0           batch_normalization_436[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_438 (Conv2D)             (None, 16, 16, 48)   9792        activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_433 (Dropout)           (None, 16, 16, 48)   0           conv2d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_437 (BatchN (None, 16, 16, 48)   192         dropout_433[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 16, 16, 48)   0           batch_normalization_437[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_439 (Conv2D)             (None, 16, 16, 12)   5184        activation_437[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_434 (Dropout)           (None, 16, 16, 12)   0           conv2d_439[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_211 (Concatenate)   (None, 16, 16, 216)  0           concatenate_210[0][0]            \n",
      "                                                                 dropout_434[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_438 (BatchN (None, 16, 16, 216)  864         concatenate_211[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 16, 16, 216)  0           batch_normalization_438[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_440 (Conv2D)             (None, 16, 16, 48)   10368       activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_435 (Dropout)           (None, 16, 16, 48)   0           conv2d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_439 (BatchN (None, 16, 16, 48)   192         dropout_435[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 16, 16, 48)   0           batch_normalization_439[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_441 (Conv2D)             (None, 16, 16, 12)   5184        activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_436 (Dropout)           (None, 16, 16, 12)   0           conv2d_441[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_212 (Concatenate)   (None, 16, 16, 228)  0           concatenate_211[0][0]            \n",
      "                                                                 dropout_436[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_440 (BatchN (None, 16, 16, 228)  912         concatenate_212[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 16, 16, 228)  0           batch_normalization_440[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_442 (Conv2D)             (None, 16, 16, 48)   10944       activation_440[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_437 (Dropout)           (None, 16, 16, 48)   0           conv2d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_441 (BatchN (None, 16, 16, 48)   192         dropout_437[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 16, 16, 48)   0           batch_normalization_441[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_443 (Conv2D)             (None, 16, 16, 12)   5184        activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_438 (Dropout)           (None, 16, 16, 12)   0           conv2d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_213 (Concatenate)   (None, 16, 16, 240)  0           concatenate_212[0][0]            \n",
      "                                                                 dropout_438[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_442 (BatchN (None, 16, 16, 240)  960         concatenate_213[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 16, 16, 240)  0           batch_normalization_442[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_444 (Conv2D)             (None, 16, 16, 48)   11520       activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_439 (Dropout)           (None, 16, 16, 48)   0           conv2d_444[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_443 (BatchN (None, 16, 16, 48)   192         dropout_439[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 16, 16, 48)   0           batch_normalization_443[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_445 (Conv2D)             (None, 16, 16, 12)   5184        activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_440 (Dropout)           (None, 16, 16, 12)   0           conv2d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_214 (Concatenate)   (None, 16, 16, 252)  0           concatenate_213[0][0]            \n",
      "                                                                 dropout_440[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_444 (BatchN (None, 16, 16, 252)  1008        concatenate_214[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 16, 16, 252)  0           batch_normalization_444[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_446 (Conv2D)             (None, 16, 16, 126)  31752       activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_441 (Dropout)           (None, 16, 16, 126)  0           conv2d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 8, 8, 126)    0           dropout_441[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_445 (BatchN (None, 8, 8, 126)    504         average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 8, 8, 126)    0           batch_normalization_445[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_447 (Conv2D)             (None, 8, 8, 48)     6048        activation_445[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_442 (Dropout)           (None, 8, 8, 48)     0           conv2d_447[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_446 (BatchN (None, 8, 8, 48)     192         dropout_442[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 8, 8, 48)     0           batch_normalization_446[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_448 (Conv2D)             (None, 8, 8, 12)     5184        activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_443 (Dropout)           (None, 8, 8, 12)     0           conv2d_448[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_215 (Concatenate)   (None, 8, 8, 138)    0           average_pooling2d_16[0][0]       \n",
      "                                                                 dropout_443[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_447 (BatchN (None, 8, 8, 138)    552         concatenate_215[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 8, 8, 138)    0           batch_normalization_447[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_449 (Conv2D)             (None, 8, 8, 48)     6624        activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_444 (Dropout)           (None, 8, 8, 48)     0           conv2d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 8, 8, 48)     192         dropout_444[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 8, 8, 48)     0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_450 (Conv2D)             (None, 8, 8, 12)     5184        activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_445 (Dropout)           (None, 8, 8, 12)     0           conv2d_450[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_216 (Concatenate)   (None, 8, 8, 150)    0           concatenate_215[0][0]            \n",
      "                                                                 dropout_445[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 8, 8, 150)    600         concatenate_216[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 8, 8, 150)    0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_451 (Conv2D)             (None, 8, 8, 48)     7200        activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_446 (Dropout)           (None, 8, 8, 48)     0           conv2d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 8, 8, 48)     192         dropout_446[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 8, 8, 48)     0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_452 (Conv2D)             (None, 8, 8, 12)     5184        activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_447 (Dropout)           (None, 8, 8, 12)     0           conv2d_452[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_217 (Concatenate)   (None, 8, 8, 162)    0           concatenate_216[0][0]            \n",
      "                                                                 dropout_447[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 8, 8, 162)    648         concatenate_217[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 8, 8, 162)    0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_453 (Conv2D)             (None, 8, 8, 48)     7776        activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_448 (Dropout)           (None, 8, 8, 48)     0           conv2d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 8, 8, 48)     192         dropout_448[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 8, 8, 48)     0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_454 (Conv2D)             (None, 8, 8, 12)     5184        activation_452[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_449 (Dropout)           (None, 8, 8, 12)     0           conv2d_454[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_218 (Concatenate)   (None, 8, 8, 174)    0           concatenate_217[0][0]            \n",
      "                                                                 dropout_449[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 8, 8, 174)    696         concatenate_218[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 8, 8, 174)    0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_455 (Conv2D)             (None, 8, 8, 48)     8352        activation_453[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_450 (Dropout)           (None, 8, 8, 48)     0           conv2d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 8, 8, 48)     192         dropout_450[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 8, 8, 48)     0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_456 (Conv2D)             (None, 8, 8, 12)     5184        activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_451 (Dropout)           (None, 8, 8, 12)     0           conv2d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_219 (Concatenate)   (None, 8, 8, 186)    0           concatenate_218[0][0]            \n",
      "                                                                 dropout_451[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 8, 8, 186)    744         concatenate_219[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 8, 8, 186)    0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_457 (Conv2D)             (None, 8, 8, 48)     8928        activation_455[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_452 (Dropout)           (None, 8, 8, 48)     0           conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 8, 8, 48)     192         dropout_452[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 8, 8, 48)     0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_458 (Conv2D)             (None, 8, 8, 12)     5184        activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_453 (Dropout)           (None, 8, 8, 12)     0           conv2d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_220 (Concatenate)   (None, 8, 8, 198)    0           concatenate_219[0][0]            \n",
      "                                                                 dropout_453[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 8, 8, 198)    792         concatenate_220[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 8, 8, 198)    0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_459 (Conv2D)             (None, 8, 8, 48)     9504        activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_454 (Dropout)           (None, 8, 8, 48)     0           conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_458 (BatchN (None, 8, 8, 48)     192         dropout_454[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 8, 8, 48)     0           batch_normalization_458[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_460 (Conv2D)             (None, 8, 8, 12)     5184        activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_455 (Dropout)           (None, 8, 8, 12)     0           conv2d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_221 (Concatenate)   (None, 8, 8, 210)    0           concatenate_220[0][0]            \n",
      "                                                                 dropout_455[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_459 (BatchN (None, 8, 8, 210)    840         concatenate_221[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 8, 8, 210)    0           batch_normalization_459[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_461 (Conv2D)             (None, 8, 8, 48)     10080       activation_459[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_456 (Dropout)           (None, 8, 8, 48)     0           conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_460 (BatchN (None, 8, 8, 48)     192         dropout_456[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 8, 8, 48)     0           batch_normalization_460[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_462 (Conv2D)             (None, 8, 8, 12)     5184        activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_457 (Dropout)           (None, 8, 8, 12)     0           conv2d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_222 (Concatenate)   (None, 8, 8, 222)    0           concatenate_221[0][0]            \n",
      "                                                                 dropout_457[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_461 (BatchN (None, 8, 8, 222)    888         concatenate_222[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 8, 8, 222)    0           batch_normalization_461[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_463 (Conv2D)             (None, 8, 8, 48)     10656       activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_458 (Dropout)           (None, 8, 8, 48)     0           conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_462 (BatchN (None, 8, 8, 48)     192         dropout_458[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 8, 8, 48)     0           batch_normalization_462[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_464 (Conv2D)             (None, 8, 8, 12)     5184        activation_462[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_459 (Dropout)           (None, 8, 8, 12)     0           conv2d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_223 (Concatenate)   (None, 8, 8, 234)    0           concatenate_222[0][0]            \n",
      "                                                                 dropout_459[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_463 (BatchN (None, 8, 8, 234)    936         concatenate_223[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_463 (Activation)     (None, 8, 8, 234)    0           batch_normalization_463[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_465 (Conv2D)             (None, 8, 8, 48)     11232       activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_460 (Dropout)           (None, 8, 8, 48)     0           conv2d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_464 (BatchN (None, 8, 8, 48)     192         dropout_460[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_464 (Activation)     (None, 8, 8, 48)     0           batch_normalization_464[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_466 (Conv2D)             (None, 8, 8, 12)     5184        activation_464[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_461 (Dropout)           (None, 8, 8, 12)     0           conv2d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_224 (Concatenate)   (None, 8, 8, 246)    0           concatenate_223[0][0]            \n",
      "                                                                 dropout_461[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_465 (BatchN (None, 8, 8, 246)    984         concatenate_224[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_465 (Activation)     (None, 8, 8, 246)    0           batch_normalization_465[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_467 (Conv2D)             (None, 8, 8, 48)     11808       activation_465[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_462 (Dropout)           (None, 8, 8, 48)     0           conv2d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_466 (BatchN (None, 8, 8, 48)     192         dropout_462[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_466 (Activation)     (None, 8, 8, 48)     0           batch_normalization_466[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_468 (Conv2D)             (None, 8, 8, 12)     5184        activation_466[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_463 (Dropout)           (None, 8, 8, 12)     0           conv2d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_225 (Concatenate)   (None, 8, 8, 258)    0           concatenate_224[0][0]            \n",
      "                                                                 dropout_463[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_467 (BatchN (None, 8, 8, 258)    1032        concatenate_225[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_467 (Activation)     (None, 8, 8, 258)    0           batch_normalization_467[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 8, 8, 48)     12384       activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_464 (Dropout)           (None, 8, 8, 48)     0           conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_468 (BatchN (None, 8, 8, 48)     192         dropout_464[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_468 (Activation)     (None, 8, 8, 48)     0           batch_normalization_468[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 8, 8, 12)     5184        activation_468[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_465 (Dropout)           (None, 8, 8, 12)     0           conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_226 (Concatenate)   (None, 8, 8, 270)    0           concatenate_225[0][0]            \n",
      "                                                                 dropout_465[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_469 (BatchN (None, 8, 8, 270)    1080        concatenate_226[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_469 (Activation)     (None, 8, 8, 270)    0           batch_normalization_469[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 8, 8, 48)     12960       activation_469[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_466 (Dropout)           (None, 8, 8, 48)     0           conv2d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_470 (BatchN (None, 8, 8, 48)     192         dropout_466[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_470 (Activation)     (None, 8, 8, 48)     0           batch_normalization_470[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 8, 8, 12)     5184        activation_470[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_467 (Dropout)           (None, 8, 8, 12)     0           conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_227 (Concatenate)   (None, 8, 8, 282)    0           concatenate_226[0][0]            \n",
      "                                                                 dropout_467[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_471 (BatchN (None, 8, 8, 282)    1128        concatenate_227[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_471 (Activation)     (None, 8, 8, 282)    0           batch_normalization_471[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 8, 8, 48)     13536       activation_471[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_468 (Dropout)           (None, 8, 8, 48)     0           conv2d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_472 (BatchN (None, 8, 8, 48)     192         dropout_468[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_472 (Activation)     (None, 8, 8, 48)     0           batch_normalization_472[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 8, 8, 12)     5184        activation_472[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_469 (Dropout)           (None, 8, 8, 12)     0           conv2d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_228 (Concatenate)   (None, 8, 8, 294)    0           concatenate_227[0][0]            \n",
      "                                                                 dropout_469[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_473 (BatchN (None, 8, 8, 294)    1176        concatenate_228[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_473 (Activation)     (None, 8, 8, 294)    0           batch_normalization_473[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_475 (Conv2D)             (None, 8, 8, 48)     14112       activation_473[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_470 (Dropout)           (None, 8, 8, 48)     0           conv2d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_474 (BatchN (None, 8, 8, 48)     192         dropout_470[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_474 (Activation)     (None, 8, 8, 48)     0           batch_normalization_474[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_476 (Conv2D)             (None, 8, 8, 12)     5184        activation_474[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_471 (Dropout)           (None, 8, 8, 12)     0           conv2d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_229 (Concatenate)   (None, 8, 8, 306)    0           concatenate_228[0][0]            \n",
      "                                                                 dropout_471[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_475 (BatchN (None, 8, 8, 306)    1224        concatenate_229[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_475 (Activation)     (None, 8, 8, 306)    0           batch_normalization_475[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_477 (Conv2D)             (None, 8, 8, 48)     14688       activation_475[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_472 (Dropout)           (None, 8, 8, 48)     0           conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_476 (BatchN (None, 8, 8, 48)     192         dropout_472[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_476 (Activation)     (None, 8, 8, 48)     0           batch_normalization_476[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_478 (Conv2D)             (None, 8, 8, 12)     5184        activation_476[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_473 (Dropout)           (None, 8, 8, 12)     0           conv2d_478[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_230 (Concatenate)   (None, 8, 8, 318)    0           concatenate_229[0][0]            \n",
      "                                                                 dropout_473[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_477 (BatchN (None, 8, 8, 318)    1272        concatenate_230[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_477 (Activation)     (None, 8, 8, 318)    0           batch_normalization_477[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_479 (Conv2D)             (None, 8, 8, 48)     15264       activation_477[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_474 (Dropout)           (None, 8, 8, 48)     0           conv2d_479[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_478 (BatchN (None, 8, 8, 48)     192         dropout_474[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_478 (Activation)     (None, 8, 8, 48)     0           batch_normalization_478[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_480 (Conv2D)             (None, 8, 8, 12)     5184        activation_478[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_475 (Dropout)           (None, 8, 8, 12)     0           conv2d_480[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_231 (Concatenate)   (None, 8, 8, 330)    0           concatenate_230[0][0]            \n",
      "                                                                 dropout_475[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_479 (BatchN (None, 8, 8, 330)    1320        concatenate_231[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_479 (Activation)     (None, 8, 8, 330)    0           batch_normalization_479[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_481 (Conv2D)             (None, 8, 8, 48)     15840       activation_479[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_476 (Dropout)           (None, 8, 8, 48)     0           conv2d_481[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_480 (BatchN (None, 8, 8, 48)     192         dropout_476[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_480 (Activation)     (None, 8, 8, 48)     0           batch_normalization_480[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_482 (Conv2D)             (None, 8, 8, 12)     5184        activation_480[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_477 (Dropout)           (None, 8, 8, 12)     0           conv2d_482[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_232 (Concatenate)   (None, 8, 8, 342)    0           concatenate_231[0][0]            \n",
      "                                                                 dropout_477[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_481 (BatchN (None, 8, 8, 342)    1368        concatenate_232[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_481 (Activation)     (None, 8, 8, 342)    0           batch_normalization_481[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_483 (Conv2D)             (None, 8, 8, 48)     16416       activation_481[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_478 (Dropout)           (None, 8, 8, 48)     0           conv2d_483[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_482 (BatchN (None, 8, 8, 48)     192         dropout_478[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_482 (Activation)     (None, 8, 8, 48)     0           batch_normalization_482[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_484 (Conv2D)             (None, 8, 8, 12)     5184        activation_482[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_479 (Dropout)           (None, 8, 8, 12)     0           conv2d_484[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_233 (Concatenate)   (None, 8, 8, 354)    0           concatenate_232[0][0]            \n",
      "                                                                 dropout_479[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_483 (BatchN (None, 8, 8, 354)    1416        concatenate_233[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_483 (Activation)     (None, 8, 8, 354)    0           batch_normalization_483[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_485 (Conv2D)             (None, 8, 8, 48)     16992       activation_483[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_480 (Dropout)           (None, 8, 8, 48)     0           conv2d_485[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_484 (BatchN (None, 8, 8, 48)     192         dropout_480[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_484 (Activation)     (None, 8, 8, 48)     0           batch_normalization_484[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_486 (Conv2D)             (None, 8, 8, 12)     5184        activation_484[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_481 (Dropout)           (None, 8, 8, 12)     0           conv2d_486[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_234 (Concatenate)   (None, 8, 8, 366)    0           concatenate_233[0][0]            \n",
      "                                                                 dropout_481[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_485 (BatchN (None, 8, 8, 366)    1464        concatenate_234[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_485 (Activation)     (None, 8, 8, 366)    0           batch_normalization_485[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_487 (Conv2D)             (None, 8, 8, 183)    66978       activation_485[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_482 (Dropout)           (None, 8, 8, 183)    0           conv2d_487[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 4, 4, 183)    0           dropout_482[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_486 (BatchN (None, 4, 4, 183)    732         average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_486 (Activation)     (None, 4, 4, 183)    0           batch_normalization_486[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_488 (Conv2D)             (None, 4, 4, 48)     8784        activation_486[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_483 (Dropout)           (None, 4, 4, 48)     0           conv2d_488[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_487 (BatchN (None, 4, 4, 48)     192         dropout_483[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_487 (Activation)     (None, 4, 4, 48)     0           batch_normalization_487[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_489 (Conv2D)             (None, 4, 4, 12)     5184        activation_487[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_484 (Dropout)           (None, 4, 4, 12)     0           conv2d_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_235 (Concatenate)   (None, 4, 4, 195)    0           average_pooling2d_17[0][0]       \n",
      "                                                                 dropout_484[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_488 (BatchN (None, 4, 4, 195)    780         concatenate_235[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_488 (Activation)     (None, 4, 4, 195)    0           batch_normalization_488[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_490 (Conv2D)             (None, 4, 4, 48)     9360        activation_488[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_485 (Dropout)           (None, 4, 4, 48)     0           conv2d_490[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_489 (BatchN (None, 4, 4, 48)     192         dropout_485[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_489 (Activation)     (None, 4, 4, 48)     0           batch_normalization_489[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_491 (Conv2D)             (None, 4, 4, 12)     5184        activation_489[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_486 (Dropout)           (None, 4, 4, 12)     0           conv2d_491[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_236 (Concatenate)   (None, 4, 4, 207)    0           concatenate_235[0][0]            \n",
      "                                                                 dropout_486[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_490 (BatchN (None, 4, 4, 207)    828         concatenate_236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_490 (Activation)     (None, 4, 4, 207)    0           batch_normalization_490[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_492 (Conv2D)             (None, 4, 4, 48)     9936        activation_490[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_487 (Dropout)           (None, 4, 4, 48)     0           conv2d_492[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_491 (BatchN (None, 4, 4, 48)     192         dropout_487[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_491 (Activation)     (None, 4, 4, 48)     0           batch_normalization_491[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_493 (Conv2D)             (None, 4, 4, 12)     5184        activation_491[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_488 (Dropout)           (None, 4, 4, 12)     0           conv2d_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_237 (Concatenate)   (None, 4, 4, 219)    0           concatenate_236[0][0]            \n",
      "                                                                 dropout_488[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_492 (BatchN (None, 4, 4, 219)    876         concatenate_237[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_492 (Activation)     (None, 4, 4, 219)    0           batch_normalization_492[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_494 (Conv2D)             (None, 4, 4, 48)     10512       activation_492[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_489 (Dropout)           (None, 4, 4, 48)     0           conv2d_494[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_493 (BatchN (None, 4, 4, 48)     192         dropout_489[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_493 (Activation)     (None, 4, 4, 48)     0           batch_normalization_493[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_495 (Conv2D)             (None, 4, 4, 12)     5184        activation_493[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_490 (Dropout)           (None, 4, 4, 12)     0           conv2d_495[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_238 (Concatenate)   (None, 4, 4, 231)    0           concatenate_237[0][0]            \n",
      "                                                                 dropout_490[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_494 (BatchN (None, 4, 4, 231)    924         concatenate_238[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_494 (Activation)     (None, 4, 4, 231)    0           batch_normalization_494[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_496 (Conv2D)             (None, 4, 4, 48)     11088       activation_494[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_491 (Dropout)           (None, 4, 4, 48)     0           conv2d_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_495 (BatchN (None, 4, 4, 48)     192         dropout_491[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_495 (Activation)     (None, 4, 4, 48)     0           batch_normalization_495[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_497 (Conv2D)             (None, 4, 4, 12)     5184        activation_495[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_492 (Dropout)           (None, 4, 4, 12)     0           conv2d_497[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_239 (Concatenate)   (None, 4, 4, 243)    0           concatenate_238[0][0]            \n",
      "                                                                 dropout_492[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_496 (BatchN (None, 4, 4, 243)    972         concatenate_239[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_496 (Activation)     (None, 4, 4, 243)    0           batch_normalization_496[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_498 (Conv2D)             (None, 4, 4, 48)     11664       activation_496[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_493 (Dropout)           (None, 4, 4, 48)     0           conv2d_498[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_497 (BatchN (None, 4, 4, 48)     192         dropout_493[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_497 (Activation)     (None, 4, 4, 48)     0           batch_normalization_497[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_499 (Conv2D)             (None, 4, 4, 12)     5184        activation_497[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_494 (Dropout)           (None, 4, 4, 12)     0           conv2d_499[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_240 (Concatenate)   (None, 4, 4, 255)    0           concatenate_239[0][0]            \n",
      "                                                                 dropout_494[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_498 (BatchN (None, 4, 4, 255)    1020        concatenate_240[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_498 (Activation)     (None, 4, 4, 255)    0           batch_normalization_498[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_500 (Conv2D)             (None, 4, 4, 48)     12240       activation_498[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_495 (Dropout)           (None, 4, 4, 48)     0           conv2d_500[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_499 (BatchN (None, 4, 4, 48)     192         dropout_495[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_499 (Activation)     (None, 4, 4, 48)     0           batch_normalization_499[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_501 (Conv2D)             (None, 4, 4, 12)     5184        activation_499[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_496 (Dropout)           (None, 4, 4, 12)     0           conv2d_501[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_241 (Concatenate)   (None, 4, 4, 267)    0           concatenate_240[0][0]            \n",
      "                                                                 dropout_496[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_500 (BatchN (None, 4, 4, 267)    1068        concatenate_241[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_500 (Activation)     (None, 4, 4, 267)    0           batch_normalization_500[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_502 (Conv2D)             (None, 4, 4, 48)     12816       activation_500[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_497 (Dropout)           (None, 4, 4, 48)     0           conv2d_502[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_501 (BatchN (None, 4, 4, 48)     192         dropout_497[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_501 (Activation)     (None, 4, 4, 48)     0           batch_normalization_501[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_503 (Conv2D)             (None, 4, 4, 12)     5184        activation_501[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_498 (Dropout)           (None, 4, 4, 12)     0           conv2d_503[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_242 (Concatenate)   (None, 4, 4, 279)    0           concatenate_241[0][0]            \n",
      "                                                                 dropout_498[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_502 (BatchN (None, 4, 4, 279)    1116        concatenate_242[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_502 (Activation)     (None, 4, 4, 279)    0           batch_normalization_502[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_504 (Conv2D)             (None, 4, 4, 48)     13392       activation_502[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_499 (Dropout)           (None, 4, 4, 48)     0           conv2d_504[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_503 (BatchN (None, 4, 4, 48)     192         dropout_499[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_503 (Activation)     (None, 4, 4, 48)     0           batch_normalization_503[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_505 (Conv2D)             (None, 4, 4, 12)     5184        activation_503[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_500 (Dropout)           (None, 4, 4, 12)     0           conv2d_505[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_243 (Concatenate)   (None, 4, 4, 291)    0           concatenate_242[0][0]            \n",
      "                                                                 dropout_500[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_504 (BatchN (None, 4, 4, 291)    1164        concatenate_243[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_504 (Activation)     (None, 4, 4, 291)    0           batch_normalization_504[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_506 (Conv2D)             (None, 4, 4, 48)     13968       activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_501 (Dropout)           (None, 4, 4, 48)     0           conv2d_506[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_505 (BatchN (None, 4, 4, 48)     192         dropout_501[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_505 (Activation)     (None, 4, 4, 48)     0           batch_normalization_505[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_507 (Conv2D)             (None, 4, 4, 12)     5184        activation_505[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_502 (Dropout)           (None, 4, 4, 12)     0           conv2d_507[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_244 (Concatenate)   (None, 4, 4, 303)    0           concatenate_243[0][0]            \n",
      "                                                                 dropout_502[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_506 (BatchN (None, 4, 4, 303)    1212        concatenate_244[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 4, 4, 303)    0           batch_normalization_506[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_508 (Conv2D)             (None, 4, 4, 48)     14544       activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_503 (Dropout)           (None, 4, 4, 48)     0           conv2d_508[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_507 (BatchN (None, 4, 4, 48)     192         dropout_503[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 4, 4, 48)     0           batch_normalization_507[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_509 (Conv2D)             (None, 4, 4, 12)     5184        activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_504 (Dropout)           (None, 4, 4, 12)     0           conv2d_509[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_245 (Concatenate)   (None, 4, 4, 315)    0           concatenate_244[0][0]            \n",
      "                                                                 dropout_504[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_508 (BatchN (None, 4, 4, 315)    1260        concatenate_245[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 4, 4, 315)    0           batch_normalization_508[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_510 (Conv2D)             (None, 4, 4, 48)     15120       activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_505 (Dropout)           (None, 4, 4, 48)     0           conv2d_510[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_509 (BatchN (None, 4, 4, 48)     192         dropout_505[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 4, 4, 48)     0           batch_normalization_509[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_511 (Conv2D)             (None, 4, 4, 12)     5184        activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_506 (Dropout)           (None, 4, 4, 12)     0           conv2d_511[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_246 (Concatenate)   (None, 4, 4, 327)    0           concatenate_245[0][0]            \n",
      "                                                                 dropout_506[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_510 (BatchN (None, 4, 4, 327)    1308        concatenate_246[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 4, 4, 327)    0           batch_normalization_510[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 2, 2, 327)    0           activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1308)         0           average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           13090       flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 982,216\n",
      "Trainable params: 953,278\n",
      "Non-trainable params: 28,938\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f68LtcYHG6i4"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 90\n",
    "decay = learning_rate/epochs\n",
    "decay = 0.0001\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=learning_rate, decay=decay, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Oj8RSwyG6i9"
   },
   "outputs": [],
   "source": [
    "#batch_size = 64\n",
    "#clr_triangular = CyclicLR(mode='triangular', base_lr = 0.1, max_lr = 0.2, step_size = (len(x_train)* 2 * 4)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HT7ZwHzG6jG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crhGk7kEhXAz"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcWydmIVhZGr"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 30.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7dl5K84G6jl"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay1(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epoch_drop_01 = 40\n",
    "    epoch_drop_02 = epoch_drop_01 + 40\n",
    "    epoch_drop_03 = epoch_drop_02 + 40\n",
    "    \n",
    "    if (epoch < epoch_drop_01):\n",
    "        lrate = initial_lrate\n",
    "    elif (epoch < epoch_drop_02):\n",
    "        lrate = initial_lrate * drop\n",
    "    else:\n",
    "        lrate = initial_lrate * drop * drop\n",
    "\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDig-b71G6jq"
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(self, epoch, logs=None):\n",
    "    print(\"epoch: \", epoch,\"learning rate for\", K.eval(self.model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zjHHVfPG6jw"
   },
   "outputs": [],
   "source": [
    "lrate = LearningRateScheduler(step_decay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aS6q4X1G6j0"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.001)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience= 4, min_delta=0.003, verbose=1, cooldown=0, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJgRsh_2G6j7"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZZuotjhG6kA"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#filepath = file_prefix + r\".best.hdf5\"\n",
    "filepath = \"DNST_CIFAR10_Conv_09_09-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE3lF6EH1r_L"
   },
   "outputs": [],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "#model.save_weights(\"densenet_tr_03-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai-yZ2ED5AK1"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "\n",
    "#files.download('DNST_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir_fg-p9G6kO"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6HtNUyQG6kS"
   },
   "outputs": [],
   "source": [
    "class AdamTracker_0(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"start , epoch = \", epoch,\", lr = \", K.eval(optimizer.lr),\", decay = \",K.eval(optimizer.decay),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BtYHxTCG6kV"
   },
   "outputs": [],
   "source": [
    "class AdamTracker_1(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"end, epoch = \", epoch,\", lr = \", K.eval(optimizer.lr),\", decay = \",K.eval(optimizer.decay),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMe9lOY9G6kb"
   },
   "outputs": [],
   "source": [
    "adam_lr_tracker_1 = AdamTracker_1()\n",
    "adam_lr_tracker_0 = AdamTracker_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2iue0UsLDzb"
   },
   "outputs": [],
   "source": [
    "class SGDLearningRateTracker(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"epoch = \", epoch,\", lr = \", K.eval(optimizer.lr), \", momentum = \",K.eval(optimizer.momentum),\n",
    "              \", decay = \",K.eval(optimizer.decay), \", Nestrov = \",optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-itoaFDG6kf"
   },
   "outputs": [],
   "source": [
    "sgd_lr_tracker = SGDLearningRateTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Og56VCRh5j8V"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint, reduce_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ki7pVU60G6ko"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [adam_lr_tracker_0, adam_lr_tracker_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBTjYaJ4G6kv"
   },
   "outputs": [],
   "source": [
    "#callbacks_list = [checkpoint, adam_lr_tracker_0, adam_lr_tracker_1, clr_triangular]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhuPjscpK0mm"
   },
   "outputs": [],
   "source": [
    "#callbacks_list = [checkpoint, sgd_lr_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bTKPS9HG6kx"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model('densenet_tr_03-46-0.71.hdf5')\n",
    "#score = model.evaluate(x_test, y_test, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OfdusR7aG6k1",
    "outputId": "f23665c5-6112-4dcc-d452-fae080e5ab70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-LiiCB9G6k8"
   },
   "source": [
    "## Call the model with the datagen, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WKzwh45G6k8"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZjsbZONG6lJ"
   },
   "outputs": [],
   "source": [
    "load_model_from_back = False\n",
    "\n",
    "if load_model_from_back:\n",
    "    model = load_model('DNST_CIFAR10_Conv_04.hdf5')\n",
    "    score = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4114
    },
    "colab_type": "code",
    "id": "ODPSQd8dG6lM",
    "outputId": "0b061b74-5653-4c53-932e-04eb30c3664e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 208s 266ms/step - loss: 1.6106 - acc: 0.4119 - val_loss: 1.2744 - val_acc: 0.5446\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/80\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 1.2059 - acc: 0.5675 - val_loss: 1.9734 - val_acc: 0.5090\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/80\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.9778 - acc: 0.6503 - val_loss: 1.1878 - val_acc: 0.6582\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/80\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.8482 - acc: 0.7010 - val_loss: 1.0626 - val_acc: 0.6796\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/80\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.7625 - acc: 0.7322 - val_loss: 1.4235 - val_acc: 0.6556\n",
      "end, epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "Epoch 6/80\n",
      "start , epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.6951 - acc: 0.7577 - val_loss: 0.9160 - val_acc: 0.7480\n",
      "end, epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "Epoch 7/80\n",
      "start , epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.6461 - acc: 0.7755 - val_loss: 0.7948 - val_acc: 0.7609\n",
      "end, epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "Epoch 8/80\n",
      "start , epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.6095 - acc: 0.7878 - val_loss: 0.6962 - val_acc: 0.7962\n",
      "end, epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "Epoch 9/80\n",
      "start , epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.5740 - acc: 0.8005 - val_loss: 0.6233 - val_acc: 0.8135\n",
      "end, epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "Epoch 10/80\n",
      "start , epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.5440 - acc: 0.8095 - val_loss: 0.7660 - val_acc: 0.7888\n",
      "end, epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "Epoch 11/80\n",
      "start , epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.5262 - acc: 0.8166 - val_loss: 0.7724 - val_acc: 0.8010\n",
      "end, epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "Epoch 12/80\n",
      "start , epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.4998 - acc: 0.8266 - val_loss: 0.7451 - val_acc: 0.7925\n",
      "end, epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "Epoch 13/80\n",
      "start , epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.4869 - acc: 0.8316 - val_loss: 0.6082 - val_acc: 0.8181\n",
      "end, epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "Epoch 14/80\n",
      "start , epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.4629 - acc: 0.8386 - val_loss: 0.7386 - val_acc: 0.7967\n",
      "end, epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "Epoch 15/80\n",
      "start , epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.4492 - acc: 0.8450 - val_loss: 0.5412 - val_acc: 0.8398\n",
      "end, epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "Epoch 16/80\n",
      "start , epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.4331 - acc: 0.8506 - val_loss: 0.5230 - val_acc: 0.8455\n",
      "end, epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "Epoch 17/80\n",
      "start , epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 146s 187ms/step - loss: 0.4255 - acc: 0.8516 - val_loss: 0.5083 - val_acc: 0.8468\n",
      "end, epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "Epoch 18/80\n",
      "start , epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.4062 - acc: 0.8572 - val_loss: 0.5275 - val_acc: 0.8445\n",
      "end, epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "Epoch 19/80\n",
      "start , epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 146s 187ms/step - loss: 0.4024 - acc: 0.8603 - val_loss: 0.4889 - val_acc: 0.8521\n",
      "end, epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "Epoch 20/80\n",
      "start , epoch =  19 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.3880 - acc: 0.8658 - val_loss: 0.5449 - val_acc: 0.8486\n",
      "end, epoch =  19 , lr =  0.001 , decay =  0.0\n",
      "Epoch 21/80\n",
      "start , epoch =  20 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.3760 - acc: 0.8679 - val_loss: 0.4366 - val_acc: 0.8718\n",
      "end, epoch =  20 , lr =  0.001 , decay =  0.0\n",
      "Epoch 22/80\n",
      "start , epoch =  21 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.3680 - acc: 0.8723 - val_loss: 0.5375 - val_acc: 0.8453\n",
      "end, epoch =  21 , lr =  0.001 , decay =  0.0\n",
      "Epoch 23/80\n",
      "start , epoch =  22 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.3598 - acc: 0.8752 - val_loss: 0.4460 - val_acc: 0.8645\n",
      "end, epoch =  22 , lr =  0.001 , decay =  0.0\n",
      "Epoch 24/80\n",
      "start , epoch =  23 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.3520 - acc: 0.8801 - val_loss: 0.6801 - val_acc: 0.8243\n",
      "end, epoch =  23 , lr =  0.001 , decay =  0.0\n",
      "Epoch 25/80\n",
      "start , epoch =  24 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.3425 - acc: 0.8805 - val_loss: 0.5291 - val_acc: 0.8582\n",
      "end, epoch =  24 , lr =  0.001 , decay =  0.0\n",
      "Epoch 26/80\n",
      "start , epoch =  25 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.3363 - acc: 0.8831 - val_loss: 0.5067 - val_acc: 0.8552\n",
      "end, epoch =  25 , lr =  0.001 , decay =  0.0\n",
      "Epoch 27/80\n",
      "start , epoch =  26 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.3327 - acc: 0.8837 - val_loss: 0.4851 - val_acc: 0.8642\n",
      "end, epoch =  26 , lr =  0.001 , decay =  0.0\n",
      "Epoch 28/80\n",
      "start , epoch =  27 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.3248 - acc: 0.8868 - val_loss: 0.5500 - val_acc: 0.8508\n",
      "end, epoch =  27 , lr =  0.001 , decay =  0.0\n",
      "Epoch 29/80\n",
      "start , epoch =  28 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.3219 - acc: 0.8859 - val_loss: 0.4929 - val_acc: 0.8639\n",
      "end, epoch =  28 , lr =  0.001 , decay =  0.0\n",
      "Epoch 30/80\n",
      "start , epoch =  29 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.3139 - acc: 0.8894 - val_loss: 0.5305 - val_acc: 0.8576\n",
      "end, epoch =  29 , lr =  0.001 , decay =  0.0\n",
      "Epoch 31/80\n",
      "start , epoch =  30 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.3096 - acc: 0.8916 - val_loss: 0.4950 - val_acc: 0.8564\n",
      "end, epoch =  30 , lr =  0.001 , decay =  0.0\n",
      "Epoch 32/80\n",
      "start , epoch =  31 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.3031 - acc: 0.8939 - val_loss: 0.4323 - val_acc: 0.8779\n",
      "end, epoch =  31 , lr =  0.001 , decay =  0.0\n",
      "Epoch 33/80\n",
      "start , epoch =  32 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2962 - acc: 0.8965 - val_loss: 0.5219 - val_acc: 0.8646\n",
      "end, epoch =  32 , lr =  0.001 , decay =  0.0\n",
      "Epoch 34/80\n",
      "start , epoch =  33 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 187ms/step - loss: 0.2890 - acc: 0.8977 - val_loss: 0.3878 - val_acc: 0.8888\n",
      "end, epoch =  33 , lr =  0.001 , decay =  0.0\n",
      "Epoch 35/80\n",
      "start , epoch =  34 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.2827 - acc: 0.8998 - val_loss: 0.6569 - val_acc: 0.8408\n",
      "end, epoch =  34 , lr =  0.001 , decay =  0.0\n",
      "Epoch 36/80\n",
      "start , epoch =  35 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2838 - acc: 0.9017 - val_loss: 0.4318 - val_acc: 0.8818\n",
      "end, epoch =  35 , lr =  0.001 , decay =  0.0\n",
      "Epoch 37/80\n",
      "start , epoch =  36 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.2764 - acc: 0.9016 - val_loss: 0.5519 - val_acc: 0.8520\n",
      "end, epoch =  36 , lr =  0.001 , decay =  0.0\n",
      "Epoch 38/80\n",
      "start , epoch =  37 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2723 - acc: 0.9039 - val_loss: 0.5150 - val_acc: 0.8621\n",
      "end, epoch =  37 , lr =  0.001 , decay =  0.0\n",
      "Epoch 39/80\n",
      "start , epoch =  38 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2648 - acc: 0.9068 - val_loss: 0.4433 - val_acc: 0.8762\n",
      "end, epoch =  38 , lr =  0.001 , decay =  0.0\n",
      "Epoch 40/80\n",
      "start , epoch =  39 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2698 - acc: 0.9044 - val_loss: 0.3846 - val_acc: 0.8903\n",
      "end, epoch =  39 , lr =  0.001 , decay =  0.0\n",
      "Epoch 41/80\n",
      "start , epoch =  40 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.2613 - acc: 0.9081 - val_loss: 0.4517 - val_acc: 0.8830\n",
      "end, epoch =  40 , lr =  0.001 , decay =  0.0\n",
      "Epoch 42/80\n",
      "start , epoch =  41 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2566 - acc: 0.9098 - val_loss: 0.4879 - val_acc: 0.8733\n",
      "end, epoch =  41 , lr =  0.001 , decay =  0.0\n",
      "Epoch 43/80\n",
      "start , epoch =  42 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2524 - acc: 0.9097 - val_loss: 0.4236 - val_acc: 0.8817\n",
      "end, epoch =  42 , lr =  0.001 , decay =  0.0\n",
      "Epoch 44/80\n",
      "start , epoch =  43 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2474 - acc: 0.9119 - val_loss: 0.4064 - val_acc: 0.8912\n",
      "end, epoch =  43 , lr =  0.001 , decay =  0.0\n",
      "Epoch 45/80\n",
      "start , epoch =  44 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2462 - acc: 0.9133 - val_loss: 0.3991 - val_acc: 0.8891\n",
      "end, epoch =  44 , lr =  0.001 , decay =  0.0\n",
      "Epoch 46/80\n",
      "start , epoch =  45 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2410 - acc: 0.9163 - val_loss: 0.3846 - val_acc: 0.8946\n",
      "end, epoch =  45 , lr =  0.001 , decay =  0.0\n",
      "Epoch 47/80\n",
      "start , epoch =  46 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2427 - acc: 0.9148 - val_loss: 0.4548 - val_acc: 0.8822\n",
      "end, epoch =  46 , lr =  0.001 , decay =  0.0\n",
      "Epoch 48/80\n",
      "start , epoch =  47 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.2336 - acc: 0.9177 - val_loss: 0.4337 - val_acc: 0.8833\n",
      "end, epoch =  47 , lr =  0.001 , decay =  0.0\n",
      "Epoch 49/80\n",
      "start , epoch =  48 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2321 - acc: 0.9173 - val_loss: 0.3655 - val_acc: 0.9009\n",
      "end, epoch =  48 , lr =  0.001 , decay =  0.0\n",
      "Epoch 50/80\n",
      "start , epoch =  49 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2357 - acc: 0.9162 - val_loss: 0.5058 - val_acc: 0.8730\n",
      "end, epoch =  49 , lr =  0.001 , decay =  0.0\n",
      "Epoch 51/80\n",
      "start , epoch =  50 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2363 - acc: 0.9172 - val_loss: 0.3693 - val_acc: 0.8987\n",
      "end, epoch =  50 , lr =  0.001 , decay =  0.0\n",
      "Epoch 52/80\n",
      "start , epoch =  51 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.2279 - acc: 0.9202 - val_loss: 0.3697 - val_acc: 0.9056\n",
      "end, epoch =  51 , lr =  0.001 , decay =  0.0\n",
      "Epoch 53/80\n",
      "start , epoch =  52 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2286 - acc: 0.9182 - val_loss: 0.4055 - val_acc: 0.8894\n",
      "end, epoch =  52 , lr =  0.001 , decay =  0.0\n",
      "Epoch 54/80\n",
      "start , epoch =  53 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2198 - acc: 0.9224 - val_loss: 0.4491 - val_acc: 0.8878\n",
      "end, epoch =  53 , lr =  0.001 , decay =  0.0\n",
      "Epoch 55/80\n",
      "start , epoch =  54 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 189ms/step - loss: 0.2199 - acc: 0.9237 - val_loss: 0.4610 - val_acc: 0.8812\n",
      "end, epoch =  54 , lr =  0.001 , decay =  0.0\n",
      "Epoch 56/80\n",
      "start , epoch =  55 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 190ms/step - loss: 0.2183 - acc: 0.9228 - val_loss: 0.3623 - val_acc: 0.9024\n",
      "end, epoch =  55 , lr =  0.001 , decay =  0.0\n",
      "Epoch 57/80\n",
      "start , epoch =  56 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2164 - acc: 0.9243 - val_loss: 0.3618 - val_acc: 0.8978\n",
      "end, epoch =  56 , lr =  0.001 , decay =  0.0\n",
      "Epoch 58/80\n",
      "start , epoch =  57 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.2141 - acc: 0.9243 - val_loss: 0.3725 - val_acc: 0.8993\n",
      "end, epoch =  57 , lr =  0.001 , decay =  0.0\n",
      "Epoch 59/80\n",
      "start , epoch =  58 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2090 - acc: 0.9277 - val_loss: 0.3710 - val_acc: 0.9004\n",
      "end, epoch =  58 , lr =  0.001 , decay =  0.0\n",
      "Epoch 60/80\n",
      "start , epoch =  59 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.2057 - acc: 0.9272 - val_loss: 0.3506 - val_acc: 0.9064\n",
      "end, epoch =  59 , lr =  0.001 , decay =  0.0\n",
      "Epoch 61/80\n",
      "start , epoch =  60 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 149s 190ms/step - loss: 0.2029 - acc: 0.9283 - val_loss: 0.3733 - val_acc: 0.8977\n",
      "end, epoch =  60 , lr =  0.001 , decay =  0.0\n",
      "Epoch 62/80\n",
      "start , epoch =  61 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 152s 194ms/step - loss: 0.2007 - acc: 0.9281 - val_loss: 0.3739 - val_acc: 0.8992\n",
      "end, epoch =  61 , lr =  0.001 , decay =  0.0\n",
      "Epoch 63/80\n",
      "start , epoch =  62 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 152s 194ms/step - loss: 0.2026 - acc: 0.9291 - val_loss: 0.3883 - val_acc: 0.8985\n",
      "end, epoch =  62 , lr =  0.001 , decay =  0.0\n",
      "Epoch 64/80\n",
      "start , epoch =  63 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 153s 196ms/step - loss: 0.1958 - acc: 0.9313 - val_loss: 0.4531 - val_acc: 0.8868\n",
      "end, epoch =  63 , lr =  0.001 , decay =  0.0\n",
      "Epoch 65/80\n",
      "start , epoch =  64 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 196ms/step - loss: 0.1980 - acc: 0.9307 - val_loss: 0.3777 - val_acc: 0.9033\n",
      "end, epoch =  64 , lr =  0.001 , decay =  0.0\n",
      "Epoch 66/80\n",
      "start , epoch =  65 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 151s 193ms/step - loss: 0.1934 - acc: 0.9316 - val_loss: 0.3898 - val_acc: 0.8981\n",
      "end, epoch =  65 , lr =  0.001 , decay =  0.0\n",
      "Epoch 67/80\n",
      "start , epoch =  66 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.1951 - acc: 0.9307 - val_loss: 0.4553 - val_acc: 0.8872\n",
      "end, epoch =  66 , lr =  0.001 , decay =  0.0\n",
      "Epoch 68/80\n",
      "start , epoch =  67 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.1894 - acc: 0.9320 - val_loss: 0.3559 - val_acc: 0.9080\n",
      "end, epoch =  67 , lr =  0.001 , decay =  0.0\n",
      "Epoch 69/80\n",
      "start , epoch =  68 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 149s 190ms/step - loss: 0.1927 - acc: 0.9310 - val_loss: 0.3797 - val_acc: 0.9015\n",
      "end, epoch =  68 , lr =  0.001 , decay =  0.0\n",
      "Epoch 70/80\n",
      "start , epoch =  69 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.1922 - acc: 0.9316 - val_loss: 0.3725 - val_acc: 0.9055\n",
      "end, epoch =  69 , lr =  0.001 , decay =  0.0\n",
      "Epoch 71/80\n",
      "start , epoch =  70 , lr =  0.001 , decay =  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/781 [==============================] - 149s 190ms/step - loss: 0.1861 - acc: 0.9333 - val_loss: 0.3612 - val_acc: 0.9031\n",
      "end, epoch =  70 , lr =  0.001 , decay =  0.0\n",
      "Epoch 72/80\n",
      "start , epoch =  71 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.1858 - acc: 0.9327 - val_loss: 0.4431 - val_acc: 0.8942\n",
      "end, epoch =  71 , lr =  0.001 , decay =  0.0\n",
      "Epoch 73/80\n",
      "start , epoch =  72 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.1794 - acc: 0.9366 - val_loss: 0.4052 - val_acc: 0.8973\n",
      "end, epoch =  72 , lr =  0.001 , decay =  0.0\n",
      "Epoch 74/80\n",
      "start , epoch =  73 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 147s 188ms/step - loss: 0.1829 - acc: 0.9349 - val_loss: 0.3876 - val_acc: 0.9038\n",
      "end, epoch =  73 , lr =  0.001 , decay =  0.0\n",
      "Epoch 75/80\n",
      "start , epoch =  74 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 148s 189ms/step - loss: 0.1801 - acc: 0.9360 - val_loss: 0.3590 - val_acc: 0.9059\n",
      "end, epoch =  74 , lr =  0.001 , decay =  0.0\n",
      "Epoch 76/80\n",
      "start , epoch =  75 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 159s 203ms/step - loss: 0.1775 - acc: 0.9366 - val_loss: 0.4002 - val_acc: 0.8969\n",
      "end, epoch =  75 , lr =  0.001 , decay =  0.0\n",
      "Epoch 77/80\n",
      "start , epoch =  76 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.1766 - acc: 0.9379 - val_loss: 0.3797 - val_acc: 0.9018\n",
      "end, epoch =  76 , lr =  0.001 , decay =  0.0\n",
      "Epoch 78/80\n",
      "start , epoch =  77 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 156s 200ms/step - loss: 0.1720 - acc: 0.9382 - val_loss: 0.3707 - val_acc: 0.9089\n",
      "end, epoch =  77 , lr =  0.001 , decay =  0.0\n",
      "Epoch 79/80\n",
      "start , epoch =  78 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 157s 201ms/step - loss: 0.1767 - acc: 0.9377 - val_loss: 0.3764 - val_acc: 0.9054\n",
      "end, epoch =  78 , lr =  0.001 , decay =  0.0\n",
      "Epoch 80/80\n",
      "start , epoch =  79 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 157s 200ms/step - loss: 0.1708 - acc: 0.9392 - val_loss: 0.3769 - val_acc: 0.9030\n",
      "end, epoch =  79 , lr =  0.001 , decay =  0.0\n"
     ]
    }
   ],
   "source": [
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 1.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are in the vicinity of the target\n",
    "\n",
    "- let's reduce lr\n",
    "- increace batch size (more stable gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.01)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 153s 195ms/step - loss: 0.1344 - acc: 0.9516 - val_loss: 0.3511 - val_acc: 0.9141\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/80\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.1336 - acc: 0.9522 - val_loss: 0.3545 - val_acc: 0.9145\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/80\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 159s 203ms/step - loss: 0.1302 - acc: 0.9531 - val_loss: 0.3659 - val_acc: 0.9108\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/80\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 157s 201ms/step - loss: 0.1303 - acc: 0.9531 - val_loss: 0.3929 - val_acc: 0.9035\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/80\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 161s 205ms/step - loss: 0.1295 - acc: 0.9537 - val_loss: 0.3807 - val_acc: 0.9128\n",
      "end, epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "Epoch 6/80\n",
      "start , epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "355/781 [============>.................] - ETA: 1:23 - loss: 0.1268 - acc: 0.9543"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-ec14abb7533b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 80\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 2.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 103s 264ms/step - loss: 0.1065 - acc: 0.9621 - val_loss: 0.3485 - val_acc: 0.9184\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/80\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 104s 265ms/step - loss: 0.1011 - acc: 0.9642 - val_loss: 0.3868 - val_acc: 0.9141\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/80\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 103s 264ms/step - loss: 0.0987 - acc: 0.9645 - val_loss: 0.4000 - val_acc: 0.9137\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/80\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 104s 265ms/step - loss: 0.0982 - acc: 0.9646 - val_loss: 0.3912 - val_acc: 0.9155\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/80\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 104s 267ms/step - loss: 0.1009 - acc: 0.9642 - val_loss: 0.4322 - val_acc: 0.9079\n",
      "end, epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "Epoch 6/80\n",
      "start , epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 104s 267ms/step - loss: 0.0981 - acc: 0.9651 - val_loss: 0.3672 - val_acc: 0.9177\n",
      "end, epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "Epoch 7/80\n",
      "start , epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 104s 266ms/step - loss: 0.0935 - acc: 0.9667 - val_loss: 0.3862 - val_acc: 0.9199\n",
      "end, epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "Epoch 8/80\n",
      "start , epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 103s 264ms/step - loss: 0.0985 - acc: 0.9649 - val_loss: 0.4097 - val_acc: 0.9139\n",
      "end, epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "Epoch 9/80\n",
      "start , epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 103s 264ms/step - loss: 0.0968 - acc: 0.9653 - val_loss: 0.3675 - val_acc: 0.9197\n",
      "end, epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "Epoch 10/80\n",
      "start , epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "391/390 [==============================] - 103s 263ms/step - loss: 0.0916 - acc: 0.9673 - val_loss: 0.3574 - val_acc: 0.9236\n",
      "end, epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "Epoch 11/80\n",
      "start , epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "146/390 [==========>...................] - ETA: 1:03 - loss: 0.0942 - acc: 0.9654"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-1f4d95bcecd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 80\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 2.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1781
    },
    "colab_type": "code",
    "id": "KbD-T7m0G6lP",
    "outputId": "198b9368-e44a-461d-a822-15e6901dc520"
   },
   "source": [
    "\n",
    "## We can observe that the model has  achived 92.36% accuracy (in 87 epochs, stopped it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWYRrcp7G6mv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DNST_CIFAR10_Conv_09.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
