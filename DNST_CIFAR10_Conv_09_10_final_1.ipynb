{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K70hAckqg0EA",
    "outputId": "a0dba4c3-f402-4223-ea89-3be41ca275fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "#!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "756bdmamG6f1",
    "outputId": "2f1c4951-b47b-40a0-ab02-1c97fd7c1a95"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight \n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhIk-iu4G6f-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time, pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBoa2F25G6gE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "#l = 40\n",
    "#num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWRR6JyzG6gT"
   },
   "outputs": [],
   "source": [
    "do_sub_sampling_of_input = False\n",
    "do_data_augmentation = True\n",
    "do_data_append       = False   #2X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "16a221e4-1075-49d1-9d76-8091d240f280"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_hYUAbsG6ge"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "class_name = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "00iNYJLkG6gj",
    "outputId": "4911e408-9a52-4009-eefd-44e5242fb219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6], dtype=uint8), 32, 32, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], img_height, img_width, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS5X5srxG6gp"
   },
   "outputs": [],
   "source": [
    "def draw_img(i, x_train, y_train, class_name):\n",
    "    im = x_train[i]\n",
    "    c = y_train[i]\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"Class %d (%s)\" % (int(c), class_name[int(c)]))\n",
    "    plt.axis('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "mjF2KvIQG6gw",
    "outputId": "6c7a984d-4db5-4d5b-d110-b740edd41a02"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_img(7, x_train, y_train, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to dispaly and analyze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions\n",
    "\n",
    "plot_confusion_matrix(): helps us to plot the confusion matrix. \n",
    "It is taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html. \n",
    "The same page has some sample examples on how to use this function \n",
    "cm: Confusion matrix calcualted using confusion_matrix() from sklearn.metrics <br//> classes: a list of labels for the classes we are plotting\n",
    "normalize=False: True means we will plot nomalized values \n",
    "title='Confusion matrix': set the tiltle of the plot \n",
    "cmap : leave it as it is \n",
    "Example Usage:plot_confusion_matrix(cm, classes=Facial_Expressions, normalize=True, title='Test Data - Using Simple Average Ensembling ')\n",
    "\n",
    "plot_histogram(): helps to plot the histogram of a list \n",
    "lst_data: the list whose histogtam we want to plot , \n",
    "class_labels: a list of labels for the classes we are plotting \n",
    "ylabel='None': set the y label of the plot, x label is always frequency \n",
    "title='None': set the tiltle of the plot -lst_data, class_labels, ylabel='None', title='None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm) #to print in text if needed\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_histogram(lst_data, class_labels, ylabel='None', title='None'):\n",
    "    data = pd.Series(lst_data)\n",
    "    distribution = data.value_counts(sort=False)\n",
    "    y_pos = np.arange(len(class_labels))\n",
    "    \n",
    "    plt.bar(y_pos, distribution, align='center', alpha=0.8)\n",
    "    plt.xticks(y_pos, class_labels)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = ['airplane', 'automobile', 'bird','cat', 'deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(list(y_train), y_classes, ylabel='Frequency',title='CIFAR 10 Class Distribution in Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(list(y_test), y_classes, ylabel='Frequency',title='CIFAR 10 Class Distribution in Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmdwlu8pG6g3"
   },
   "outputs": [],
   "source": [
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JGd4ezgG6g7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "z_arXgOSG6hA",
    "outputId": "8960acf7-335b-4c96-9ae9-7fe9b9ad56fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  62,  63],\n",
       "       [ 43,  46,  45],\n",
       "       [ 50,  48,  43],\n",
       "       [ 68,  54,  42],\n",
       "       [ 98,  73,  52],\n",
       "       [119,  91,  63],\n",
       "       [139, 107,  75],\n",
       "       [145, 110,  80],\n",
       "       [149, 117,  89],\n",
       "       [149, 120,  93],\n",
       "       [131, 103,  77],\n",
       "       [125,  99,  76],\n",
       "       [142, 115,  91],\n",
       "       [144, 112,  86],\n",
       "       [137, 105,  79],\n",
       "       [129,  97,  71],\n",
       "       [137, 106,  79],\n",
       "       [134, 106,  76],\n",
       "       [124,  97,  64],\n",
       "       [139, 113,  78],\n",
       "       [139, 112,  75],\n",
       "       [133, 105,  69],\n",
       "       [136, 105,  74],\n",
       "       [139, 108,  77],\n",
       "       [152, 120,  89],\n",
       "       [163, 131, 100],\n",
       "       [168, 136, 108],\n",
       "       [159, 129, 102],\n",
       "       [158, 130, 104],\n",
       "       [158, 132, 108],\n",
       "       [152, 125, 102],\n",
       "       [148, 124, 103]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train[0][0:32][0:32][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "jd2usuXDG6hH",
    "outputId": "a0716d82-94e5-4f1b-ba28-39b1cdced01b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "T372AU6RG6hN",
    "outputId": "4739d668-5818-4f70-e6d3-3491edcb274c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Mean :  [[[[125.3069  122.95015 113.866  ]]]]\n",
      "Channel Std :  [[[[62.99325  62.088604 66.70501 ]]]]\n",
      "Channel Mean1 :  [[[[126.02428 123.70843 114.85442]]]]\n",
      "Channel Std1 :  [[[[62.896416 61.937508 66.70607 ]]]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "mean  = np.mean(x_train, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "std   = np.std(x_train, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "\n",
    "mean1  = np.mean(x_test, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "std1   = np.std(x_test, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "\n",
    "\n",
    "print(\"Channel Mean : \", mean)\n",
    "print(\"Channel Std : \", std)\n",
    "print(\"Channel Mean1 : \", mean1)\n",
    "print(\"Channel Std1 : \", std1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = (x_train - mean) / (std)\n",
    "x_test  = (x_test - mean1) / (std1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOCMAYM1G6hT"
   },
   "outputs": [],
   "source": [
    "#x_train = (x_train - 127.5)/255.0\n",
    "#x_test  = (x_test  - 127.5)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZF74U6zcG6hX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.0526032e+00, -9.8166406e-01, -7.6255137e-01],\n",
       "        [-1.3065987e+00, -1.2393603e+00, -1.0323962e+00],\n",
       "        [-1.1954757e+00, -1.2071482e+00, -1.0623789e+00],\n",
       "        ...,\n",
       "        [ 5.1899368e-01,  1.4575703e-01, -8.7939382e-02],\n",
       "        [ 4.2374539e-01,  3.3014923e-02, -1.7788765e-01],\n",
       "        [ 3.6024651e-01,  1.6908908e-02, -1.6289628e-01]],\n",
       "\n",
       "       [[-1.7352160e+00, -1.6581167e+00, -1.4071807e+00],\n",
       "        [-1.9892114e+00, -1.9802370e+00, -1.7070082e+00],\n",
       "        [-1.7034665e+00, -1.8513888e+00, -1.7070082e+00],\n",
       "        ...,\n",
       "        [-3.6621384e-02, -5.6290764e-01, -8.8248241e-01],\n",
       "        [-1.0012025e-01, -6.4343774e-01, -9.5743930e-01],\n",
       "        [-5.2496098e-02, -5.7901365e-01, -8.5249966e-01]],\n",
       "\n",
       "       [[-1.5923436e+00, -1.5936927e+00, -1.3921893e+00],\n",
       "        [-1.7352160e+00, -1.8674948e+00, -1.7070082e+00],\n",
       "        [-1.2113504e+00, -1.5453745e+00, -1.5870771e+00],\n",
       "        ...,\n",
       "        [-1.1599497e-01, -6.2733167e-01, -9.5743930e-01],\n",
       "        [-8.4245533e-02, -6.2733167e-01, -9.5743930e-01],\n",
       "        [-2.5886741e-01, -8.0449790e-01, -1.0773703e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.3127295e+00,  7.5778562e-01, -2.6783592e-01],\n",
       "        [ 1.2016065e+00,  4.8398334e-01, -1.1973014e+00],\n",
       "        [ 1.1539823e+00,  6.1283147e-01, -1.3172324e+00],\n",
       "        ...,\n",
       "        [ 5.5074310e-01,  1.6186304e-01, -6.5761173e-01],\n",
       "        [-1.1002274e+00, -1.4809505e+00, -1.6020685e+00],\n",
       "        [-1.1478515e+00, -1.4326324e+00, -1.4071807e+00]],\n",
       "\n",
       "       [[ 8.6823744e-01,  2.5849915e-01, -2.6783592e-01],\n",
       "        [ 7.5711441e-01,  8.0289232e-04, -1.0773703e+00],\n",
       "        [ 9.6348572e-01,  3.3902922e-01, -1.2572669e+00],\n",
       "        ...,\n",
       "        [ 9.3173629e-01,  4.0345326e-01, -2.9781866e-01],\n",
       "        [-4.4936401e-01, -9.8166406e-01, -1.1973014e+00],\n",
       "        [-6.7161006e-01, -1.1266181e+00, -1.1973014e+00]],\n",
       "\n",
       "       [[ 8.2061332e-01,  3.3902922e-01,  3.1991642e-02],\n",
       "        [ 6.7774087e-01,  9.7438984e-02, -2.9781866e-01],\n",
       "        [ 8.5236275e-01,  3.0681717e-01, -4.0275833e-01],\n",
       "        ...,\n",
       "        [ 1.4397272e+00,  9.8326981e-01,  3.9178470e-01],\n",
       "        [ 4.0787068e-01, -7.9727180e-02, -4.4773245e-01],\n",
       "        [-3.6621384e-02, -4.9848357e-01, -6.2762898e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "acEl5EjAG6hd",
    "outputId": "9efce3d3-3074-4f2e-b660-4c6b2f0caf20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XuGiXp6rG6hk",
    "outputId": "acc28bb5-470e-48c3-8e6c-354db61c19f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb0EH5H_G6hq"
   },
   "outputs": [],
   "source": [
    "if do_sub_sampling_of_input:\n",
    "    x_, x_train, x_, y_train    = cross_validation.train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
    "    x_, x_test,  y_, y_test    = cross_validation.train_test_split(x_test, y_test, test_size=0.25, random_state=0)\n",
    "    print(\"After SubSampling\")\n",
    "    print(x_train.shape, x_test.shape)\n",
    "    print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jz7L_trG6hv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if do_data_append :\n",
    "    print(\"Doing Data Appending\")\n",
    "    x_train = np.append(x_train, x_train,axis=0)\n",
    "    y_train = np.append(y_train, y_train,axis=0)\n",
    "#print(np.append(x_train, x_train,axis=0).shape)\n",
    "#print(np.append(y_train, y_train,axis=0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MkHX4hNLG6h0",
    "outputId": "25cf1428-1767-461c-ced2-577e40a2dec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "(50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMuvM67DG6h8"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npxonLr5G6h-"
   },
   "outputs": [],
   "source": [
    "keras.utils.Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.25,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.25,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,    # randomly flip images\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Tk4q4iYG6iD"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SeparableConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, growth_rate, dropout_rate = 0.2, l = 0):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        #Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        #Conv2D_1_1 = Conv2D(int(num_filter*4*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "        Conv2D_1_1 = Conv2D(int(growth_rate*4), (1,1), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_1_1 = Dropout(dropout_rate)(Conv2D_1_1)\n",
    "        BatchNorm_1_1 = BatchNormalization()(Conv2D_1_1)\n",
    "        relu_1_1 = Activation('relu')(BatchNorm_1_1)\n",
    "        \n",
    "        #Conv2D_3_3 = Conv2D(int(growth_rate), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        Conv2D_3_3 = SeparableConv2D(int(growth_rate), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        #Conv2D_3_3 = SeparableConv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        #Conv2D_3_3 = SeparableConv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eEu8gikG6iP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    num_channels = int(input.shape[-1]) #assuming it is tensor\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_channels*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    \n",
    "    #Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    #flat = Dropout(0.25)(flat)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbzC-GOZG6ie"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "num_filter = growth_rate = 12\n",
    "dropout_rate = 0.2\n",
    "compression = 0.5\n",
    "l = 16\n",
    "\n",
    "dense_l= [8, 16, 20, 12]\n",
    "dense_l= [12, 12, 12, 12]\n",
    "dense_l= [14, 14, 14, 14]\n",
    "dense_l= [16, 16, 18, 16]\n",
    "\n",
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(2*num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, growth_rate, dropout_rate, dense_l[0])\n",
    "First_Transition = add_transition(First_Block, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, growth_rate, dropout_rate, dense_l[1])\n",
    "Second_Transition = add_transition(Second_Block, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, growth_rate, dropout_rate, dense_l[2])\n",
    "Third_Transition = add_transition(Third_Block, dropout_rate)\n",
    "\n",
    "Last_Block = add_denseblock(Third_Transition,  growth_rate, dropout_rate, dense_l[3])\n",
    "output = output_layer(Last_Block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "p9uA_iR8G6io",
    "outputId": "87a7b724-7e86-416d-e7e9-6526b4bc860b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(?, 32, 32, 24)\n",
      "(?, 32, 32, 216) 216\n",
      "(?, 16, 16, 108)\n",
      "(?, 16, 16, 300)\n",
      "(?, 8, 8, 150)\n",
      "(?, 8, 8, 366)\n",
      "(?, 4, 4, 183)\n",
      "(?, 4, 4, 375)\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "print(First_Conv2D.shape)\n",
    "print(First_Block.shape, First_Block.shape[-1])\n",
    "print(First_Transition.shape)\n",
    "\n",
    "print(Second_Block.shape)\n",
    "print(Second_Transition.shape)\n",
    "\n",
    "print(Third_Block.shape)\n",
    "print(Third_Transition.shape)\n",
    "\n",
    "print(Last_Block.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 19618
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "7bc989ab-6517-466c-8120-25c63379a4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)             (None, 32, 32, 24)   648         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 32, 32, 24)   96          conv2d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 32, 32, 24)   0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)             (None, 32, 32, 48)   1152        activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_549 (Dropout)           (None, 32, 32, 48)   0           conv2d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 32, 32, 48)   192         dropout_549[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 32, 32, 48)   0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_269 (Separable (None, 32, 32, 12)   1008        activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_550 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_269[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_269 (Concatenate)   (None, 32, 32, 36)   0           conv2d_285[0][0]                 \n",
      "                                                                 dropout_550[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 32, 32, 36)   144         concatenate_269[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 32, 32, 36)   0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)             (None, 32, 32, 48)   1728        activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_551 (Dropout)           (None, 32, 32, 48)   0           conv2d_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 32, 32, 48)   192         dropout_551[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 32, 32, 48)   0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_270 (Separable (None, 32, 32, 12)   1008        activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_552 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_270[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_270 (Concatenate)   (None, 32, 32, 48)   0           concatenate_269[0][0]            \n",
      "                                                                 dropout_552[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 32, 32, 48)   192         concatenate_270[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 32, 32, 48)   0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)             (None, 32, 32, 48)   2304        activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_553 (Dropout)           (None, 32, 32, 48)   0           conv2d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 32, 32, 48)   192         dropout_553[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 32, 32, 48)   0           batch_normalization_558[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_271 (Separable (None, 32, 32, 12)   1008        activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_554 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_271[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_271 (Concatenate)   (None, 32, 32, 60)   0           concatenate_270[0][0]            \n",
      "                                                                 dropout_554[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 32, 32, 60)   240         concatenate_271[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 32, 32, 60)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_289 (Conv2D)             (None, 32, 32, 48)   2880        activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_555 (Dropout)           (None, 32, 32, 48)   0           conv2d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 32, 32, 48)   192         dropout_555[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 32, 32, 48)   0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_272 (Separable (None, 32, 32, 12)   1008        activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_556 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_272[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_272 (Concatenate)   (None, 32, 32, 72)   0           concatenate_271[0][0]            \n",
      "                                                                 dropout_556[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 32, 32, 72)   288         concatenate_272[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 32, 32, 72)   0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_290 (Conv2D)             (None, 32, 32, 48)   3456        activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_557 (Dropout)           (None, 32, 32, 48)   0           conv2d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 32, 32, 48)   192         dropout_557[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 32, 32, 48)   0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_273 (Separable (None, 32, 32, 12)   1008        activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_558 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_273[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_273 (Concatenate)   (None, 32, 32, 84)   0           concatenate_272[0][0]            \n",
      "                                                                 dropout_558[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 32, 32, 84)   336         concatenate_273[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 32, 32, 84)   0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_291 (Conv2D)             (None, 32, 32, 48)   4032        activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_559 (Dropout)           (None, 32, 32, 48)   0           conv2d_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 32, 32, 48)   192         dropout_559[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 32, 32, 48)   0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_274 (Separable (None, 32, 32, 12)   1008        activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_560 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_274[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_274 (Concatenate)   (None, 32, 32, 96)   0           concatenate_273[0][0]            \n",
      "                                                                 dropout_560[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 32, 32, 96)   384         concatenate_274[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 32, 32, 96)   0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_292 (Conv2D)             (None, 32, 32, 48)   4608        activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_561 (Dropout)           (None, 32, 32, 48)   0           conv2d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 32, 32, 48)   192         dropout_561[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 32, 32, 48)   0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_275 (Separable (None, 32, 32, 12)   1008        activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_562 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_275[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_275 (Concatenate)   (None, 32, 32, 108)  0           concatenate_274[0][0]            \n",
      "                                                                 dropout_562[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 32, 32, 108)  432         concatenate_275[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 32, 32, 108)  0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_293 (Conv2D)             (None, 32, 32, 48)   5184        activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_563 (Dropout)           (None, 32, 32, 48)   0           conv2d_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 32, 32, 48)   192         dropout_563[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 32, 32, 48)   0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_276 (Separable (None, 32, 32, 12)   1008        activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_564 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_276[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_276 (Concatenate)   (None, 32, 32, 120)  0           concatenate_275[0][0]            \n",
      "                                                                 dropout_564[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 32, 32, 120)  480         concatenate_276[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 32, 32, 120)  0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_294 (Conv2D)             (None, 32, 32, 48)   5760        activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_565 (Dropout)           (None, 32, 32, 48)   0           conv2d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 32, 32, 48)   192         dropout_565[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 32, 32, 48)   0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_277 (Separable (None, 32, 32, 12)   1008        activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_566 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_277[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_277 (Concatenate)   (None, 32, 32, 132)  0           concatenate_276[0][0]            \n",
      "                                                                 dropout_566[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 32, 32, 132)  528         concatenate_277[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 32, 32, 132)  0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 32, 32, 48)   6336        activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_567 (Dropout)           (None, 32, 32, 48)   0           conv2d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 32, 32, 48)   192         dropout_567[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 32, 32, 48)   0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_278 (Separable (None, 32, 32, 12)   1008        activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_568 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_278[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_278 (Concatenate)   (None, 32, 32, 144)  0           concatenate_277[0][0]            \n",
      "                                                                 dropout_568[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 32, 32, 144)  576         concatenate_278[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 32, 32, 144)  0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 32, 32, 48)   6912        activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_569 (Dropout)           (None, 32, 32, 48)   0           conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 32, 32, 48)   192         dropout_569[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 32, 32, 48)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_279 (Separable (None, 32, 32, 12)   1008        activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_570 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_279[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_279 (Concatenate)   (None, 32, 32, 156)  0           concatenate_278[0][0]            \n",
      "                                                                 dropout_570[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 32, 32, 156)  624         concatenate_279[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 32, 32, 156)  0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 32, 32, 48)   7488        activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_571 (Dropout)           (None, 32, 32, 48)   0           conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 32, 32, 48)   192         dropout_571[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 32, 32, 48)   0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_280 (Separable (None, 32, 32, 12)   1008        activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_572 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_280[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_280 (Concatenate)   (None, 32, 32, 168)  0           concatenate_279[0][0]            \n",
      "                                                                 dropout_572[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 32, 32, 168)  672         concatenate_280[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 32, 32, 168)  0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 32, 32, 48)   8064        activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_573 (Dropout)           (None, 32, 32, 48)   0           conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 32, 32, 48)   192         dropout_573[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 32, 32, 48)   0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_281 (Separable (None, 32, 32, 12)   1008        activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_574 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_281[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_281 (Concatenate)   (None, 32, 32, 180)  0           concatenate_280[0][0]            \n",
      "                                                                 dropout_574[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 32, 32, 180)  720         concatenate_281[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 32, 32, 180)  0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 32, 32, 48)   8640        activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_575 (Dropout)           (None, 32, 32, 48)   0           conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 32, 32, 48)   192         dropout_575[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 32, 32, 48)   0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_282 (Separable (None, 32, 32, 12)   1008        activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_576 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_282[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_282 (Concatenate)   (None, 32, 32, 192)  0           concatenate_281[0][0]            \n",
      "                                                                 dropout_576[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 32, 32, 192)  768         concatenate_282[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 32, 32, 192)  0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 32, 32, 48)   9216        activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_577 (Dropout)           (None, 32, 32, 48)   0           conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 32, 32, 48)   192         dropout_577[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 32, 32, 48)   0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_283 (Separable (None, 32, 32, 12)   1008        activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_578 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_283[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_283 (Concatenate)   (None, 32, 32, 204)  0           concatenate_282[0][0]            \n",
      "                                                                 dropout_578[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 32, 32, 204)  816         concatenate_283[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 32, 32, 204)  0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 32, 32, 48)   9792        activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_579 (Dropout)           (None, 32, 32, 48)   0           conv2d_301[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 32, 32, 48)   192         dropout_579[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 32, 32, 48)   0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_284 (Separable (None, 32, 32, 12)   1008        activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_580 (Dropout)           (None, 32, 32, 12)   0           separable_conv2d_284[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_284 (Concatenate)   (None, 32, 32, 216)  0           concatenate_283[0][0]            \n",
      "                                                                 dropout_580[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 32, 32, 216)  864         concatenate_284[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 32, 32, 216)  0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 32, 32, 108)  23328       activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_581 (Dropout)           (None, 32, 32, 108)  0           conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 16, 16, 108)  0           dropout_581[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 16, 16, 108)  432         average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 16, 16, 108)  0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 16, 16, 48)   5184        activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_582 (Dropout)           (None, 16, 16, 48)   0           conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 16, 16, 48)   192         dropout_582[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 16, 16, 48)   0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_285 (Separable (None, 16, 16, 12)   1008        activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_583 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_285[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_285 (Concatenate)   (None, 16, 16, 120)  0           average_pooling2d_17[0][0]       \n",
      "                                                                 dropout_583[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 16, 16, 120)  480         concatenate_285[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 16, 16, 120)  0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 16, 16, 48)   5760        activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_584 (Dropout)           (None, 16, 16, 48)   0           conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 16, 16, 48)   192         dropout_584[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 16, 16, 48)   0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_286 (Separable (None, 16, 16, 12)   1008        activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_585 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_286[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_286 (Concatenate)   (None, 16, 16, 132)  0           concatenate_285[0][0]            \n",
      "                                                                 dropout_585[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 16, 16, 132)  528         concatenate_286[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 16, 16, 132)  0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 16, 16, 48)   6336        activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_586 (Dropout)           (None, 16, 16, 48)   0           conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 16, 16, 48)   192         dropout_586[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 16, 16, 48)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_287 (Separable (None, 16, 16, 12)   1008        activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_587 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_287[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_287 (Concatenate)   (None, 16, 16, 144)  0           concatenate_286[0][0]            \n",
      "                                                                 dropout_587[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 16, 16, 144)  576         concatenate_287[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 16, 16, 144)  0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 16, 16, 48)   6912        activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_588 (Dropout)           (None, 16, 16, 48)   0           conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 16, 16, 48)   192         dropout_588[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 16, 16, 48)   0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_288 (Separable (None, 16, 16, 12)   1008        activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_589 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_288[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_288 (Concatenate)   (None, 16, 16, 156)  0           concatenate_287[0][0]            \n",
      "                                                                 dropout_589[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 16, 16, 156)  624         concatenate_288[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 16, 16, 156)  0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 16, 16, 48)   7488        activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_590 (Dropout)           (None, 16, 16, 48)   0           conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 16, 16, 48)   192         dropout_590[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 16, 16, 48)   0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_289 (Separable (None, 16, 16, 12)   1008        activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_591 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_289[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_289 (Concatenate)   (None, 16, 16, 168)  0           concatenate_288[0][0]            \n",
      "                                                                 dropout_591[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 16, 16, 168)  672         concatenate_289[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 16, 16, 168)  0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 16, 16, 48)   8064        activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_592 (Dropout)           (None, 16, 16, 48)   0           conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 16, 16, 48)   192         dropout_592[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 16, 16, 48)   0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_290 (Separable (None, 16, 16, 12)   1008        activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_593 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_290[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_290 (Concatenate)   (None, 16, 16, 180)  0           concatenate_289[0][0]            \n",
      "                                                                 dropout_593[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 16, 16, 180)  720         concatenate_290[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_598 (Activation)     (None, 16, 16, 180)  0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 16, 16, 48)   8640        activation_598[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_594 (Dropout)           (None, 16, 16, 48)   0           conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 16, 16, 48)   192         dropout_594[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_599 (Activation)     (None, 16, 16, 48)   0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_291 (Separable (None, 16, 16, 12)   1008        activation_599[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_595 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_291[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_291 (Concatenate)   (None, 16, 16, 192)  0           concatenate_290[0][0]            \n",
      "                                                                 dropout_595[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 16, 16, 192)  768         concatenate_291[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_600 (Activation)     (None, 16, 16, 192)  0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 16, 16, 48)   9216        activation_600[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_596 (Dropout)           (None, 16, 16, 48)   0           conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 16, 16, 48)   192         dropout_596[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_601 (Activation)     (None, 16, 16, 48)   0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_292 (Separable (None, 16, 16, 12)   1008        activation_601[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_597 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_292[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_292 (Concatenate)   (None, 16, 16, 204)  0           concatenate_291[0][0]            \n",
      "                                                                 dropout_597[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 16, 16, 204)  816         concatenate_292[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_602 (Activation)     (None, 16, 16, 204)  0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 16, 16, 48)   9792        activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_598 (Dropout)           (None, 16, 16, 48)   0           conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 16, 16, 48)   192         dropout_598[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_603 (Activation)     (None, 16, 16, 48)   0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_293 (Separable (None, 16, 16, 12)   1008        activation_603[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_599 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_293[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_293 (Concatenate)   (None, 16, 16, 216)  0           concatenate_292[0][0]            \n",
      "                                                                 dropout_599[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 16, 16, 216)  864         concatenate_293[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_604 (Activation)     (None, 16, 16, 216)  0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 16, 16, 48)   10368       activation_604[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_600 (Dropout)           (None, 16, 16, 48)   0           conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 16, 16, 48)   192         dropout_600[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_605 (Activation)     (None, 16, 16, 48)   0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_294 (Separable (None, 16, 16, 12)   1008        activation_605[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_601 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_294[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_294 (Concatenate)   (None, 16, 16, 228)  0           concatenate_293[0][0]            \n",
      "                                                                 dropout_601[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 16, 16, 228)  912         concatenate_294[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_606 (Activation)     (None, 16, 16, 228)  0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 16, 16, 48)   10944       activation_606[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_602 (Dropout)           (None, 16, 16, 48)   0           conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 16, 16, 48)   192         dropout_602[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_607 (Activation)     (None, 16, 16, 48)   0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_295 (Separable (None, 16, 16, 12)   1008        activation_607[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_603 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_295[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_295 (Concatenate)   (None, 16, 16, 240)  0           concatenate_294[0][0]            \n",
      "                                                                 dropout_603[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 16, 16, 240)  960         concatenate_295[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_608 (Activation)     (None, 16, 16, 240)  0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 16, 16, 48)   11520       activation_608[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_604 (Dropout)           (None, 16, 16, 48)   0           conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 16, 16, 48)   192         dropout_604[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_609 (Activation)     (None, 16, 16, 48)   0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_296 (Separable (None, 16, 16, 12)   1008        activation_609[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_605 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_296[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_296 (Concatenate)   (None, 16, 16, 252)  0           concatenate_295[0][0]            \n",
      "                                                                 dropout_605[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_610 (BatchN (None, 16, 16, 252)  1008        concatenate_296[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_610 (Activation)     (None, 16, 16, 252)  0           batch_normalization_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 16, 16, 48)   12096       activation_610[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_606 (Dropout)           (None, 16, 16, 48)   0           conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_611 (BatchN (None, 16, 16, 48)   192         dropout_606[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_611 (Activation)     (None, 16, 16, 48)   0           batch_normalization_611[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_297 (Separable (None, 16, 16, 12)   1008        activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_607 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_297[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_297 (Concatenate)   (None, 16, 16, 264)  0           concatenate_296[0][0]            \n",
      "                                                                 dropout_607[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_612 (BatchN (None, 16, 16, 264)  1056        concatenate_297[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_612 (Activation)     (None, 16, 16, 264)  0           batch_normalization_612[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 16, 16, 48)   12672       activation_612[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_608 (Dropout)           (None, 16, 16, 48)   0           conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_613 (BatchN (None, 16, 16, 48)   192         dropout_608[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_613 (Activation)     (None, 16, 16, 48)   0           batch_normalization_613[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_298 (Separable (None, 16, 16, 12)   1008        activation_613[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_609 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_298[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_298 (Concatenate)   (None, 16, 16, 276)  0           concatenate_297[0][0]            \n",
      "                                                                 dropout_609[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_614 (BatchN (None, 16, 16, 276)  1104        concatenate_298[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 16, 16, 276)  0           batch_normalization_614[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 16, 16, 48)   13248       activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_610 (Dropout)           (None, 16, 16, 48)   0           conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchN (None, 16, 16, 48)   192         dropout_610[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 16, 16, 48)   0           batch_normalization_615[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_299 (Separable (None, 16, 16, 12)   1008        activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_611 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_299[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_299 (Concatenate)   (None, 16, 16, 288)  0           concatenate_298[0][0]            \n",
      "                                                                 dropout_611[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchN (None, 16, 16, 288)  1152        concatenate_299[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 16, 16, 288)  0           batch_normalization_616[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 16, 16, 48)   13824       activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_612 (Dropout)           (None, 16, 16, 48)   0           conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_617 (BatchN (None, 16, 16, 48)   192         dropout_612[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 16, 16, 48)   0           batch_normalization_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_300 (Separable (None, 16, 16, 12)   1008        activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_613 (Dropout)           (None, 16, 16, 12)   0           separable_conv2d_300[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_300 (Concatenate)   (None, 16, 16, 300)  0           concatenate_299[0][0]            \n",
      "                                                                 dropout_613[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_618 (BatchN (None, 16, 16, 300)  1200        concatenate_300[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 16, 16, 300)  0           batch_normalization_618[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 16, 16, 150)  45000       activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_614 (Dropout)           (None, 16, 16, 150)  0           conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 8, 8, 150)    0           dropout_614[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchN (None, 8, 8, 150)    600         average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 8, 8, 150)    0           batch_normalization_619[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 8, 8, 48)     7200        activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_615 (Dropout)           (None, 8, 8, 48)     0           conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchN (None, 8, 8, 48)     192         dropout_615[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 8, 8, 48)     0           batch_normalization_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_301 (Separable (None, 8, 8, 12)     1008        activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_616 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_301[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_301 (Concatenate)   (None, 8, 8, 162)    0           average_pooling2d_18[0][0]       \n",
      "                                                                 dropout_616[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_621 (BatchN (None, 8, 8, 162)    648         concatenate_301[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 8, 8, 162)    0           batch_normalization_621[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 8, 8, 48)     7776        activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_617 (Dropout)           (None, 8, 8, 48)     0           conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_622 (BatchN (None, 8, 8, 48)     192         dropout_617[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 8, 8, 48)     0           batch_normalization_622[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_302 (Separable (None, 8, 8, 12)     1008        activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_618 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_302[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_302 (Concatenate)   (None, 8, 8, 174)    0           concatenate_301[0][0]            \n",
      "                                                                 dropout_618[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_623 (BatchN (None, 8, 8, 174)    696         concatenate_302[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 8, 8, 174)    0           batch_normalization_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 8, 8, 48)     8352        activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_619 (Dropout)           (None, 8, 8, 48)     0           conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_624 (BatchN (None, 8, 8, 48)     192         dropout_619[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 8, 8, 48)     0           batch_normalization_624[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_303 (Separable (None, 8, 8, 12)     1008        activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_620 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_303[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_303 (Concatenate)   (None, 8, 8, 186)    0           concatenate_302[0][0]            \n",
      "                                                                 dropout_620[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_625 (BatchN (None, 8, 8, 186)    744         concatenate_303[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 8, 8, 186)    0           batch_normalization_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 8, 8, 48)     8928        activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_621 (Dropout)           (None, 8, 8, 48)     0           conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_626 (BatchN (None, 8, 8, 48)     192         dropout_621[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 8, 8, 48)     0           batch_normalization_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_304 (Separable (None, 8, 8, 12)     1008        activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_622 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_304[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_304 (Concatenate)   (None, 8, 8, 198)    0           concatenate_303[0][0]            \n",
      "                                                                 dropout_622[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_627 (BatchN (None, 8, 8, 198)    792         concatenate_304[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 8, 8, 198)    0           batch_normalization_627[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 8, 8, 48)     9504        activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_623 (Dropout)           (None, 8, 8, 48)     0           conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 8, 8, 48)     192         dropout_623[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 8, 8, 48)     0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_305 (Separable (None, 8, 8, 12)     1008        activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_624 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_305[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_305 (Concatenate)   (None, 8, 8, 210)    0           concatenate_304[0][0]            \n",
      "                                                                 dropout_624[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 8, 8, 210)    840         concatenate_305[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 8, 8, 210)    0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 8, 8, 48)     10080       activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_625 (Dropout)           (None, 8, 8, 48)     0           conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 8, 8, 48)     192         dropout_625[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 8, 8, 48)     0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_306 (Separable (None, 8, 8, 12)     1008        activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_626 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_306[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_306 (Concatenate)   (None, 8, 8, 222)    0           concatenate_305[0][0]            \n",
      "                                                                 dropout_626[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 8, 8, 222)    888         concatenate_306[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 8, 8, 222)    0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 8, 8, 48)     10656       activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_627 (Dropout)           (None, 8, 8, 48)     0           conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 8, 8, 48)     192         dropout_627[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 8, 8, 48)     0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_307 (Separable (None, 8, 8, 12)     1008        activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_628 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_307[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_307 (Concatenate)   (None, 8, 8, 234)    0           concatenate_306[0][0]            \n",
      "                                                                 dropout_628[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 8, 8, 234)    936         concatenate_307[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 8, 8, 234)    0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 8, 8, 48)     11232       activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_629 (Dropout)           (None, 8, 8, 48)     0           conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 8, 8, 48)     192         dropout_629[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 8, 8, 48)     0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_308 (Separable (None, 8, 8, 12)     1008        activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_630 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_308[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_308 (Concatenate)   (None, 8, 8, 246)    0           concatenate_307[0][0]            \n",
      "                                                                 dropout_630[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 8, 8, 246)    984         concatenate_308[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 8, 8, 246)    0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 8, 8, 48)     11808       activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_631 (Dropout)           (None, 8, 8, 48)     0           conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 8, 8, 48)     192         dropout_631[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 8, 8, 48)     0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_309 (Separable (None, 8, 8, 12)     1008        activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_632 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_309[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_309 (Concatenate)   (None, 8, 8, 258)    0           concatenate_308[0][0]            \n",
      "                                                                 dropout_632[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 8, 8, 258)    1032        concatenate_309[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 8, 8, 258)    0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 8, 8, 48)     12384       activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_633 (Dropout)           (None, 8, 8, 48)     0           conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 8, 8, 48)     192         dropout_633[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 8, 8, 48)     0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_310 (Separable (None, 8, 8, 12)     1008        activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_634 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_310[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_310 (Concatenate)   (None, 8, 8, 270)    0           concatenate_309[0][0]            \n",
      "                                                                 dropout_634[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 8, 8, 270)    1080        concatenate_310[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 8, 8, 270)    0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 8, 8, 48)     12960       activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_635 (Dropout)           (None, 8, 8, 48)     0           conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 8, 8, 48)     192         dropout_635[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 8, 8, 48)     0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_311 (Separable (None, 8, 8, 12)     1008        activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_636 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_311[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_311 (Concatenate)   (None, 8, 8, 282)    0           concatenate_310[0][0]            \n",
      "                                                                 dropout_636[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 8, 8, 282)    1128        concatenate_311[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 8, 8, 282)    0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 8, 8, 48)     13536       activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_637 (Dropout)           (None, 8, 8, 48)     0           conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 8, 8, 48)     192         dropout_637[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 8, 8, 48)     0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_312 (Separable (None, 8, 8, 12)     1008        activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_638 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_312[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_312 (Concatenate)   (None, 8, 8, 294)    0           concatenate_311[0][0]            \n",
      "                                                                 dropout_638[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 8, 8, 294)    1176        concatenate_312[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 8, 8, 294)    0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 8, 8, 48)     14112       activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_639 (Dropout)           (None, 8, 8, 48)     0           conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 8, 8, 48)     192         dropout_639[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 8, 8, 48)     0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_313 (Separable (None, 8, 8, 12)     1008        activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_640 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_313[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_313 (Concatenate)   (None, 8, 8, 306)    0           concatenate_312[0][0]            \n",
      "                                                                 dropout_640[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 8, 8, 306)    1224        concatenate_313[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 8, 8, 306)    0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 8, 8, 48)     14688       activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_641 (Dropout)           (None, 8, 8, 48)     0           conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 8, 8, 48)     192         dropout_641[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 8, 8, 48)     0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_314 (Separable (None, 8, 8, 12)     1008        activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_642 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_314[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_314 (Concatenate)   (None, 8, 8, 318)    0           concatenate_313[0][0]            \n",
      "                                                                 dropout_642[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 8, 8, 318)    1272        concatenate_314[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 8, 8, 318)    0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 8, 8, 48)     15264       activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_643 (Dropout)           (None, 8, 8, 48)     0           conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 8, 8, 48)     192         dropout_643[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_648 (Activation)     (None, 8, 8, 48)     0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_315 (Separable (None, 8, 8, 12)     1008        activation_648[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_644 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_315[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_315 (Concatenate)   (None, 8, 8, 330)    0           concatenate_314[0][0]            \n",
      "                                                                 dropout_644[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 8, 8, 330)    1320        concatenate_315[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_649 (Activation)     (None, 8, 8, 330)    0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 8, 8, 48)     15840       activation_649[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_645 (Dropout)           (None, 8, 8, 48)     0           conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 8, 8, 48)     192         dropout_645[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_650 (Activation)     (None, 8, 8, 48)     0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_316 (Separable (None, 8, 8, 12)     1008        activation_650[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_646 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_316[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_316 (Concatenate)   (None, 8, 8, 342)    0           concatenate_315[0][0]            \n",
      "                                                                 dropout_646[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 8, 8, 342)    1368        concatenate_316[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_651 (Activation)     (None, 8, 8, 342)    0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 8, 8, 48)     16416       activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_647 (Dropout)           (None, 8, 8, 48)     0           conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 8, 8, 48)     192         dropout_647[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_652 (Activation)     (None, 8, 8, 48)     0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_317 (Separable (None, 8, 8, 12)     1008        activation_652[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_648 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_317[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_317 (Concatenate)   (None, 8, 8, 354)    0           concatenate_316[0][0]            \n",
      "                                                                 dropout_648[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 8, 8, 354)    1416        concatenate_317[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_653 (Activation)     (None, 8, 8, 354)    0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 8, 8, 48)     16992       activation_653[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_649 (Dropout)           (None, 8, 8, 48)     0           conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 8, 8, 48)     192         dropout_649[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_654 (Activation)     (None, 8, 8, 48)     0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_318 (Separable (None, 8, 8, 12)     1008        activation_654[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_650 (Dropout)           (None, 8, 8, 12)     0           separable_conv2d_318[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_318 (Concatenate)   (None, 8, 8, 366)    0           concatenate_317[0][0]            \n",
      "                                                                 dropout_650[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 8, 8, 366)    1464        concatenate_318[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_655 (Activation)     (None, 8, 8, 366)    0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 8, 8, 183)    66978       activation_655[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_651 (Dropout)           (None, 8, 8, 183)    0           conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 4, 4, 183)    0           dropout_651[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 4, 4, 183)    732         average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_656 (Activation)     (None, 4, 4, 183)    0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 4, 4, 48)     8784        activation_656[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_652 (Dropout)           (None, 4, 4, 48)     0           conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 4, 4, 48)     192         dropout_652[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_657 (Activation)     (None, 4, 4, 48)     0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_319 (Separable (None, 4, 4, 12)     1008        activation_657[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_653 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_319[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_319 (Concatenate)   (None, 4, 4, 195)    0           average_pooling2d_19[0][0]       \n",
      "                                                                 dropout_653[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 4, 4, 195)    780         concatenate_319[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_658 (Activation)     (None, 4, 4, 195)    0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 4, 4, 48)     9360        activation_658[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_654 (Dropout)           (None, 4, 4, 48)     0           conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 4, 4, 48)     192         dropout_654[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_659 (Activation)     (None, 4, 4, 48)     0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_320 (Separable (None, 4, 4, 12)     1008        activation_659[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_655 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_320[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_320 (Concatenate)   (None, 4, 4, 207)    0           concatenate_319[0][0]            \n",
      "                                                                 dropout_655[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 4, 4, 207)    828         concatenate_320[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_660 (Activation)     (None, 4, 4, 207)    0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 4, 4, 48)     9936        activation_660[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_656 (Dropout)           (None, 4, 4, 48)     0           conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 4, 4, 48)     192         dropout_656[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_661 (Activation)     (None, 4, 4, 48)     0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_321 (Separable (None, 4, 4, 12)     1008        activation_661[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_657 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_321[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_321 (Concatenate)   (None, 4, 4, 219)    0           concatenate_320[0][0]            \n",
      "                                                                 dropout_657[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, 4, 4, 219)    876         concatenate_321[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_662 (Activation)     (None, 4, 4, 219)    0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 4, 4, 48)     10512       activation_662[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_658 (Dropout)           (None, 4, 4, 48)     0           conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, 4, 4, 48)     192         dropout_658[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_663 (Activation)     (None, 4, 4, 48)     0           batch_normalization_663[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_322 (Separable (None, 4, 4, 12)     1008        activation_663[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_659 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_322[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_322 (Concatenate)   (None, 4, 4, 231)    0           concatenate_321[0][0]            \n",
      "                                                                 dropout_659[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, 4, 4, 231)    924         concatenate_322[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_664 (Activation)     (None, 4, 4, 231)    0           batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 4, 4, 48)     11088       activation_664[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_660 (Dropout)           (None, 4, 4, 48)     0           conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, 4, 4, 48)     192         dropout_660[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_665 (Activation)     (None, 4, 4, 48)     0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_323 (Separable (None, 4, 4, 12)     1008        activation_665[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_661 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_323[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_323 (Concatenate)   (None, 4, 4, 243)    0           concatenate_322[0][0]            \n",
      "                                                                 dropout_661[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, 4, 4, 243)    972         concatenate_323[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_666 (Activation)     (None, 4, 4, 243)    0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 4, 4, 48)     11664       activation_666[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_662 (Dropout)           (None, 4, 4, 48)     0           conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, 4, 4, 48)     192         dropout_662[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_667 (Activation)     (None, 4, 4, 48)     0           batch_normalization_667[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_324 (Separable (None, 4, 4, 12)     1008        activation_667[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_663 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_324[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_324 (Concatenate)   (None, 4, 4, 255)    0           concatenate_323[0][0]            \n",
      "                                                                 dropout_663[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, 4, 4, 255)    1020        concatenate_324[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_668 (Activation)     (None, 4, 4, 255)    0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 4, 4, 48)     12240       activation_668[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_664 (Dropout)           (None, 4, 4, 48)     0           conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, 4, 4, 48)     192         dropout_664[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_669 (Activation)     (None, 4, 4, 48)     0           batch_normalization_669[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_325 (Separable (None, 4, 4, 12)     1008        activation_669[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_665 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_325[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_325 (Concatenate)   (None, 4, 4, 267)    0           concatenate_324[0][0]            \n",
      "                                                                 dropout_665[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, 4, 4, 267)    1068        concatenate_325[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_670 (Activation)     (None, 4, 4, 267)    0           batch_normalization_670[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 4, 4, 48)     12816       activation_670[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_666 (Dropout)           (None, 4, 4, 48)     0           conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, 4, 4, 48)     192         dropout_666[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_671 (Activation)     (None, 4, 4, 48)     0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_326 (Separable (None, 4, 4, 12)     1008        activation_671[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_667 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_326[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_326 (Concatenate)   (None, 4, 4, 279)    0           concatenate_325[0][0]            \n",
      "                                                                 dropout_667[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_672 (BatchN (None, 4, 4, 279)    1116        concatenate_326[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_672 (Activation)     (None, 4, 4, 279)    0           batch_normalization_672[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 4, 4, 48)     13392       activation_672[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_668 (Dropout)           (None, 4, 4, 48)     0           conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_673 (BatchN (None, 4, 4, 48)     192         dropout_668[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_673 (Activation)     (None, 4, 4, 48)     0           batch_normalization_673[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_327 (Separable (None, 4, 4, 12)     1008        activation_673[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_669 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_327[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_327 (Concatenate)   (None, 4, 4, 291)    0           concatenate_326[0][0]            \n",
      "                                                                 dropout_669[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_674 (BatchN (None, 4, 4, 291)    1164        concatenate_327[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_674 (Activation)     (None, 4, 4, 291)    0           batch_normalization_674[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 4, 4, 48)     13968       activation_674[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_670 (Dropout)           (None, 4, 4, 48)     0           conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_675 (BatchN (None, 4, 4, 48)     192         dropout_670[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_675 (Activation)     (None, 4, 4, 48)     0           batch_normalization_675[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_328 (Separable (None, 4, 4, 12)     1008        activation_675[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_671 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_328[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_328 (Concatenate)   (None, 4, 4, 303)    0           concatenate_327[0][0]            \n",
      "                                                                 dropout_671[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_676 (BatchN (None, 4, 4, 303)    1212        concatenate_328[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_676 (Activation)     (None, 4, 4, 303)    0           batch_normalization_676[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 4, 4, 48)     14544       activation_676[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_672 (Dropout)           (None, 4, 4, 48)     0           conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_677 (BatchN (None, 4, 4, 48)     192         dropout_672[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_677 (Activation)     (None, 4, 4, 48)     0           batch_normalization_677[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_329 (Separable (None, 4, 4, 12)     1008        activation_677[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_673 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_329[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_329 (Concatenate)   (None, 4, 4, 315)    0           concatenate_328[0][0]            \n",
      "                                                                 dropout_673[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_678 (BatchN (None, 4, 4, 315)    1260        concatenate_329[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_678 (Activation)     (None, 4, 4, 315)    0           batch_normalization_678[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 4, 4, 48)     15120       activation_678[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_674 (Dropout)           (None, 4, 4, 48)     0           conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_679 (BatchN (None, 4, 4, 48)     192         dropout_674[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_679 (Activation)     (None, 4, 4, 48)     0           batch_normalization_679[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_330 (Separable (None, 4, 4, 12)     1008        activation_679[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_675 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_330[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_330 (Concatenate)   (None, 4, 4, 327)    0           concatenate_329[0][0]            \n",
      "                                                                 dropout_675[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_680 (BatchN (None, 4, 4, 327)    1308        concatenate_330[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_680 (Activation)     (None, 4, 4, 327)    0           batch_normalization_680[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 4, 4, 48)     15696       activation_680[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_676 (Dropout)           (None, 4, 4, 48)     0           conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_681 (BatchN (None, 4, 4, 48)     192         dropout_676[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_681 (Activation)     (None, 4, 4, 48)     0           batch_normalization_681[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_331 (Separable (None, 4, 4, 12)     1008        activation_681[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_677 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_331[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_331 (Concatenate)   (None, 4, 4, 339)    0           concatenate_330[0][0]            \n",
      "                                                                 dropout_677[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_682 (BatchN (None, 4, 4, 339)    1356        concatenate_331[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_682 (Activation)     (None, 4, 4, 339)    0           batch_normalization_682[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 4, 4, 48)     16272       activation_682[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_678 (Dropout)           (None, 4, 4, 48)     0           conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_683 (BatchN (None, 4, 4, 48)     192         dropout_678[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_683 (Activation)     (None, 4, 4, 48)     0           batch_normalization_683[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_332 (Separable (None, 4, 4, 12)     1008        activation_683[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_679 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_332[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_332 (Concatenate)   (None, 4, 4, 351)    0           concatenate_331[0][0]            \n",
      "                                                                 dropout_679[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_684 (BatchN (None, 4, 4, 351)    1404        concatenate_332[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_684 (Activation)     (None, 4, 4, 351)    0           batch_normalization_684[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 4, 4, 48)     16848       activation_684[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_680 (Dropout)           (None, 4, 4, 48)     0           conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_685 (BatchN (None, 4, 4, 48)     192         dropout_680[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_685 (Activation)     (None, 4, 4, 48)     0           batch_normalization_685[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_333 (Separable (None, 4, 4, 12)     1008        activation_685[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_681 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_333[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_333 (Concatenate)   (None, 4, 4, 363)    0           concatenate_332[0][0]            \n",
      "                                                                 dropout_681[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_686 (BatchN (None, 4, 4, 363)    1452        concatenate_333[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_686 (Activation)     (None, 4, 4, 363)    0           batch_normalization_686[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 4, 4, 48)     17424       activation_686[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_682 (Dropout)           (None, 4, 4, 48)     0           conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_687 (BatchN (None, 4, 4, 48)     192         dropout_682[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_687 (Activation)     (None, 4, 4, 48)     0           batch_normalization_687[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_334 (Separable (None, 4, 4, 12)     1008        activation_687[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_683 (Dropout)           (None, 4, 4, 12)     0           separable_conv2d_334[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_334 (Concatenate)   (None, 4, 4, 375)    0           concatenate_333[0][0]            \n",
      "                                                                 dropout_683[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_688 (BatchN (None, 4, 4, 375)    1500        concatenate_334[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_688 (Activation)     (None, 4, 4, 375)    0           batch_normalization_688[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePo (None, 2, 2, 375)    0           activation_688[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1500)         0           average_pooling2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           15010       flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 957,784\n",
      "Trainable params: 921,142\n",
      "Non-trainable params: 36,642\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f68LtcYHG6i4"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 90\n",
    "decay = learning_rate/epochs\n",
    "decay = 0.0001\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=learning_rate, decay=decay, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Oj8RSwyG6i9"
   },
   "outputs": [],
   "source": [
    "#batch_size = 64\n",
    "#clr_triangular = CyclicLR(mode='triangular', base_lr = 0.1, max_lr = 0.2, step_size = (len(x_train)* 2 * 4)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HT7ZwHzG6jG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crhGk7kEhXAz"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcWydmIVhZGr"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 30.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7dl5K84G6jl"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay1(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epoch_drop_01 = 40\n",
    "    epoch_drop_02 = epoch_drop_01 + 40\n",
    "    epoch_drop_03 = epoch_drop_02 + 40\n",
    "    \n",
    "    if (epoch < epoch_drop_01):\n",
    "        lrate = initial_lrate\n",
    "    elif (epoch < epoch_drop_02):\n",
    "        lrate = initial_lrate * drop\n",
    "    else:\n",
    "        lrate = initial_lrate * drop * drop\n",
    "\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDig-b71G6jq"
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(self, epoch, logs=None):\n",
    "    print(\"epoch: \", epoch,\"learning rate for\", K.eval(self.model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zjHHVfPG6jw"
   },
   "outputs": [],
   "source": [
    "lrate = LearningRateScheduler(step_decay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aS6q4X1G6j0"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.001)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience= 4, min_delta=0.003, verbose=1, cooldown=0, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJgRsh_2G6j7"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZZuotjhG6kA"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#filepath = file_prefix + r\".best.hdf5\"\n",
    "filepath = \"DNST_CIFAR10_Conv_09_10_final_1-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE3lF6EH1r_L"
   },
   "outputs": [],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "#model.save_weights(\"densenet_tr_03-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai-yZ2ED5AK1"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "\n",
    "#files.download('DNST_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir_fg-p9G6kO"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6HtNUyQG6kS"
   },
   "outputs": [],
   "source": [
    "class AdamTracker_0(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"start , epoch = \", epoch,\", lr = \", K.eval(optimizer.lr),\", decay = \",K.eval(optimizer.decay),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BtYHxTCG6kV"
   },
   "outputs": [],
   "source": [
    "class AdamTracker_1(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"end, epoch = \", epoch,\", lr = \", K.eval(optimizer.lr),\", decay = \",K.eval(optimizer.decay),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMe9lOY9G6kb"
   },
   "outputs": [],
   "source": [
    "adam_lr_tracker_1 = AdamTracker_1()\n",
    "adam_lr_tracker_0 = AdamTracker_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2iue0UsLDzb"
   },
   "outputs": [],
   "source": [
    "class SGDLearningRateTracker(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"epoch = \", epoch,\", lr = \", K.eval(optimizer.lr), \", momentum = \",K.eval(optimizer.momentum),\n",
    "              \", decay = \",K.eval(optimizer.decay), \", Nestrov = \",optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-itoaFDG6kf"
   },
   "outputs": [],
   "source": [
    "sgd_lr_tracker = SGDLearningRateTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Og56VCRh5j8V"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint, reduce_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ki7pVU60G6ko"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [adam_lr_tracker_0, adam_lr_tracker_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBTjYaJ4G6kv"
   },
   "outputs": [],
   "source": [
    "#callbacks_list = [checkpoint, adam_lr_tracker_0, adam_lr_tracker_1, clr_triangular]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhuPjscpK0mm"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint, sgd_lr_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bTKPS9HG6kx"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model('densenet_tr_03-46-0.71.hdf5')\n",
    "#score = model.evaluate(x_test, y_test, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OfdusR7aG6k1",
    "outputId": "f23665c5-6112-4dcc-d452-fae080e5ab70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-LiiCB9G6k8"
   },
   "source": [
    "## Call the model with the datagen, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WKzwh45G6k8"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZjsbZONG6lJ"
   },
   "outputs": [],
   "source": [
    "load_model_from_back = False\n",
    "\n",
    "if load_model_from_back:\n",
    "    model = load_model('--------------------')\n",
    "    score = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4114
    },
    "colab_type": "code",
    "id": "ODPSQd8dG6lM",
    "outputId": "0b061b74-5653-4c53-932e-04eb30c3664e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1563/1562 [==============================] - 379s 243ms/step - loss: 1.9980 - acc: 0.3032 - val_loss: 1.6512 - val_acc: 0.4218\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.42180, saving model to DNST_CIFAR10_Conv_09_10_final_1-01-0.4218.hdf5\n",
      "epoch =  0 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 1.5858 - acc: 0.4209 - val_loss: 1.4755 - val_acc: 0.4854\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.42180 to 0.48540, saving model to DNST_CIFAR10_Conv_09_10_final_1-02-0.4854.hdf5\n",
      "epoch =  1 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 1.3868 - acc: 0.4982 - val_loss: 1.4663 - val_acc: 0.5231\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.48540 to 0.52310, saving model to DNST_CIFAR10_Conv_09_10_final_1-03-0.5231.hdf5\n",
      "epoch =  2 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 1.2296 - acc: 0.5594 - val_loss: 1.6394 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.52310 to 0.54040, saving model to DNST_CIFAR10_Conv_09_10_final_1-04-0.5404.hdf5\n",
      "epoch =  3 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 1.1157 - acc: 0.6038 - val_loss: 1.1602 - val_acc: 0.6345\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.54040 to 0.63450, saving model to DNST_CIFAR10_Conv_09_10_final_1-05-0.6345.hdf5\n",
      "epoch =  4 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 1.0310 - acc: 0.6341 - val_loss: 1.0678 - val_acc: 0.6662\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.63450 to 0.66620, saving model to DNST_CIFAR10_Conv_09_10_final_1-06-0.6662.hdf5\n",
      "epoch =  5 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.9649 - acc: 0.6576 - val_loss: 0.9933 - val_acc: 0.6892\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.66620 to 0.68920, saving model to DNST_CIFAR10_Conv_09_10_final_1-07-0.6892.hdf5\n",
      "epoch =  6 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.9154 - acc: 0.6777 - val_loss: 1.2565 - val_acc: 0.6539\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.68920\n",
      "epoch =  7 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.8740 - acc: 0.6922 - val_loss: 0.9112 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.68920 to 0.71470, saving model to DNST_CIFAR10_Conv_09_10_final_1-09-0.7147.hdf5\n",
      "epoch =  8 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.8375 - acc: 0.7061 - val_loss: 0.9884 - val_acc: 0.6999\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.71470\n",
      "epoch =  9 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 11/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.8046 - acc: 0.7163 - val_loss: 0.8957 - val_acc: 0.7284\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.71470 to 0.72840, saving model to DNST_CIFAR10_Conv_09_10_final_1-11-0.7284.hdf5\n",
      "epoch =  10 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 12/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 0.7731 - acc: 0.7286 - val_loss: 0.9625 - val_acc: 0.7269\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.72840\n",
      "epoch =  11 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 13/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.7555 - acc: 0.7334 - val_loss: 0.9117 - val_acc: 0.7326\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.72840 to 0.73260, saving model to DNST_CIFAR10_Conv_09_10_final_1-13-0.7326.hdf5\n",
      "epoch =  12 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 14/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.7322 - acc: 0.7428 - val_loss: 0.8000 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.73260 to 0.75940, saving model to DNST_CIFAR10_Conv_09_10_final_1-14-0.7594.hdf5\n",
      "epoch =  13 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 15/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.7087 - acc: 0.7514 - val_loss: 0.9095 - val_acc: 0.7431\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.75940\n",
      "epoch =  14 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 16/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.6846 - acc: 0.7593 - val_loss: 0.7743 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.75940 to 0.76800, saving model to DNST_CIFAR10_Conv_09_10_final_1-16-0.7680.hdf5\n",
      "epoch =  15 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 17/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.6667 - acc: 0.7641 - val_loss: 0.8183 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76800\n",
      "epoch =  16 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 18/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 0.6558 - acc: 0.7694 - val_loss: 0.8338 - val_acc: 0.7541\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76800\n",
      "epoch =  17 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 19/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 0.6364 - acc: 0.7757 - val_loss: 0.7737 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.76800 to 0.77970, saving model to DNST_CIFAR10_Conv_09_10_final_1-19-0.7797.hdf5\n",
      "epoch =  18 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 20/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.6217 - acc: 0.7815 - val_loss: 0.7388 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.77970 to 0.78090, saving model to DNST_CIFAR10_Conv_09_10_final_1-20-0.7809.hdf5\n",
      "epoch =  19 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 21/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.6049 - acc: 0.7879 - val_loss: 0.6755 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.78090 to 0.79880, saving model to DNST_CIFAR10_Conv_09_10_final_1-21-0.7988.hdf5\n",
      "epoch =  20 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 22/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.5970 - acc: 0.7905 - val_loss: 0.7858 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79880\n",
      "epoch =  21 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 23/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.5788 - acc: 0.7966 - val_loss: 0.7324 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79880\n",
      "epoch =  22 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 24/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.5656 - acc: 0.8013 - val_loss: 0.8415 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79880\n",
      "epoch =  23 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 25/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.5592 - acc: 0.8050 - val_loss: 0.7647 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.79880\n",
      "epoch =  24 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 26/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.5444 - acc: 0.8089 - val_loss: 0.6692 - val_acc: 0.8051\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.79880 to 0.80510, saving model to DNST_CIFAR10_Conv_09_10_final_1-26-0.8051.hdf5\n",
      "epoch =  25 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 27/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.5343 - acc: 0.8111 - val_loss: 0.7349 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.80510\n",
      "epoch =  26 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 28/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.5242 - acc: 0.8152 - val_loss: 0.6298 - val_acc: 0.8183\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.80510 to 0.81830, saving model to DNST_CIFAR10_Conv_09_10_final_1-28-0.8183.hdf5\n",
      "epoch =  27 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 29/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.5186 - acc: 0.8177 - val_loss: 0.7117 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81830\n",
      "epoch =  28 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 30/80\n",
      "1563/1562 [==============================] - 312s 200ms/step - loss: 0.5045 - acc: 0.8233 - val_loss: 0.6749 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81830\n",
      "epoch =  29 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 31/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.5032 - acc: 0.8243 - val_loss: 0.6585 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81830\n",
      "epoch =  30 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 32/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.4934 - acc: 0.8260 - val_loss: 0.6225 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.81830 to 0.82290, saving model to DNST_CIFAR10_Conv_09_10_final_1-32-0.8229.hdf5\n",
      "epoch =  31 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 33/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.4854 - acc: 0.8297 - val_loss: 0.5982 - val_acc: 0.8307\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.82290 to 0.83070, saving model to DNST_CIFAR10_Conv_09_10_final_1-33-0.8307.hdf5\n",
      "epoch =  32 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 34/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.4737 - acc: 0.8338 - val_loss: 0.7285 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83070\n",
      "epoch =  33 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 35/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.4689 - acc: 0.8342 - val_loss: 0.7273 - val_acc: 0.8074\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83070\n",
      "epoch =  34 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 36/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.4590 - acc: 0.8388 - val_loss: 0.6460 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83070\n",
      "epoch =  35 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 37/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 0.4528 - acc: 0.8400 - val_loss: 0.6205 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83070\n",
      "epoch =  36 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 38/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.4472 - acc: 0.8422 - val_loss: 0.7271 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83070\n",
      "epoch =  37 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 39/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.4401 - acc: 0.8445 - val_loss: 0.6887 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83070\n",
      "epoch =  38 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 40/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 0.4339 - acc: 0.8466 - val_loss: 0.6083 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.83070 to 0.83590, saving model to DNST_CIFAR10_Conv_09_10_final_1-40-0.8359.hdf5\n",
      "epoch =  39 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 41/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.4287 - acc: 0.8491 - val_loss: 0.6062 - val_acc: 0.8442\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.83590 to 0.84420, saving model to DNST_CIFAR10_Conv_09_10_final_1-41-0.8442.hdf5\n",
      "epoch =  40 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 42/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.4257 - acc: 0.8501 - val_loss: 0.6083 - val_acc: 0.8357\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84420\n",
      "epoch =  41 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 43/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 0.4144 - acc: 0.8536 - val_loss: 0.6834 - val_acc: 0.8307\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84420\n",
      "epoch =  42 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 44/80\n",
      "1563/1562 [==============================] - 305s 195ms/step - loss: 0.4139 - acc: 0.8541 - val_loss: 0.7030 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84420\n",
      "epoch =  43 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 45/80\n",
      "1563/1562 [==============================] - 304s 194ms/step - loss: 0.4055 - acc: 0.8572 - val_loss: 0.6851 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84420\n",
      "epoch =  44 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 46/80\n",
      "1563/1562 [==============================] - 303s 194ms/step - loss: 0.4023 - acc: 0.8581 - val_loss: 0.6096 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84420\n",
      "epoch =  45 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 47/80\n",
      "1563/1562 [==============================] - 304s 195ms/step - loss: 0.3974 - acc: 0.8591 - val_loss: 0.6849 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84420\n",
      "epoch =  46 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 48/80\n",
      "  90/1562 [>.............................] - ETA: 4:41 - loss: 0.3894 - acc: 0.8641"
     ]
    }
   ],
   "source": [
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 2.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"DNST_CIFAR10_Conv_09_10_final_80_epochs_CP.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 499us/step\n",
      "Test loss: 0.5716010042786598\n",
      "Test accuracy: 0.8546\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"DNST_CIFAR10_Conv_09_10_final_80_epochs_CP.hdf5\")\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2907 - acc: 0.8969 - val_loss: 0.5165 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.88120\n",
      "epoch =  0 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2875 - acc: 0.8987 - val_loss: 0.5292 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.88120\n",
      "epoch =  1 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2817 - acc: 0.8998 - val_loss: 0.5320 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.88120\n",
      "epoch =  2 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2847 - acc: 0.9007 - val_loss: 0.5291 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.88120\n",
      "epoch =  3 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2812 - acc: 0.9004 - val_loss: 0.4918 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.88120\n",
      "epoch =  4 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/20\n",
      "782/781 [==============================] - 127s 163ms/step - loss: 0.2770 - acc: 0.9031 - val_loss: 0.5245 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88120\n",
      "epoch =  5 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/20\n",
      "782/781 [==============================] - 127s 163ms/step - loss: 0.2789 - acc: 0.9029 - val_loss: 0.5126 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88120\n",
      "epoch =  6 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2760 - acc: 0.9028 - val_loss: 0.4780 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88120\n",
      "epoch =  7 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/20\n",
      "782/781 [==============================] - 127s 163ms/step - loss: 0.2765 - acc: 0.9016 - val_loss: 0.4683 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88120\n",
      "epoch =  8 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/20\n",
      "782/781 [==============================] - 129s 165ms/step - loss: 0.2744 - acc: 0.9038 - val_loss: 0.4783 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88120\n",
      "epoch =  9 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 11/20\n",
      "782/781 [==============================] - 128s 163ms/step - loss: 0.2709 - acc: 0.9029 - val_loss: 0.4441 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88120\n",
      "epoch =  10 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 12/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2672 - acc: 0.9053 - val_loss: 0.5165 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88120\n",
      "epoch =  11 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 13/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2667 - acc: 0.9054 - val_loss: 0.4967 - val_acc: 0.8727\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88120\n",
      "epoch =  12 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 14/20\n",
      "782/781 [==============================] - 127s 163ms/step - loss: 0.2628 - acc: 0.9082 - val_loss: 0.4561 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.88120 to 0.88200, saving model to DNST_CIFAR10_Conv_09_10_final-14-0.8820.hdf5\n",
      "epoch =  13 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 15/20\n",
      "782/781 [==============================] - 129s 165ms/step - loss: 0.2636 - acc: 0.9060 - val_loss: 0.5738 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88200\n",
      "epoch =  14 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 16/20\n",
      "782/781 [==============================] - 128s 163ms/step - loss: 0.2624 - acc: 0.9078 - val_loss: 0.4766 - val_acc: 0.8769\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88200\n",
      "epoch =  15 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 17/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2602 - acc: 0.9072 - val_loss: 0.4467 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.88200 to 0.88440, saving model to DNST_CIFAR10_Conv_09_10_final-17-0.8844.hdf5\n",
      "epoch =  16 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 18/20\n",
      "782/781 [==============================] - 127s 163ms/step - loss: 0.2611 - acc: 0.9076 - val_loss: 0.4776 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88440\n",
      "epoch =  17 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 19/20\n",
      "782/781 [==============================] - 128s 164ms/step - loss: 0.2570 - acc: 0.9089 - val_loss: 0.4525 - val_acc: 0.8823\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88440\n",
      "epoch =  18 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 20/20\n",
      "782/781 [==============================] - 129s 165ms/step - loss: 0.2577 - acc: 0.9079 - val_loss: 0.4511 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88440\n",
      "epoch =  19 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 1.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"DNST_CIFAR10_Conv_09_10_final_100_epochs_CP.hdf5\") # this is ruiined now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 497us/step\n",
      "Test loss: 0.5716010042786598\n",
      "Test accuracy: 0.8546\n",
      "50000/50000 [==============================] - 25s 503us/step\n",
      "Train loss: 0.31507150228619574\n",
      "Train accuracy: 0.9074\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"DNST_CIFAR10_Conv_09_10_final_80_epochs_CP.hdf5\")\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "score = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13:56 30 october, more agumentaion, 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,    # randomly flip images\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.1)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "782/781 [==============================] - 194s 248ms/step - loss: 0.2764 - acc: 0.9020 - val_loss: 0.4597 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.89430\n",
      "epoch =  0 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/180\n",
      "782/781 [==============================] - 193s 247ms/step - loss: 0.2676 - acc: 0.9057 - val_loss: 0.5028 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.89430\n",
      "epoch =  1 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/180\n",
      "782/781 [==============================] - 193s 247ms/step - loss: 0.2644 - acc: 0.9065 - val_loss: 0.5068 - val_acc: 0.8742\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.89430\n",
      "epoch =  2 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/180\n",
      "782/781 [==============================] - 192s 246ms/step - loss: 0.2615 - acc: 0.9067 - val_loss: 0.5178 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.89430\n",
      "epoch =  3 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/180\n",
      "782/781 [==============================] - 192s 245ms/step - loss: 0.2606 - acc: 0.9083 - val_loss: 0.4894 - val_acc: 0.8793\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.89430\n",
      "epoch =  4 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/180\n",
      "782/781 [==============================] - 191s 244ms/step - loss: 0.2570 - acc: 0.9087 - val_loss: 0.4855 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89430\n",
      "epoch =  5 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/180\n",
      "782/781 [==============================] - 190s 244ms/step - loss: 0.2561 - acc: 0.9091 - val_loss: 0.5081 - val_acc: 0.8755\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.89430\n",
      "epoch =  6 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/180\n",
      "782/781 [==============================] - 191s 244ms/step - loss: 0.2527 - acc: 0.9106 - val_loss: 0.5068 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89430\n",
      "epoch =  7 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/180\n",
      "782/781 [==============================] - 190s 243ms/step - loss: 0.2501 - acc: 0.9114 - val_loss: 0.4903 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89430\n",
      "epoch =  8 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/180\n",
      "782/781 [==============================] - 190s 243ms/step - loss: 0.2513 - acc: 0.9103 - val_loss: 0.4860 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89430\n",
      "epoch =  9 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 11/180\n",
      "782/781 [==============================] - 190s 242ms/step - loss: 0.2475 - acc: 0.9121 - val_loss: 0.4999 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89430\n",
      "epoch =  10 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 12/180\n",
      "782/781 [==============================] - 189s 242ms/step - loss: 0.2464 - acc: 0.9127 - val_loss: 0.4993 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89430\n",
      "epoch =  11 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 13/180\n",
      "782/781 [==============================] - 190s 242ms/step - loss: 0.2465 - acc: 0.9126 - val_loss: 0.4947 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89430\n",
      "epoch =  12 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 14/180\n",
      "782/781 [==============================] - 189s 241ms/step - loss: 0.2432 - acc: 0.9138 - val_loss: 0.4848 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89430\n",
      "epoch =  13 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 15/180\n",
      "782/781 [==============================] - 189s 242ms/step - loss: 0.2427 - acc: 0.9137 - val_loss: 0.5016 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89430\n",
      "epoch =  14 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 16/180\n",
      "782/781 [==============================] - 189s 242ms/step - loss: 0.2427 - acc: 0.9139 - val_loss: 0.4809 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89430\n",
      "epoch =  15 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 17/180\n",
      "782/781 [==============================] - 188s 241ms/step - loss: 0.2420 - acc: 0.9141 - val_loss: 0.4991 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89430\n",
      "epoch =  16 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 18/180\n",
      "782/781 [==============================] - 189s 241ms/step - loss: 0.2379 - acc: 0.9160 - val_loss: 0.5075 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89430\n",
      "epoch =  17 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 19/180\n",
      "782/781 [==============================] - 188s 240ms/step - loss: 0.2377 - acc: 0.9162 - val_loss: 0.4869 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89430\n",
      "epoch =  18 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 20/180\n",
      "782/781 [==============================] - 188s 241ms/step - loss: 0.2362 - acc: 0.9156 - val_loss: 0.4822 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89430\n",
      "epoch =  19 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 21/180\n",
      "782/781 [==============================] - 188s 240ms/step - loss: 0.2352 - acc: 0.9169 - val_loss: 0.5250 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.89430\n",
      "epoch =  20 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 22/180\n",
      "782/781 [==============================] - 187s 240ms/step - loss: 0.2341 - acc: 0.9163 - val_loss: 0.5090 - val_acc: 0.8770\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.89430\n",
      "epoch =  21 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 23/180\n",
      "782/781 [==============================] - 188s 240ms/step - loss: 0.2335 - acc: 0.9174 - val_loss: 0.5151 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.89430\n",
      "epoch =  22 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 24/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2323 - acc: 0.9176 - val_loss: 0.4974 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.89430\n",
      "epoch =  23 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 25/180\n",
      "782/781 [==============================] - 188s 240ms/step - loss: 0.2328 - acc: 0.9172 - val_loss: 0.4872 - val_acc: 0.8793\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.89430\n",
      "epoch =  24 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 26/180\n",
      "782/781 [==============================] - 187s 240ms/step - loss: 0.2327 - acc: 0.9168 - val_loss: 0.4948 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.89430\n",
      "epoch =  25 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 27/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2291 - acc: 0.9184 - val_loss: 0.4882 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.89430\n",
      "epoch =  26 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 28/180\n",
      "782/781 [==============================] - 188s 240ms/step - loss: 0.2260 - acc: 0.9202 - val_loss: 0.4743 - val_acc: 0.8830\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.89430\n",
      "epoch =  27 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 29/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2268 - acc: 0.9198 - val_loss: 0.4897 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.89430\n",
      "epoch =  28 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 30/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2263 - acc: 0.9191 - val_loss: 0.5132 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.89430\n",
      "epoch =  29 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 31/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2247 - acc: 0.9203 - val_loss: 0.5072 - val_acc: 0.8754\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.89430\n",
      "epoch =  30 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 32/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2242 - acc: 0.9193 - val_loss: 0.4996 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.89430\n",
      "epoch =  31 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 33/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2248 - acc: 0.9200 - val_loss: 0.5074 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.89430\n",
      "epoch =  32 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 34/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2228 - acc: 0.9208 - val_loss: 0.4749 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.89430\n",
      "epoch =  33 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 35/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2216 - acc: 0.9206 - val_loss: 0.4910 - val_acc: 0.8794\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89430\n",
      "epoch =  34 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 36/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2192 - acc: 0.9226 - val_loss: 0.4913 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89430\n",
      "epoch =  35 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 37/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2191 - acc: 0.9207 - val_loss: 0.5075 - val_acc: 0.8798\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89430\n",
      "epoch =  36 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 38/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2162 - acc: 0.9231 - val_loss: 0.4933 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89430\n",
      "epoch =  39 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 41/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2174 - acc: 0.9224 - val_loss: 0.4879 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89430\n",
      "epoch =  40 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 42/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2186 - acc: 0.9225 - val_loss: 0.4836 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89430\n",
      "epoch =  41 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 43/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2119 - acc: 0.9241 - val_loss: 0.5110 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89430\n",
      "epoch =  42 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 44/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2149 - acc: 0.9233 - val_loss: 0.5229 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89430\n",
      "epoch =  43 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 45/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2141 - acc: 0.9241 - val_loss: 0.4775 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89430\n",
      "epoch =  44 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 46/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2137 - acc: 0.9240 - val_loss: 0.5208 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89430\n",
      "epoch =  45 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 47/180\n",
      "782/781 [==============================] - 186s 237ms/step - loss: 0.2101 - acc: 0.9252 - val_loss: 0.5043 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89430\n",
      "epoch =  46 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 48/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2115 - acc: 0.9244 - val_loss: 0.4833 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89430\n",
      "epoch =  47 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 49/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2094 - acc: 0.9253 - val_loss: 0.5048 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89430\n",
      "epoch =  48 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 50/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2074 - acc: 0.9258 - val_loss: 0.5100 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89430\n",
      "epoch =  49 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 51/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2079 - acc: 0.9258 - val_loss: 0.5073 - val_acc: 0.8822\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89430\n",
      "epoch =  50 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 52/180\n",
      "782/781 [==============================] - 186s 237ms/step - loss: 0.2054 - acc: 0.9263 - val_loss: 0.5208 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89430\n",
      "epoch =  51 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 53/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2052 - acc: 0.9266 - val_loss: 0.5172 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89430\n",
      "epoch =  52 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 54/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2019 - acc: 0.9276 - val_loss: 0.5115 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89430\n",
      "epoch =  53 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 55/180\n",
      "782/781 [==============================] - 185s 237ms/step - loss: 0.2043 - acc: 0.9268 - val_loss: 0.5052 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89430\n",
      "epoch =  54 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 56/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2023 - acc: 0.9275 - val_loss: 0.4954 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89430\n",
      "epoch =  55 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 57/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2017 - acc: 0.9278 - val_loss: 0.5121 - val_acc: 0.8810\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89430\n",
      "epoch =  56 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 58/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2024 - acc: 0.9276 - val_loss: 0.5128 - val_acc: 0.8798\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89430\n",
      "epoch =  57 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 59/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.2019 - acc: 0.9280 - val_loss: 0.5111 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89430\n",
      "epoch =  58 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 60/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.2009 - acc: 0.9286 - val_loss: 0.5110 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89430\n",
      "epoch =  59 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 61/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1986 - acc: 0.9296 - val_loss: 0.5016 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89430\n",
      "epoch =  60 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 62/180\n",
      "782/781 [==============================] - 185s 237ms/step - loss: 0.1993 - acc: 0.9285 - val_loss: 0.5019 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89430\n",
      "epoch =  61 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 63/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1982 - acc: 0.9295 - val_loss: 0.5131 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89430\n",
      "epoch =  62 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 64/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1982 - acc: 0.9292 - val_loss: 0.5265 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89430\n",
      "epoch =  63 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 65/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1974 - acc: 0.9297 - val_loss: 0.5376 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89430\n",
      "epoch =  64 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 66/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.1970 - acc: 0.9292 - val_loss: 0.5142 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89430\n",
      "epoch =  65 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 67/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1966 - acc: 0.9298 - val_loss: 0.5091 - val_acc: 0.8802\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89430\n",
      "epoch =  66 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 68/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1958 - acc: 0.9299 - val_loss: 0.5034 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89430\n",
      "epoch =  67 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 69/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1931 - acc: 0.9310 - val_loss: 0.5354 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89430\n",
      "epoch =  68 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 70/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1948 - acc: 0.9305 - val_loss: 0.5208 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89430\n",
      "epoch =  69 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 71/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1953 - acc: 0.9302 - val_loss: 0.5096 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89430\n",
      "epoch =  70 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 72/180\n",
      "782/781 [==============================] - 185s 237ms/step - loss: 0.1917 - acc: 0.9315 - val_loss: 0.5557 - val_acc: 0.8763\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89430\n",
      "epoch =  71 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 73/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1957 - acc: 0.9305 - val_loss: 0.5233 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89430\n",
      "epoch =  72 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 74/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1922 - acc: 0.9312 - val_loss: 0.5254 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89430\n",
      "epoch =  73 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 75/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1887 - acc: 0.9328 - val_loss: 0.5160 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89430\n",
      "epoch =  74 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 76/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.1887 - acc: 0.9327 - val_loss: 0.5327 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89430\n",
      "epoch =  75 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 77/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1891 - acc: 0.9322 - val_loss: 0.5138 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89430\n",
      "epoch =  76 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 78/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.1880 - acc: 0.9327 - val_loss: 0.5079 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89430\n",
      "epoch =  77 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 79/180\n",
      "782/781 [==============================] - 187s 238ms/step - loss: 0.1893 - acc: 0.9319 - val_loss: 0.5434 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89430\n",
      "epoch =  78 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 80/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1889 - acc: 0.9322 - val_loss: 0.5253 - val_acc: 0.8815\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89430\n",
      "epoch =  79 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 81/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.1875 - acc: 0.9332 - val_loss: 0.5320 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89430\n",
      "epoch =  80 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 82/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1873 - acc: 0.9328 - val_loss: 0.5198 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89430\n",
      "epoch =  81 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 83/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1854 - acc: 0.9337 - val_loss: 0.5102 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89430\n",
      "epoch =  82 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 84/180\n",
      "782/781 [==============================] - 186s 237ms/step - loss: 0.1863 - acc: 0.9334 - val_loss: 0.5261 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89430\n",
      "epoch =  83 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 85/180\n",
      "782/781 [==============================] - 186s 237ms/step - loss: 0.1856 - acc: 0.9336 - val_loss: 0.5094 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89430\n",
      "epoch =  84 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 86/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1823 - acc: 0.9348 - val_loss: 0.5081 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89430\n",
      "epoch =  85 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 87/180\n",
      "782/781 [==============================] - 185s 237ms/step - loss: 0.1829 - acc: 0.9347 - val_loss: 0.5184 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89430\n",
      "epoch =  86 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 88/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1832 - acc: 0.9343 - val_loss: 0.5228 - val_acc: 0.8821\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89430\n",
      "epoch =  87 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 89/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1829 - acc: 0.9348 - val_loss: 0.5299 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89430\n",
      "epoch =  88 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 90/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1819 - acc: 0.9342 - val_loss: 0.5124 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89430\n",
      "epoch =  89 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 91/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1806 - acc: 0.9352 - val_loss: 0.5266 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89430\n",
      "epoch =  90 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 92/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1827 - acc: 0.9342 - val_loss: 0.5352 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89430\n",
      "epoch =  91 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 93/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.1790 - acc: 0.9355 - val_loss: 0.5159 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89430\n",
      "epoch =  92 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 94/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1793 - acc: 0.9358 - val_loss: 0.5360 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89430\n",
      "epoch =  93 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 95/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1789 - acc: 0.9356 - val_loss: 0.5475 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89430\n",
      "epoch =  94 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 96/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1772 - acc: 0.9370 - val_loss: 0.5363 - val_acc: 0.8822\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89430\n",
      "epoch =  95 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 97/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1770 - acc: 0.9365 - val_loss: 0.5214 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89430\n",
      "epoch =  96 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 98/180\n",
      "782/781 [==============================] - 186s 237ms/step - loss: 0.1772 - acc: 0.9362 - val_loss: 0.5126 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89430\n",
      "epoch =  97 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 99/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1781 - acc: 0.9363 - val_loss: 0.5430 - val_acc: 0.8827\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89430\n",
      "epoch =  98 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 100/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1754 - acc: 0.9372 - val_loss: 0.5289 - val_acc: 0.8849\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89430\n",
      "epoch =  99 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 101/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1745 - acc: 0.9368 - val_loss: 0.5435 - val_acc: 0.8818\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89430\n",
      "epoch =  100 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 102/180\n",
      "782/781 [==============================] - 185s 237ms/step - loss: 0.1760 - acc: 0.9369 - val_loss: 0.5554 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89430\n",
      "epoch =  101 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 103/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1752 - acc: 0.9374 - val_loss: 0.5319 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89430\n",
      "epoch =  102 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 104/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1752 - acc: 0.9372 - val_loss: 0.5174 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89430\n",
      "epoch =  103 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 105/180\n",
      "782/781 [==============================] - 187s 239ms/step - loss: 0.1737 - acc: 0.9376 - val_loss: 0.5430 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89430\n",
      "epoch =  104 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 106/180\n",
      "782/781 [==============================] - 186s 238ms/step - loss: 0.1725 - acc: 0.9377 - val_loss: 0.5229 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89430\n",
      "epoch =  105 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 107/180\n",
      "671/781 [========================>.....] - ETA: 25s - loss: 0.1730 - acc: 0.9375"
     ]
    }
   ],
   "source": [
    "batch_size = 192\n",
    "epochs = 180\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 3.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "391/390 [==============================] - 81s 206ms/step - loss: 0.2320 - acc: 0.9184 - val_loss: 0.4497 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.88440\n",
      "epoch =  0 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/40\n",
      "391/390 [==============================] - 79s 202ms/step - loss: 0.2286 - acc: 0.9189 - val_loss: 0.4881 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.88440\n",
      "epoch =  1 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/40\n",
      "391/390 [==============================] - 78s 200ms/step - loss: 0.2300 - acc: 0.9179 - val_loss: 0.4785 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.88440\n",
      "epoch =  2 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/40\n",
      "391/390 [==============================] - 78s 200ms/step - loss: 0.2259 - acc: 0.9196 - val_loss: 0.4780 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.88440\n",
      "epoch =  3 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/40\n",
      "391/390 [==============================] - 79s 201ms/step - loss: 0.2236 - acc: 0.9202 - val_loss: 0.4770 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.88440\n",
      "epoch =  4 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/40\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.2212 - acc: 0.9224 - val_loss: 0.4484 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.88440 to 0.88550, saving model to DNST_CIFAR10_Conv_09_10_final-06-0.8855.hdf5\n",
      "epoch =  5 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/40\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.2217 - acc: 0.9216 - val_loss: 0.4580 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.88550 to 0.88670, saving model to DNST_CIFAR10_Conv_09_10_final-07-0.8867.hdf5\n",
      "epoch =  6 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/40\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.2190 - acc: 0.9217 - val_loss: 0.4655 - val_acc: 0.8872\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88670 to 0.88720, saving model to DNST_CIFAR10_Conv_09_10_final-08-0.8872.hdf5\n",
      "epoch =  7 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/40\n",
      "391/390 [==============================] - 78s 199ms/step - loss: 0.2168 - acc: 0.9231 - val_loss: 0.4543 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88720\n",
      "epoch =  8 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/40\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.2158 - acc: 0.9228 - val_loss: 0.4617 - val_acc: 0.8876\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.88720 to 0.88760, saving model to DNST_CIFAR10_Conv_09_10_final-10-0.8876.hdf5\n",
      "epoch =  9 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 11/40\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 0.2148 - acc: 0.9225 - val_loss: 0.4384 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.88760 to 0.88970, saving model to DNST_CIFAR10_Conv_09_10_final-11-0.8897.hdf5\n",
      "epoch =  10 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 12/40\n",
      "391/390 [==============================] - 76s 196ms/step - loss: 0.2183 - acc: 0.9216 - val_loss: 0.4539 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88970\n",
      "epoch =  11 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 13/40\n",
      "391/390 [==============================] - 78s 198ms/step - loss: 0.2159 - acc: 0.9237 - val_loss: 0.4372 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.88970 to 0.88980, saving model to DNST_CIFAR10_Conv_09_10_final-13-0.8898.hdf5\n",
      "epoch =  12 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 14/40\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 0.2127 - acc: 0.9241 - val_loss: 0.4721 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88980\n",
      "epoch =  13 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 15/40\n",
      "391/390 [==============================] - 76s 196ms/step - loss: 0.2175 - acc: 0.9225 - val_loss: 0.4515 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88980\n",
      "epoch =  14 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 16/40\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 0.2144 - acc: 0.9232 - val_loss: 0.4648 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88980\n",
      "epoch =  15 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 17/40\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.2110 - acc: 0.9258 - val_loss: 0.5034 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88980\n",
      "epoch =  16 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 18/40\n",
      "391/390 [==============================] - 76s 195ms/step - loss: 0.2086 - acc: 0.9257 - val_loss: 0.4774 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88980\n",
      "epoch =  17 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 19/40\n",
      "391/390 [==============================] - 76s 195ms/step - loss: 0.2111 - acc: 0.9244 - val_loss: 0.5017 - val_acc: 0.8825\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88980\n",
      "epoch =  18 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 20/40\n",
      "391/390 [==============================] - 76s 195ms/step - loss: 0.2111 - acc: 0.9243 - val_loss: 0.4929 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88980\n",
      "epoch =  19 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 21/40\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.2113 - acc: 0.9246 - val_loss: 0.4935 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88980\n",
      "epoch =  20 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 22/40\n",
      "391/390 [==============================] - 76s 194ms/step - loss: 0.2080 - acc: 0.9263 - val_loss: 0.4641 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88980\n",
      "epoch =  21 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 23/40\n",
      "391/390 [==============================] - 76s 194ms/step - loss: 0.2098 - acc: 0.9256 - val_loss: 0.4986 - val_acc: 0.8827\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88980\n",
      "epoch =  22 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 24/40\n",
      "391/390 [==============================] - 76s 193ms/step - loss: 0.2033 - acc: 0.9268 - val_loss: 0.4876 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88980\n",
      "epoch =  23 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 25/40\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 0.2068 - acc: 0.9263 - val_loss: 0.4762 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88980\n",
      "epoch =  24 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 26/40\n",
      "391/390 [==============================] - 75s 193ms/step - loss: 0.2016 - acc: 0.9267 - val_loss: 0.4922 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88980\n",
      "epoch =  25 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 27/40\n",
      "391/390 [==============================] - 76s 193ms/step - loss: 0.2032 - acc: 0.9279 - val_loss: 0.4780 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88980\n",
      "epoch =  26 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 28/40\n",
      "173/390 [============>.................] - ETA: 40s - loss: 0.2071 - acc: 0.9272"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-975837102df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 40\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 1.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"DNST_CIFAR10_Conv_09_10_final_128p_epochs_CP.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 448us/step\n",
      "Test loss: 0.45804361943006516\n",
      "Test accuracy: 0.8894\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"DNST_CIFAR10_Conv_09_10_final_128p_epochs_CP.hdf5\")\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "score = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.05)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "261/260 [==============================] - 65s 250ms/step - loss: 0.1965 - acc: 0.9285 - val_loss: 0.4724 - val_acc: 0.8874\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.89180\n",
      "epoch =  0 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/110\n",
      "261/260 [==============================] - 65s 250ms/step - loss: 0.1912 - acc: 0.9324 - val_loss: 0.4680 - val_acc: 0.8873\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.89180\n",
      "epoch =  1 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/110\n",
      "261/260 [==============================] - 66s 251ms/step - loss: 0.1902 - acc: 0.9323 - val_loss: 0.4631 - val_acc: 0.8894\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.89180\n",
      "epoch =  2 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/110\n",
      "261/260 [==============================] - 65s 249ms/step - loss: 0.1912 - acc: 0.9319 - val_loss: 0.4667 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.89180\n",
      "epoch =  3 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1906 - acc: 0.9325 - val_loss: 0.4602 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.89180\n",
      "epoch =  4 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1907 - acc: 0.9333 - val_loss: 0.4658 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89180\n",
      "epoch =  5 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1896 - acc: 0.9327 - val_loss: 0.4629 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.89180\n",
      "epoch =  6 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/110\n",
      "261/260 [==============================] - 65s 250ms/step - loss: 0.1872 - acc: 0.9343 - val_loss: 0.4659 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89180\n",
      "epoch =  7 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1906 - acc: 0.9318 - val_loss: 0.4616 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89180\n",
      "epoch =  8 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1845 - acc: 0.9334 - val_loss: 0.4612 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89180\n",
      "epoch =  9 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 11/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1864 - acc: 0.9320 - val_loss: 0.4653 - val_acc: 0.8892\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89180\n",
      "epoch =  10 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 12/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1847 - acc: 0.9344 - val_loss: 0.4724 - val_acc: 0.8876\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89180\n",
      "epoch =  11 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 13/110\n",
      "261/260 [==============================] - 65s 249ms/step - loss: 0.1858 - acc: 0.9345 - val_loss: 0.4733 - val_acc: 0.8893\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89180\n",
      "epoch =  12 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 14/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1855 - acc: 0.9341 - val_loss: 0.4752 - val_acc: 0.8866\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89180\n",
      "epoch =  13 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 15/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1870 - acc: 0.9338 - val_loss: 0.4750 - val_acc: 0.8877\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89180\n",
      "epoch =  14 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 16/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1862 - acc: 0.9333 - val_loss: 0.4644 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89180\n",
      "epoch =  15 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 17/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1861 - acc: 0.9325 - val_loss: 0.4700 - val_acc: 0.8893\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89180\n",
      "epoch =  16 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 18/110\n",
      "261/260 [==============================] - 65s 250ms/step - loss: 0.1837 - acc: 0.9334 - val_loss: 0.4669 - val_acc: 0.8871\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89180\n",
      "epoch =  17 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 19/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1833 - acc: 0.9356 - val_loss: 0.4811 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89180\n",
      "epoch =  18 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 20/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1799 - acc: 0.9359 - val_loss: 0.4771 - val_acc: 0.8893\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89180\n",
      "epoch =  19 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 21/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1810 - acc: 0.9345 - val_loss: 0.4798 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.89180\n",
      "epoch =  20 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 22/110\n",
      "261/260 [==============================] - 65s 249ms/step - loss: 0.1826 - acc: 0.9356 - val_loss: 0.4745 - val_acc: 0.8876\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.89180\n",
      "epoch =  21 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 23/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1844 - acc: 0.9336 - val_loss: 0.4803 - val_acc: 0.8877\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.89180\n",
      "epoch =  22 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 24/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1811 - acc: 0.9352 - val_loss: 0.4617 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.89180\n",
      "epoch =  23 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 25/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1829 - acc: 0.9339 - val_loss: 0.4953 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.89180\n",
      "epoch =  24 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 26/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1795 - acc: 0.9355 - val_loss: 0.4750 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.89180\n",
      "epoch =  25 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 27/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1832 - acc: 0.9343 - val_loss: 0.4667 - val_acc: 0.8908\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.89180\n",
      "epoch =  26 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 28/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1801 - acc: 0.9347 - val_loss: 0.4661 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.89180\n",
      "epoch =  27 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 29/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1804 - acc: 0.9353 - val_loss: 0.4763 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.89180\n",
      "epoch =  28 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 30/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1824 - acc: 0.9346 - val_loss: 0.4757 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.89180\n",
      "epoch =  29 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 31/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1781 - acc: 0.9361 - val_loss: 0.4780 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.89180\n",
      "epoch =  30 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 32/110\n",
      "261/260 [==============================] - 65s 249ms/step - loss: 0.1803 - acc: 0.9360 - val_loss: 0.4801 - val_acc: 0.8893\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.89180\n",
      "epoch =  31 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 33/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1785 - acc: 0.9364 - val_loss: 0.4758 - val_acc: 0.8891\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.89180\n",
      "epoch =  32 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 34/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1818 - acc: 0.9339 - val_loss: 0.4973 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.89180\n",
      "epoch =  33 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 35/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1824 - acc: 0.9349 - val_loss: 0.4814 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89180\n",
      "epoch =  34 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 36/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1796 - acc: 0.9356 - val_loss: 0.4785 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89180\n",
      "epoch =  35 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 37/110\n",
      "261/260 [==============================] - 65s 249ms/step - loss: 0.1819 - acc: 0.9349 - val_loss: 0.4844 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89180\n",
      "epoch =  36 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 38/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1825 - acc: 0.9353 - val_loss: 0.4726 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89180\n",
      "epoch =  37 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 39/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1781 - acc: 0.9354 - val_loss: 0.4743 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.89180\n",
      "epoch =  38 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 40/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1777 - acc: 0.9370 - val_loss: 0.4818 - val_acc: 0.8879\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89180\n",
      "epoch =  39 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 41/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1780 - acc: 0.9365 - val_loss: 0.4866 - val_acc: 0.8866\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89180\n",
      "epoch =  40 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 42/110\n",
      "261/260 [==============================] - 65s 250ms/step - loss: 0.1785 - acc: 0.9364 - val_loss: 0.4762 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89180\n",
      "epoch =  41 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 43/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1816 - acc: 0.9350 - val_loss: 0.4738 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89180\n",
      "epoch =  42 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 44/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1795 - acc: 0.9361 - val_loss: 0.4809 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89180\n",
      "epoch =  43 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 45/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1753 - acc: 0.9372 - val_loss: 0.4829 - val_acc: 0.8877\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89180\n",
      "epoch =  44 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 46/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1811 - acc: 0.9362 - val_loss: 0.4677 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89180\n",
      "epoch =  45 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 47/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1747 - acc: 0.9379 - val_loss: 0.4678 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89180\n",
      "epoch =  46 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 48/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1769 - acc: 0.9386 - val_loss: 0.4775 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89180\n",
      "epoch =  47 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 49/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1787 - acc: 0.9361 - val_loss: 0.4647 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89180\n",
      "epoch =  48 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 50/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1745 - acc: 0.9379 - val_loss: 0.4805 - val_acc: 0.8892\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89180\n",
      "epoch =  49 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 51/110\n",
      "261/260 [==============================] - 65s 250ms/step - loss: 0.1775 - acc: 0.9362 - val_loss: 0.4870 - val_acc: 0.8881\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89180\n",
      "epoch =  50 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 52/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1749 - acc: 0.9376 - val_loss: 0.4663 - val_acc: 0.8915\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89180\n",
      "epoch =  51 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 53/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1783 - acc: 0.9357 - val_loss: 0.4801 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89180\n",
      "epoch =  52 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 54/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1785 - acc: 0.9364 - val_loss: 0.4788 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89180\n",
      "epoch =  53 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 55/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1763 - acc: 0.9366 - val_loss: 0.4680 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89180\n",
      "epoch =  54 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 56/110\n",
      "261/260 [==============================] - 65s 249ms/step - loss: 0.1734 - acc: 0.9371 - val_loss: 0.4775 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89180\n",
      "epoch =  55 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 57/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1776 - acc: 0.9362 - val_loss: 0.4692 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89180\n",
      "epoch =  56 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 58/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1755 - acc: 0.9371 - val_loss: 0.4808 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89180\n",
      "epoch =  57 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 59/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1770 - acc: 0.9361 - val_loss: 0.4792 - val_acc: 0.8892\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89180\n",
      "epoch =  58 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 60/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1776 - acc: 0.9370 - val_loss: 0.4764 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89180\n",
      "epoch =  59 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 61/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1749 - acc: 0.9376 - val_loss: 0.4821 - val_acc: 0.8877\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89180\n",
      "epoch =  60 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 62/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1776 - acc: 0.9359 - val_loss: 0.4797 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89180\n",
      "epoch =  61 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 63/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1740 - acc: 0.9378 - val_loss: 0.4681 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89180\n",
      "epoch =  62 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 64/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1760 - acc: 0.9369 - val_loss: 0.4756 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89180\n",
      "epoch =  63 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 65/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1793 - acc: 0.9361 - val_loss: 0.4761 - val_acc: 0.8901\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89180\n",
      "epoch =  64 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 66/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1741 - acc: 0.9368 - val_loss: 0.4988 - val_acc: 0.8852\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89180\n",
      "epoch =  65 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 67/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1771 - acc: 0.9370 - val_loss: 0.4793 - val_acc: 0.8891\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89180\n",
      "epoch =  66 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 68/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1745 - acc: 0.9373 - val_loss: 0.4973 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89180\n",
      "epoch =  67 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 69/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1741 - acc: 0.9377 - val_loss: 0.4933 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89180\n",
      "epoch =  68 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 70/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1734 - acc: 0.9381 - val_loss: 0.4853 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89180\n",
      "epoch =  69 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 71/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1753 - acc: 0.9366 - val_loss: 0.4826 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89180\n",
      "epoch =  70 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 72/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1765 - acc: 0.9371 - val_loss: 0.4872 - val_acc: 0.8874\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89180\n",
      "epoch =  71 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 73/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1733 - acc: 0.9384 - val_loss: 0.4722 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89180\n",
      "epoch =  72 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 74/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1714 - acc: 0.9382 - val_loss: 0.5028 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89180\n",
      "epoch =  73 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 75/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1737 - acc: 0.9375 - val_loss: 0.4823 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89180\n",
      "epoch =  74 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 76/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1702 - acc: 0.9389 - val_loss: 0.4794 - val_acc: 0.8899\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89180\n",
      "epoch =  75 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 77/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1706 - acc: 0.9402 - val_loss: 0.4708 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89180\n",
      "epoch =  76 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 78/110\n",
      "261/260 [==============================] - 64s 244ms/step - loss: 0.1728 - acc: 0.9389 - val_loss: 0.4733 - val_acc: 0.8914\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89180\n",
      "epoch =  77 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 79/110\n",
      "261/260 [==============================] - 64s 244ms/step - loss: 0.1725 - acc: 0.9383 - val_loss: 0.4747 - val_acc: 0.8914\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89180\n",
      "epoch =  78 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 80/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1710 - acc: 0.9391 - val_loss: 0.4851 - val_acc: 0.8901\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89180\n",
      "epoch =  79 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 81/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1717 - acc: 0.9389 - val_loss: 0.4945 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89180\n",
      "epoch =  80 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 82/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1702 - acc: 0.9395 - val_loss: 0.4995 - val_acc: 0.8869\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89180\n",
      "epoch =  81 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 83/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1743 - acc: 0.9391 - val_loss: 0.4812 - val_acc: 0.8893\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89180\n",
      "epoch =  82 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 84/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1693 - acc: 0.9390 - val_loss: 0.4745 - val_acc: 0.8894\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89180\n",
      "epoch =  83 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 85/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1720 - acc: 0.9381 - val_loss: 0.4777 - val_acc: 0.8908\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89180\n",
      "epoch =  84 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 86/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1737 - acc: 0.9375 - val_loss: 0.4659 - val_acc: 0.8943\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.89180 to 0.89430, saving model to DNST_CIFAR10_Conv_09_10_final-86-0.8943.hdf5\n",
      "epoch =  85 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 87/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1695 - acc: 0.9400 - val_loss: 0.4746 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89430\n",
      "epoch =  86 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 88/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1686 - acc: 0.9397 - val_loss: 0.4924 - val_acc: 0.8894\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89430\n",
      "epoch =  87 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 89/110\n",
      "261/260 [==============================] - 65s 247ms/step - loss: 0.1689 - acc: 0.9395 - val_loss: 0.4762 - val_acc: 0.8918\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89430\n",
      "epoch =  88 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 90/110\n",
      "261/260 [==============================] - 65s 250ms/step - loss: 0.1708 - acc: 0.9394 - val_loss: 0.4893 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89430\n",
      "epoch =  89 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 91/110\n",
      "261/260 [==============================] - 64s 247ms/step - loss: 0.1687 - acc: 0.9394 - val_loss: 0.4644 - val_acc: 0.8921\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89430\n",
      "epoch =  90 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 92/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1698 - acc: 0.9379 - val_loss: 0.4661 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89430\n",
      "epoch =  91 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 93/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1694 - acc: 0.9383 - val_loss: 0.4812 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89430\n",
      "epoch =  92 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 94/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1681 - acc: 0.9390 - val_loss: 0.5014 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89430\n",
      "epoch =  93 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 95/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1692 - acc: 0.9385 - val_loss: 0.4842 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89430\n",
      "epoch =  94 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 96/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1713 - acc: 0.9383 - val_loss: 0.4743 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89430\n",
      "epoch =  95 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 97/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1690 - acc: 0.9399 - val_loss: 0.4796 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89430\n",
      "epoch =  96 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 98/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1687 - acc: 0.9402 - val_loss: 0.4774 - val_acc: 0.8923\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89430\n",
      "epoch =  97 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 99/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1690 - acc: 0.9386 - val_loss: 0.4844 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89430\n",
      "epoch =  98 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 100/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1669 - acc: 0.9407 - val_loss: 0.4922 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89430\n",
      "epoch =  99 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 101/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1700 - acc: 0.9386 - val_loss: 0.4732 - val_acc: 0.8894\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89430\n",
      "epoch =  100 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 102/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1683 - acc: 0.9396 - val_loss: 0.4808 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89430\n",
      "epoch =  101 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 103/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1693 - acc: 0.9396 - val_loss: 0.4904 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89430\n",
      "epoch =  102 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 104/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1681 - acc: 0.9399 - val_loss: 0.4806 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89430\n",
      "epoch =  103 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 105/110\n",
      "261/260 [==============================] - 64s 245ms/step - loss: 0.1689 - acc: 0.9391 - val_loss: 0.4841 - val_acc: 0.8905\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89430\n",
      "epoch =  104 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 106/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1666 - acc: 0.9399 - val_loss: 0.4851 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89430\n",
      "epoch =  105 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 107/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1649 - acc: 0.9409 - val_loss: 0.4852 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89430\n",
      "epoch =  106 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 108/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1646 - acc: 0.9410 - val_loss: 0.4821 - val_acc: 0.8905\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89430\n",
      "epoch =  107 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 109/110\n",
      "261/260 [==============================] - 65s 248ms/step - loss: 0.1645 - acc: 0.9407 - val_loss: 0.4856 - val_acc: 0.8901\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89430\n",
      "epoch =  108 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 110/110\n",
      "261/260 [==============================] - 64s 246ms/step - loss: 0.1665 - acc: 0.9402 - val_loss: 0.4672 - val_acc: 0.8919\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89430\n",
      "epoch =  109 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 192\n",
    "epochs = 110\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 1.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 192\n",
    "epochs = 100\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 1.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1133 - acc: 0.9596 - val_loss: 0.3938 - val_acc: 0.9109\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/20\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1155 - acc: 0.9588 - val_loss: 0.3899 - val_acc: 0.9108\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/20\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1110 - acc: 0.9604 - val_loss: 0.4234 - val_acc: 0.9085\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/20\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1108 - acc: 0.9600 - val_loss: 0.3743 - val_acc: 0.9144\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/20\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1089 - acc: 0.9606 - val_loss: 0.4073 - val_acc: 0.9121\n",
      "end, epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "Epoch 6/20\n",
      "start , epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1062 - acc: 0.9618 - val_loss: 0.4523 - val_acc: 0.9041\n",
      "end, epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "Epoch 7/20\n",
      "start , epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1083 - acc: 0.9616 - val_loss: 0.3759 - val_acc: 0.9155\n",
      "end, epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "Epoch 8/20\n",
      "start , epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1044 - acc: 0.9630 - val_loss: 0.4134 - val_acc: 0.9105\n",
      "end, epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "Epoch 9/20\n",
      "start , epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1048 - acc: 0.9625 - val_loss: 0.3635 - val_acc: 0.9210\n",
      "end, epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "Epoch 10/20\n",
      "start , epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1011 - acc: 0.9638 - val_loss: 0.3794 - val_acc: 0.9177\n",
      "end, epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "Epoch 11/20\n",
      "start , epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1017 - acc: 0.9633 - val_loss: 0.4528 - val_acc: 0.9047\n",
      "end, epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "Epoch 12/20\n",
      "start , epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.0999 - acc: 0.9648 - val_loss: 0.4364 - val_acc: 0.9077\n",
      "end, epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "Epoch 13/20\n",
      "start , epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 270ms/step - loss: 0.0989 - acc: 0.9639 - val_loss: 0.4290 - val_acc: 0.9071\n",
      "end, epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "Epoch 14/20\n",
      "start , epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.0986 - acc: 0.9648 - val_loss: 0.3866 - val_acc: 0.9185\n",
      "end, epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "Epoch 15/20\n",
      "start , epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.0995 - acc: 0.9645 - val_loss: 0.4332 - val_acc: 0.9086\n",
      "end, epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "Epoch 16/20\n",
      "start , epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.0959 - acc: 0.9652 - val_loss: 0.4299 - val_acc: 0.9109\n",
      "end, epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "Epoch 17/20\n",
      "start , epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.0985 - acc: 0.9647 - val_loss: 0.4761 - val_acc: 0.9014\n",
      "end, epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "Epoch 18/20\n",
      "start , epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.0944 - acc: 0.9663 - val_loss: 0.4014 - val_acc: 0.9184\n",
      "end, epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "Epoch 19/20\n",
      "start , epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.0950 - acc: 0.9662 - val_loss: 0.3941 - val_acc: 0.9163\n",
      "end, epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "Epoch 20/20\n",
      "start , epoch =  19 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.0896 - acc: 0.9677 - val_loss: 0.4334 - val_acc: 0.9110\n",
      "end, epoch =  19 , lr =  0.001 , decay =  0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 190\n",
    "epochs = 20\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 2.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.01)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1781
    },
    "colab_type": "code",
    "id": "KbD-T7m0G6lP",
    "outputId": "198b9368-e44a-461d-a822-15e6901dc520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.3081 - acc: 0.8911 - val_loss: 0.4591 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.86310 to 0.86530, saving model to DNST_CIFAR10_Conv_09_03-01-0.8653.hdf5\n",
      "epoch =  0 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/60\n",
      "782/781 [==============================] - 124s 159ms/step - loss: 0.3000 - acc: 0.8931 - val_loss: 0.4573 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86530 to 0.86580, saving model to DNST_CIFAR10_Conv_09_03-02-0.8658.hdf5\n",
      "epoch =  1 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2992 - acc: 0.8929 - val_loss: 0.4548 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86580 to 0.86690, saving model to DNST_CIFAR10_Conv_09_03-03-0.8669.hdf5\n",
      "epoch =  2 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.3003 - acc: 0.8937 - val_loss: 0.4554 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86690\n",
      "epoch =  3 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2971 - acc: 0.8952 - val_loss: 0.4551 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86690\n",
      "epoch =  4 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2956 - acc: 0.8946 - val_loss: 0.4575 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86690\n",
      "epoch =  5 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.3015 - acc: 0.8936 - val_loss: 0.4637 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86690\n",
      "epoch =  6 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2970 - acc: 0.8933 - val_loss: 0.4593 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86690\n",
      "epoch =  7 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2967 - acc: 0.8938 - val_loss: 0.4617 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86690\n",
      "epoch =  8 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/60\n",
      "782/781 [==============================] - 122s 157ms/step - loss: 0.2978 - acc: 0.8944 - val_loss: 0.4546 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.86690 to 0.86750, saving model to DNST_CIFAR10_Conv_09_03-10-0.8675.hdf5\n",
      "epoch =  9 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 11/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2955 - acc: 0.8955 - val_loss: 0.4630 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86750\n",
      "epoch =  10 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 12/60\n",
      "782/781 [==============================] - 125s 159ms/step - loss: 0.2956 - acc: 0.8949 - val_loss: 0.4592 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86750\n",
      "epoch =  11 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 13/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2892 - acc: 0.8969 - val_loss: 0.4599 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86750\n",
      "epoch =  12 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 14/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2880 - acc: 0.8972 - val_loss: 0.4627 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86750\n",
      "epoch =  13 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 15/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2910 - acc: 0.8964 - val_loss: 0.4618 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86750\n",
      "epoch =  14 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 16/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2911 - acc: 0.8973 - val_loss: 0.4637 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86750\n",
      "epoch =  15 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 17/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2908 - acc: 0.8968 - val_loss: 0.4564 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86750\n",
      "epoch =  16 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 18/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2919 - acc: 0.8959 - val_loss: 0.4587 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86750\n",
      "epoch =  17 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 19/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2881 - acc: 0.8976 - val_loss: 0.4573 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86750\n",
      "epoch =  18 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 20/60\n",
      "782/781 [==============================] - 124s 158ms/step - loss: 0.2912 - acc: 0.8958 - val_loss: 0.4579 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86750\n",
      "epoch =  19 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 21/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2923 - acc: 0.8952 - val_loss: 0.4626 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86750\n",
      "epoch =  20 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 22/60\n",
      "782/781 [==============================] - 124s 158ms/step - loss: 0.2868 - acc: 0.8985 - val_loss: 0.4568 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86750\n",
      "epoch =  21 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 23/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2925 - acc: 0.8967 - val_loss: 0.4636 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86750\n",
      "epoch =  22 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 24/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2891 - acc: 0.8970 - val_loss: 0.4595 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86750\n",
      "epoch =  23 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 25/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2911 - acc: 0.8965 - val_loss: 0.4651 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86750\n",
      "epoch =  24 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 26/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2835 - acc: 0.8980 - val_loss: 0.4641 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86750\n",
      "epoch =  25 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 27/60\n",
      "782/781 [==============================] - 124s 159ms/step - loss: 0.2910 - acc: 0.8948 - val_loss: 0.4661 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86750\n",
      "epoch =  26 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 28/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2870 - acc: 0.8996 - val_loss: 0.4631 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.86750\n",
      "epoch =  27 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 29/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2910 - acc: 0.8972 - val_loss: 0.4663 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.86750\n",
      "epoch =  28 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 30/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2862 - acc: 0.8978 - val_loss: 0.4664 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86750\n",
      "epoch =  29 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 31/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2834 - acc: 0.8988 - val_loss: 0.4645 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.86750\n",
      "epoch =  30 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 32/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2880 - acc: 0.8975 - val_loss: 0.4660 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.86750\n",
      "epoch =  31 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 33/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2918 - acc: 0.8960 - val_loss: 0.4604 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.86750\n",
      "epoch =  32 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 34/60\n",
      "782/781 [==============================] - 124s 159ms/step - loss: 0.2857 - acc: 0.8975 - val_loss: 0.4603 - val_acc: 0.8660\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.86750\n",
      "epoch =  33 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 35/60\n",
      "782/781 [==============================] - 124s 159ms/step - loss: 0.2908 - acc: 0.8967 - val_loss: 0.4611 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.86750\n",
      "epoch =  34 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 36/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2871 - acc: 0.8982 - val_loss: 0.4682 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.86750\n",
      "epoch =  35 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 37/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2807 - acc: 0.9007 - val_loss: 0.4632 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.86750\n",
      "epoch =  36 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 38/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2865 - acc: 0.8983 - val_loss: 0.4579 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.86750\n",
      "epoch =  37 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 39/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2870 - acc: 0.8982 - val_loss: 0.4607 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.86750\n",
      "epoch =  38 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 40/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2816 - acc: 0.8994 - val_loss: 0.4626 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.86750\n",
      "epoch =  39 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 41/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2846 - acc: 0.8989 - val_loss: 0.4602 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.86750\n",
      "epoch =  40 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 42/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2849 - acc: 0.8993 - val_loss: 0.4633 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.86750\n",
      "epoch =  41 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 43/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2799 - acc: 0.9006 - val_loss: 0.4610 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.86750\n",
      "epoch =  42 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 44/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2905 - acc: 0.8958 - val_loss: 0.4635 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.86750\n",
      "epoch =  43 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 45/60\n",
      "782/781 [==============================] - 125s 160ms/step - loss: 0.2860 - acc: 0.8987 - val_loss: 0.4631 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86750\n",
      "epoch =  44 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 46/60\n",
      "782/781 [==============================] - 124s 158ms/step - loss: 0.2844 - acc: 0.8970 - val_loss: 0.4645 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.86750\n",
      "epoch =  45 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 47/60\n",
      "782/781 [==============================] - 122s 157ms/step - loss: 0.2837 - acc: 0.8994 - val_loss: 0.4639 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.86750\n",
      "epoch =  46 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 48/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2858 - acc: 0.8990 - val_loss: 0.4651 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.86750\n",
      "epoch =  47 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 49/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2855 - acc: 0.8998 - val_loss: 0.4647 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.86750\n",
      "epoch =  48 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 50/60\n",
      "782/781 [==============================] - 124s 159ms/step - loss: 0.2868 - acc: 0.8964 - val_loss: 0.4662 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.86750\n",
      "epoch =  49 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 51/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2802 - acc: 0.9008 - val_loss: 0.4611 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.86750\n",
      "epoch =  50 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 52/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2879 - acc: 0.8971 - val_loss: 0.4639 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.86750\n",
      "epoch =  51 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 53/60\n",
      "782/781 [==============================] - 124s 158ms/step - loss: 0.2840 - acc: 0.9001 - val_loss: 0.4680 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.86750\n",
      "epoch =  52 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 54/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2811 - acc: 0.8995 - val_loss: 0.4644 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.86750\n",
      "epoch =  53 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 55/60\n",
      "782/781 [==============================] - 124s 158ms/step - loss: 0.2850 - acc: 0.8995 - val_loss: 0.4669 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.86750\n",
      "epoch =  54 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 56/60\n",
      "782/781 [==============================] - 122s 155ms/step - loss: 0.2824 - acc: 0.8994 - val_loss: 0.4652 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.86750\n",
      "epoch =  55 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 57/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2820 - acc: 0.9008 - val_loss: 0.4670 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.86750\n",
      "epoch =  56 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 58/60\n",
      "782/781 [==============================] - 123s 157ms/step - loss: 0.2844 - acc: 0.8978 - val_loss: 0.4667 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.86750\n",
      "epoch =  57 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 59/60\n",
      "782/781 [==============================] - 122s 156ms/step - loss: 0.2787 - acc: 0.9013 - val_loss: 0.4630 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.86750\n",
      "epoch =  58 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 60/60\n",
      "782/781 [==============================] - 123s 158ms/step - loss: 0.2841 - acc: 0.8993 - val_loss: 0.4649 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.86750\n",
      "epoch =  59 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 60\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 1 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imdB9nUyoCn0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "eEgYnYbSoDJg",
    "outputId": "bdc7ee86-c7dd-4905-ed02-b91d18c0424f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1373
    },
    "colab_type": "code",
    "id": "LLcxWH2hoDXw",
    "outputId": "5143ffbd-b989-4dc5-c1fc-3ee16dfe0d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1563/1562 [==============================] - 479s 306ms/step - loss: 0.3255 - acc: 0.8856 - val_loss: 0.4556 - val_acc: 0.8753\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87530, saving model to /content/gdrive/DNST_CIFAR10_Conv_09-01-0.8753.hdf5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-8641d232ca3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2266\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    445\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \"\"\"\n\u001b[1;32m   2590\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2591\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/content/gdrive/DNST_CIFAR10_Conv_09-01-0.8753.hdf5', errno = 95, error message = 'Operation not supported', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 2 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rpig1ugEsLfe"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('Yolo_Basic_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H0PIL3pysLxt",
    "outputId": "f4855f05-c410-49f0-839b-c1c62e169196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"DNST_CIFAR10_Conv_09-{epoch:02d}-{val_acc:.4f}.hdf5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn3m4_EjsMGE"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('DNST_CIFAR10_Conv_09-{epoch:02d}-{val_acc:.4f}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z4PzRvhLvHsK",
    "outputId": "01b9cc7a-df0a-41c5-e312-eab3b46fb24a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"DNST_CIFAR10_Conv_09-8753.hdf5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ethaY4Y2wi4I"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('DNST_CIFAR10_Conv_09-8753.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0yrzJxXw0rM"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [reduce_lr, sgd_lr_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "A5uOHjlkw045",
    "outputId": "db583ad7-967e-416f-f212-7b83f77f3c6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1562 [==============================] - 483s 309ms/step - loss: 0.3218 - acc: 0.8858 - val_loss: 0.5009 - val_acc: 0.8662\n",
      "epoch =  0 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/10\n",
      "1563/1562 [==============================] - 481s 308ms/step - loss: 0.3188 - acc: 0.8869 - val_loss: 0.4564 - val_acc: 0.8762\n",
      "epoch =  1 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/10\n",
      "1563/1562 [==============================] - 481s 308ms/step - loss: 0.3142 - acc: 0.8877 - val_loss: 0.4933 - val_acc: 0.8682\n",
      "epoch =  2 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/10\n",
      "1563/1562 [==============================] - 481s 308ms/step - loss: 0.3168 - acc: 0.8882 - val_loss: 0.4712 - val_acc: 0.8749\n",
      "epoch =  3 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/10\n",
      "1563/1562 [==============================] - 480s 307ms/step - loss: 0.3098 - acc: 0.8895 - val_loss: 0.4724 - val_acc: 0.8723\n",
      "epoch =  4 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/10\n",
      "1563/1562 [==============================] - 479s 307ms/step - loss: 0.3074 - acc: 0.8901 - val_loss: 0.4926 - val_acc: 0.8706\n",
      "epoch =  5 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/10\n",
      "1563/1562 [==============================] - 477s 305ms/step - loss: 0.3081 - acc: 0.8904 - val_loss: 0.4645 - val_acc: 0.8759\n",
      "epoch =  6 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/10\n",
      "1563/1562 [==============================] - 476s 305ms/step - loss: 0.3038 - acc: 0.8912 - val_loss: 0.5058 - val_acc: 0.8652\n",
      "epoch =  7 , lr =  0.1 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/10\n",
      "1563/1562 [==============================] - 476s 304ms/step - loss: 0.3017 - acc: 0.8923 - val_loss: 0.4680 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "epoch =  8 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/10\n",
      "1563/1562 [==============================] - 476s 304ms/step - loss: 0.2874 - acc: 0.8977 - val_loss: 0.4869 - val_acc: 0.8738\n",
      "epoch =  9 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 2 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "EpyFvpU0w1GY",
    "outputId": "8e3f9817-ef81-4709-f332-689ab3a7eba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1562 [==============================] - 478s 306ms/step - loss: 0.2864 - acc: 0.8983 - val_loss: 0.4615 - val_acc: 0.8770\n",
      "epoch =  0 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/10\n",
      "1563/1562 [==============================] - 477s 305ms/step - loss: 0.2874 - acc: 0.8973 - val_loss: 0.4415 - val_acc: 0.8826\n",
      "epoch =  1 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/10\n",
      "1563/1562 [==============================] - 475s 304ms/step - loss: 0.2812 - acc: 0.9003 - val_loss: 0.4585 - val_acc: 0.8797\n",
      "epoch =  2 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/10\n",
      "1563/1562 [==============================] - 475s 304ms/step - loss: 0.2811 - acc: 0.9005 - val_loss: 0.4513 - val_acc: 0.8810\n",
      "epoch =  3 , lr =  0.05 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/10\n",
      "1563/1562 [==============================] - 477s 305ms/step - loss: 0.2795 - acc: 0.9013 - val_loss: 0.4491 - val_acc: 0.8793\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "epoch =  4 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/10\n",
      "1563/1562 [==============================] - 481s 308ms/step - loss: 0.2745 - acc: 0.9024 - val_loss: 0.4465 - val_acc: 0.8810\n",
      "epoch =  5 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/10\n",
      "1563/1562 [==============================] - 482s 308ms/step - loss: 0.2738 - acc: 0.9034 - val_loss: 0.4462 - val_acc: 0.8829\n",
      "epoch =  6 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/10\n",
      "1563/1562 [==============================] - 481s 307ms/step - loss: 0.2697 - acc: 0.9044 - val_loss: 0.4580 - val_acc: 0.8827\n",
      "epoch =  7 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/10\n",
      "1563/1562 [==============================] - 481s 308ms/step - loss: 0.2717 - acc: 0.9034 - val_loss: 0.4518 - val_acc: 0.8845\n",
      "epoch =  8 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/10\n",
      "1563/1562 [==============================] - 481s 308ms/step - loss: 0.2675 - acc: 0.9043 - val_loss: 0.4606 - val_acc: 0.8823\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "epoch =  9 , lr =  0.0125 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 2 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x5ZsYV6yw1DV",
    "outputId": "b98facfe-1bd6-40bb-8f47-bd6027867392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"DNST_CIFAR10_Conv_09-8823.hdf5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWzP6Edpw0_x"
   },
   "outputs": [],
   "source": [
    "files.download('DNST_CIFAR10_Conv_09-8823.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_HMxDUvWZ0F"
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience= 4, min_delta=0.004, verbose=1, cooldown=0, min_lr=0.001)\n",
    "callbacks_list = [reduce_lr, sgd_lr_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "eadO_adxWaFe",
    "outputId": "c8909100-25c9-4a0b-f79d-e0fece23c4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1562 [==============================] - 481s 308ms/step - loss: 0.2684 - acc: 0.9047 - val_loss: 0.4599 - val_acc: 0.8813\n",
      "epoch =  0 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/10\n",
      "1563/1562 [==============================] - 486s 311ms/step - loss: 0.2674 - acc: 0.9048 - val_loss: 0.4478 - val_acc: 0.8850\n",
      "epoch =  1 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/10\n",
      "1563/1562 [==============================] - 485s 311ms/step - loss: 0.2653 - acc: 0.9050 - val_loss: 0.4503 - val_acc: 0.8833\n",
      "epoch =  2 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/10\n",
      "1563/1562 [==============================] - 486s 311ms/step - loss: 0.2659 - acc: 0.9053 - val_loss: 0.4546 - val_acc: 0.8836\n",
      "epoch =  3 , lr =  0.025 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/10\n",
      "1563/1562 [==============================] - 483s 309ms/step - loss: 0.2644 - acc: 0.9063 - val_loss: 0.4456 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "epoch =  4 , lr =  0.0125 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/10\n",
      "1563/1562 [==============================] - 482s 308ms/step - loss: 0.2620 - acc: 0.9073 - val_loss: 0.4529 - val_acc: 0.8840\n",
      "epoch =  5 , lr =  0.0125 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/10\n",
      "1563/1562 [==============================] - 485s 310ms/step - loss: 0.2614 - acc: 0.9065 - val_loss: 0.4499 - val_acc: 0.8853\n",
      "epoch =  6 , lr =  0.0125 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/10\n",
      "1563/1562 [==============================] - 487s 312ms/step - loss: 0.2636 - acc: 0.9063 - val_loss: 0.4418 - val_acc: 0.8851\n",
      "epoch =  7 , lr =  0.0125 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/10\n",
      "1563/1562 [==============================] - 486s 311ms/step - loss: 0.2605 - acc: 0.9057 - val_loss: 0.4454 - val_acc: 0.8846\n",
      "epoch =  8 , lr =  0.0125 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/10\n",
      "1563/1562 [==============================] - 480s 307ms/step - loss: 0.2616 - acc: 0.9067 - val_loss: 0.4538 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "epoch =  9 , lr =  0.00625 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 2 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_32NGkhiprV7",
    "outputId": "df92c9ac-63ea-4d1a-912f-d7562598d5ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"DNST_CIFAR10_Conv_09-8831.hdf5\")\n",
    "print(\"Saved model to disk\")\n",
    "files.download('DNST_CIFAR10_Conv_09-8831.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restarting from GC - Epochs done: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 90\n",
    "decay = learning_rate/epochs\n",
    "decay = 0.0001\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=learning_rate, decay=decay, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMEISrvNprzz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 811us/step\n",
      "Test loss: 0.4537913758456707\n",
      "Test accuracy: 0.8831\n"
     ]
    }
   ],
   "source": [
    "load_model_from_back = True\n",
    "\n",
    "if load_model_from_back:\n",
    "    model.load_weights('DNST_CIFAR10_Conv_09-8831.hdf5')\n",
    "    score = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.01)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "clr_triangular = CyclicLR(mode='triangular2', base_lr = 0.008, max_lr = 0.025, step_size = (len(x_train)* 2 * 5)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience= 5, min_delta=0.002, verbose=1, cooldown=0, min_lr=0.001)\n",
    "callbacks_list = [checkpoint, sgd_lr_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,    # randomly flip images\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 247s 158ms/step - loss: 0.2804 - acc: 0.9015 - val_loss: 0.4936 - val_acc: 0.8746\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.88620\n",
      "epoch =  0 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2812 - acc: 0.8998 - val_loss: 0.4642 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.88620\n",
      "epoch =  1 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2753 - acc: 0.9020 - val_loss: 0.4753 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.88620\n",
      "epoch =  2 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2727 - acc: 0.9017 - val_loss: 0.4598 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.88620\n",
      "epoch =  3 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2700 - acc: 0.9033 - val_loss: 0.4615 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.88620\n",
      "epoch =  4 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2699 - acc: 0.9027 - val_loss: 0.4661 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88620\n",
      "epoch =  5 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 7/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2693 - acc: 0.9041 - val_loss: 0.4612 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88620\n",
      "epoch =  6 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2667 - acc: 0.9048 - val_loss: 0.4547 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88620\n",
      "epoch =  7 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2618 - acc: 0.9066 - val_loss: 0.4574 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88620\n",
      "epoch =  8 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2591 - acc: 0.9071 - val_loss: 0.4504 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88620\n",
      "epoch =  9 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 226s 144ms/step - loss: 0.2586 - acc: 0.9080 - val_loss: 0.4404 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88620\n",
      "epoch =  10 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 224s 144ms/step - loss: 0.2574 - acc: 0.9080 - val_loss: 0.4533 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88620\n",
      "epoch =  11 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2561 - acc: 0.9084 - val_loss: 0.4523 - val_acc: 0.8853\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88620\n",
      "epoch =  12 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2572 - acc: 0.9080 - val_loss: 0.4540 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88620\n",
      "epoch =  13 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 226s 145ms/step - loss: 0.2545 - acc: 0.9093 - val_loss: 0.4603 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88620\n",
      "epoch =  14 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2513 - acc: 0.9098 - val_loss: 0.4546 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88620\n",
      "epoch =  15 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 17/50\n",
      "1563/1562 [==============================] - 226s 145ms/step - loss: 0.2523 - acc: 0.9095 - val_loss: 0.4501 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88620\n",
      "epoch =  16 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2503 - acc: 0.9105 - val_loss: 0.4631 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88620\n",
      "epoch =  17 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2515 - acc: 0.9098 - val_loss: 0.4518 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.88620 to 0.88640, saving model to DNST_CIFAR10_Conv_09_8831-19-0.8864.hdf5\n",
      "epoch =  18 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2460 - acc: 0.9114 - val_loss: 0.4680 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88640\n",
      "epoch =  19 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2490 - acc: 0.9113 - val_loss: 0.4632 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88640\n",
      "epoch =  20 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 22/50\n",
      "1563/1562 [==============================] - 226s 145ms/step - loss: 0.2478 - acc: 0.9113 - val_loss: 0.4549 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88640\n",
      "epoch =  21 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2478 - acc: 0.9115 - val_loss: 0.4685 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88640\n",
      "epoch =  22 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2469 - acc: 0.9119 - val_loss: 0.4682 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88640\n",
      "epoch =  23 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 25/50\n",
      "1563/1562 [==============================] - 226s 144ms/step - loss: 0.2447 - acc: 0.9123 - val_loss: 0.4590 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88640\n",
      "epoch =  24 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2451 - acc: 0.9120 - val_loss: 0.4617 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88640\n",
      "epoch =  25 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 224s 144ms/step - loss: 0.2432 - acc: 0.9117 - val_loss: 0.4662 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88640\n",
      "epoch =  26 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 226s 145ms/step - loss: 0.2431 - acc: 0.9129 - val_loss: 0.4595 - val_acc: 0.8870\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.88640 to 0.88700, saving model to DNST_CIFAR10_Conv_09_8831-28-0.8870.hdf5\n",
      "epoch =  27 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 226s 144ms/step - loss: 0.2418 - acc: 0.9136 - val_loss: 0.4752 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88700\n",
      "epoch =  28 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 224s 144ms/step - loss: 0.2418 - acc: 0.9135 - val_loss: 0.4635 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88700\n",
      "epoch =  29 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 226s 144ms/step - loss: 0.2410 - acc: 0.9136 - val_loss: 0.4637 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88700\n",
      "epoch =  30 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2404 - acc: 0.9125 - val_loss: 0.4592 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88700\n",
      "epoch =  31 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2428 - acc: 0.9129 - val_loss: 0.4622 - val_acc: 0.8850\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88700\n",
      "epoch =  32 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2434 - acc: 0.9125 - val_loss: 0.4609 - val_acc: 0.8863\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88700\n",
      "epoch =  33 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 226s 144ms/step - loss: 0.2358 - acc: 0.9152 - val_loss: 0.4616 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88700\n",
      "epoch =  34 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2387 - acc: 0.9148 - val_loss: 0.4650 - val_acc: 0.8872\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.88700 to 0.88720, saving model to DNST_CIFAR10_Conv_09_8831-36-0.8872.hdf5\n",
      "epoch =  35 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2400 - acc: 0.9131 - val_loss: 0.4688 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88720\n",
      "epoch =  36 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2378 - acc: 0.9142 - val_loss: 0.4590 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88720\n",
      "epoch =  37 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2381 - acc: 0.9142 - val_loss: 0.4645 - val_acc: 0.8859\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88720\n",
      "epoch =  38 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2388 - acc: 0.9145 - val_loss: 0.4699 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88720\n",
      "epoch =  39 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2350 - acc: 0.9159 - val_loss: 0.4648 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88720\n",
      "epoch =  40 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 226s 145ms/step - loss: 0.2371 - acc: 0.9152 - val_loss: 0.4702 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88720\n",
      "epoch =  41 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2370 - acc: 0.9151 - val_loss: 0.4597 - val_acc: 0.8873\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.88720 to 0.88730, saving model to DNST_CIFAR10_Conv_09_8831-43-0.8873.hdf5\n",
      "epoch =  42 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2363 - acc: 0.9162 - val_loss: 0.4738 - val_acc: 0.8849\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88730\n",
      "epoch =  43 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2345 - acc: 0.9153 - val_loss: 0.4654 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88730\n",
      "epoch =  44 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2364 - acc: 0.9162 - val_loss: 0.4674 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88730\n",
      "epoch =  45 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2320 - acc: 0.9173 - val_loss: 0.4683 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88730\n",
      "epoch =  46 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2344 - acc: 0.9159 - val_loss: 0.4700 - val_acc: 0.8871\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88730\n",
      "epoch =  47 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 225s 144ms/step - loss: 0.2336 - acc: 0.9157 - val_loss: 0.4689 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88730\n",
      "epoch =  48 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 224s 143ms/step - loss: 0.2359 - acc: 0.9161 - val_loss: 0.4672 - val_acc: 0.8851\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88730\n",
      "epoch =  49 , lr =  0.01 , momentum =  0.9 , decay =  1e-04 , Nestrov =  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 2 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "U3PICoCgw01Z",
    "outputId": "e6e8a94b-cfd8-4ce8-9bab-b1e0081a759d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.025)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8p916JhoWHz6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlZmM-_zG6lS",
    "outputId": "9977e1ed-3f85-4f39-f9f7-4baa702e2e76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.9\n",
      "0.0011111111\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.05)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6zvEjTAoBM4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qq-HWYiln_b2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmvoF8QVG6lU",
    "outputId": "9e7ac435-50e3-4a84-f347-7e78bf01d9de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1562 [==============================] - 561s 359ms/step - loss: 0.8322 - acc: 0.7061 - val_loss: 0.9455 - val_acc: 0.6963\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.69460 to 0.69630, saving model to DNST_CIFAR10_Conv_04-01-0.6963.hdf5\n",
      "Epoch 2/15\n",
      "1563/1562 [==============================] - 564s 361ms/step - loss: 0.8273 - acc: 0.7075 - val_loss: 0.9425 - val_acc: 0.6957\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69630\n",
      "Epoch 3/15\n",
      "1563/1562 [==============================] - 562s 360ms/step - loss: 0.8265 - acc: 0.7071 - val_loss: 0.9484 - val_acc: 0.6992\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.69630 to 0.69920, saving model to DNST_CIFAR10_Conv_04-03-0.6992.hdf5\n",
      "Epoch 4/15\n",
      "1563/1562 [==============================] - 562s 360ms/step - loss: 0.8223 - acc: 0.7102 - val_loss: 0.9854 - val_acc: 0.6874\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69920\n",
      "Epoch 5/15\n",
      "1563/1562 [==============================] - 564s 361ms/step - loss: 0.8192 - acc: 0.7084 - val_loss: 0.9261 - val_acc: 0.7032\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.69920 to 0.70320, saving model to DNST_CIFAR10_Conv_04-05-0.7032.hdf5\n",
      "Epoch 6/15\n",
      "1563/1562 [==============================] - 563s 360ms/step - loss: 0.8144 - acc: 0.7126 - val_loss: 0.9145 - val_acc: 0.7073\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.70320 to 0.70730, saving model to DNST_CIFAR10_Conv_04-06-0.7073.hdf5\n",
      "Epoch 7/15\n",
      "1563/1562 [==============================] - 563s 360ms/step - loss: 0.8178 - acc: 0.7117 - val_loss: 0.9360 - val_acc: 0.7014\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70730\n",
      "Epoch 8/15\n",
      "1563/1562 [==============================] - 563s 360ms/step - loss: 0.8108 - acc: 0.7134 - val_loss: 0.9511 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.70730\n",
      "Epoch 9/15\n",
      "1563/1562 [==============================] - 578s 370ms/step - loss: 0.8074 - acc: 0.7138 - val_loss: 0.9691 - val_acc: 0.6956\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.70730\n",
      "Epoch 10/15\n",
      "1563/1562 [==============================] - 570s 365ms/step - loss: 0.8075 - acc: 0.7140 - val_loss: 0.9771 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70730\n",
      "Epoch 11/15\n",
      " 758/1562 [=============>................] - ETA: 4:54 - loss: 0.8052 - acc: 0.7155"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-beb62971c941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 15\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 2 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zix_gEdBG6lX"
   },
   "outputs": [],
   "source": [
    "#Let's see what Adam can do here\n",
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAFfDq18G6lZ",
    "outputId": "b7ac644b-f46b-4fc6-95e1-2437e58f3898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1562 [==============================] - 805s 515ms/step - loss: 0.9252 - acc: 0.6718 - val_loss: 0.9238 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.70730\n",
      "Epoch 2/15\n",
      "1563/1562 [==============================] - 685s 438ms/step - loss: 0.8538 - acc: 0.6967 - val_loss: 0.9833 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.70730\n",
      "Epoch 3/15\n",
      "1563/1562 [==============================] - 684s 438ms/step - loss: 0.7890 - acc: 0.7196 - val_loss: 0.8938 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.70730 to 0.72040, saving model to DNST_CIFAR10_Conv_04-03-0.7204.hdf5\n",
      "Epoch 4/15\n",
      "1563/1562 [==============================] - 688s 440ms/step - loss: 0.7361 - acc: 0.7415 - val_loss: 0.7951 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.72040 to 0.74750, saving model to DNST_CIFAR10_Conv_04-04-0.7475.hdf5\n",
      "Epoch 5/15\n",
      "1563/1562 [==============================] - 685s 438ms/step - loss: 0.6885 - acc: 0.7583 - val_loss: 0.8537 - val_acc: 0.7391\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.74750\n",
      "Epoch 6/15\n",
      "1563/1562 [==============================] - 685s 438ms/step - loss: 0.6465 - acc: 0.7737 - val_loss: 0.9545 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.74750\n",
      "Epoch 7/15\n",
      "1563/1562 [==============================] - 690s 441ms/step - loss: 0.6043 - acc: 0.7869 - val_loss: 0.7918 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.74750 to 0.77020, saving model to DNST_CIFAR10_Conv_04-07-0.7702.hdf5\n",
      "Epoch 8/15\n",
      "1563/1562 [==============================] - 696s 445ms/step - loss: 0.5722 - acc: 0.7991 - val_loss: 0.6356 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.77020 to 0.80620, saving model to DNST_CIFAR10_Conv_04-08-0.8062.hdf5\n",
      "Epoch 9/15\n",
      "1563/1562 [==============================] - 687s 440ms/step - loss: 0.5452 - acc: 0.8093 - val_loss: 0.7793 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80620\n",
      "Epoch 10/15\n",
      " 776/1562 [=============>................] - ETA: 5:41 - loss: 0.5204 - acc: 0.8173"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-beb62971c941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 15\n",
    "\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train) * 2 /batch_size),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcMp-CA3G6lc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUMfE3mfG6ld",
    "outputId": "b12040ea-6043-46c5-9c5c-87f1d12873b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/390 [==============================] - 78s 199ms/step - loss: 1.3040 - acc: 0.5284 - val_loss: 1.6906 - val_acc: 0.4920\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.52840\n",
      "Epoch 2/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.2827 - acc: 0.5393 - val_loss: 1.5553 - val_acc: 0.5072\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.52840\n",
      "Epoch 3/20\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 1.2592 - acc: 0.5477 - val_loss: 2.0982 - val_acc: 0.4572\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.52840\n",
      "Epoch 4/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.2321 - acc: 0.5524 - val_loss: 1.7296 - val_acc: 0.5060\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.52840\n",
      "Epoch 5/20\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 1.2145 - acc: 0.5609 - val_loss: 1.4300 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.52840 to 0.53600, saving model to densenet_tr_04_tr_10_-05-0.5360.hdf5\n",
      "Epoch 6/20\n",
      "391/390 [==============================] - 78s 199ms/step - loss: 1.2117 - acc: 0.5636 - val_loss: 1.8191 - val_acc: 0.4876\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.53600\n",
      "Epoch 7/20\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 1.1759 - acc: 0.5786 - val_loss: 1.6574 - val_acc: 0.5140\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.53600\n",
      "Epoch 8/20\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 1.1640 - acc: 0.5801 - val_loss: 1.5770 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.53600 to 0.54360, saving model to densenet_tr_04_tr_10_-08-0.5436.hdf5\n",
      "Epoch 9/20\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 1.1482 - acc: 0.5875 - val_loss: 1.4169 - val_acc: 0.5572\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.54360 to 0.55720, saving model to densenet_tr_04_tr_10_-09-0.5572.hdf5\n",
      "Epoch 10/20\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 1.1337 - acc: 0.5933 - val_loss: 1.3917 - val_acc: 0.5808\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.55720 to 0.58080, saving model to densenet_tr_04_tr_10_-10-0.5808.hdf5\n",
      "Epoch 11/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.1133 - acc: 0.5993 - val_loss: 1.3243 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.58080\n",
      "Epoch 12/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.0998 - acc: 0.6047 - val_loss: 1.5786 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.58080\n",
      "Epoch 13/20\n",
      "391/390 [==============================] - 78s 198ms/step - loss: 1.0959 - acc: 0.6067 - val_loss: 2.5486 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.58080\n",
      "Epoch 14/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.0730 - acc: 0.6159 - val_loss: 1.4460 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.58080\n",
      "Epoch 15/20\n",
      "391/390 [==============================] - 78s 200ms/step - loss: 1.0622 - acc: 0.6164 - val_loss: 1.4284 - val_acc: 0.5776\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.58080\n",
      "Epoch 16/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.0467 - acc: 0.6218 - val_loss: 1.5520 - val_acc: 0.5516\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.58080\n",
      "Epoch 17/20\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 1.0361 - acc: 0.6296 - val_loss: 1.6165 - val_acc: 0.5444\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.58080\n",
      "Epoch 18/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.0165 - acc: 0.6374 - val_loss: 1.4061 - val_acc: 0.5776\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.58080\n",
      "Epoch 19/20\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 1.0118 - acc: 0.6406 - val_loss: 1.9957 - val_acc: 0.5192\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.58080\n",
      "Epoch 20/20\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 1.0042 - acc: 0.6409 - val_loss: 1.1609 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.58080 to 0.62840, saving model to densenet_tr_04_tr_10_-20-0.6284.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbf57919e8>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                    batch_size=batch_size),\n",
    "                    steps_per_epoch= len(x_train)/batch_size,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uP9uPg9GG6lh",
    "outputId": "cf1ac1cb-8982-4136-ded1-0f7cbcc5e729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from keras.backend import eval\n",
    "print(eval(model.optimizer.lr))\n",
    "print(eval(model.optimizer.momentum))\n",
    "print(eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZVtRNGtG6ll",
    "outputId": "17891475-5760-497c-9e8b-795465f9d46b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.001)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iq_aLgxuG6lp"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jQ3lM8zG6lv"
   },
   "outputs": [],
   "source": [
    "k.set_value(model.optimizer.lr, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpU6cQarG6lx",
    "outputId": "6d08876d-c10d-4b68-9f6c-0b4240ecf71d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.0\n",
      "0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(eval(model.optimizer.lr))\n",
    "print(eval(model.optimizer.momentum))\n",
    "print(eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCE9_64ZG6l2",
    "outputId": "3cc5efd4-7104-4da9-8997-894249ee5562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 23s 2ms/step\n",
      "Test loss: 0.6240345459699631\n",
      "Test accuracy: 0.8099\n"
     ]
    }
   ],
   "source": [
    "model = load_model('densenet_tr_04_tr_10_tr_05_-10-0.8099.hdf5')\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gS6ODLgHG6l5",
    "outputId": "8b30c208-9fe2-4ceb-e3a6-d66d9ffac8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "50000/50000 [==============================] - 340s 7ms/step - loss: 0.3626 - acc: 0.8698 - val_loss: 0.6486 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79910, saving model to densenet_tr_04_tr_10_tr_05_1_-01-0.7991.hdf5\n",
      "Epoch 2/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3594 - acc: 0.8692 - val_loss: 0.7186 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79910\n",
      "Epoch 3/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3546 - acc: 0.8725 - val_loss: 0.7258 - val_acc: 0.7852\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79910\n",
      "Epoch 4/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3464 - acc: 0.8765 - val_loss: 0.7185 - val_acc: 0.7932\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79910\n",
      "Epoch 5/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3459 - acc: 0.8754 - val_loss: 0.6642 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79910 to 0.80020, saving model to densenet_tr_04_tr_10_tr_05_1_-05-0.8002.hdf5\n",
      "Epoch 6/40\n",
      "50000/50000 [==============================] - 278s 6ms/step - loss: 0.3436 - acc: 0.8775 - val_loss: 0.6392 - val_acc: 0.8071\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.80020 to 0.80710, saving model to densenet_tr_04_tr_10_tr_05_1_-06-0.8071.hdf5\n",
      "Epoch 7/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3352 - acc: 0.8816 - val_loss: 0.6809 - val_acc: 0.8023\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80710\n",
      "Epoch 8/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3316 - acc: 0.8791 - val_loss: 0.6872 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80710\n",
      "Epoch 9/40\n",
      "50000/50000 [==============================] - 280s 6ms/step - loss: 0.3297 - acc: 0.8820 - val_loss: 0.6418 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80710 to 0.80850, saving model to densenet_tr_04_tr_10_tr_05_1_-09-0.8085.hdf5\n",
      "Epoch 10/40\n",
      "50000/50000 [==============================] - 280s 6ms/step - loss: 0.3210 - acc: 0.8842 - val_loss: 0.6917 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80850\n",
      "Epoch 11/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3187 - acc: 0.8854 - val_loss: 0.6742 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80850\n",
      "Epoch 12/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3160 - acc: 0.8856 - val_loss: 0.6595 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80850\n",
      "Epoch 13/40\n",
      "50000/50000 [==============================] - 280s 6ms/step - loss: 0.3106 - acc: 0.8890 - val_loss: 0.6992 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80850\n",
      "Epoch 14/40\n",
      "50000/50000 [==============================] - 280s 6ms/step - loss: 0.3091 - acc: 0.8889 - val_loss: 0.7034 - val_acc: 0.7984\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80850\n",
      "Epoch 15/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.3023 - acc: 0.8913 - val_loss: 0.6828 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80850\n",
      "Epoch 16/40\n",
      "50000/50000 [==============================] - 278s 6ms/step - loss: 0.2992 - acc: 0.8928 - val_loss: 0.6616 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.80850 to 0.81190, saving model to densenet_tr_04_tr_10_tr_05_1_-16-0.8119.hdf5\n",
      "Epoch 17/40\n",
      "50000/50000 [==============================] - 280s 6ms/step - loss: 0.2972 - acc: 0.8924 - val_loss: 0.6935 - val_acc: 0.8013\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81190\n",
      "Epoch 18/40\n",
      "50000/50000 [==============================] - 278s 6ms/step - loss: 0.2937 - acc: 0.8945 - val_loss: 0.7528 - val_acc: 0.7924\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.81190\n",
      "Epoch 19/40\n",
      "50000/50000 [==============================] - 278s 6ms/step - loss: 0.2883 - acc: 0.8959 - val_loss: 0.6475 - val_acc: 0.8146\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.81190 to 0.81460, saving model to densenet_tr_04_tr_10_tr_05_1_-19-0.8146.hdf5\n",
      "Epoch 20/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2831 - acc: 0.8983 - val_loss: 0.6407 - val_acc: 0.8159\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.81460 to 0.81590, saving model to densenet_tr_04_tr_10_tr_05_1_-20-0.8159.hdf5\n",
      "Epoch 21/40\n",
      "50000/50000 [==============================] - 278s 6ms/step - loss: 0.2780 - acc: 0.8994 - val_loss: 0.6633 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.81590 to 0.81700, saving model to densenet_tr_04_tr_10_tr_05_1_-21-0.8170.hdf5\n",
      "Epoch 22/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2760 - acc: 0.9000 - val_loss: 0.6300 - val_acc: 0.8217\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.81700 to 0.82170, saving model to densenet_tr_04_tr_10_tr_05_1_-22-0.8217.hdf5\n",
      "Epoch 23/40\n",
      "50000/50000 [==============================] - 278s 6ms/step - loss: 0.2734 - acc: 0.9014 - val_loss: 0.6981 - val_acc: 0.8057\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.82170\n",
      "Epoch 24/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2710 - acc: 0.9026 - val_loss: 0.6551 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.82170\n",
      "Epoch 25/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2642 - acc: 0.9057 - val_loss: 0.6988 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.82170\n",
      "Epoch 26/40\n",
      "50000/50000 [==============================] - 278s 6ms/step - loss: 0.2630 - acc: 0.9057 - val_loss: 0.6742 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.82170\n",
      "Epoch 27/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2602 - acc: 0.9056 - val_loss: 0.7088 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.82170\n",
      "Epoch 28/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2600 - acc: 0.9055 - val_loss: 0.6688 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82170\n",
      "Epoch 29/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2524 - acc: 0.9082 - val_loss: 0.6573 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82170\n",
      "Epoch 30/40\n",
      "50000/50000 [==============================] - 280s 6ms/step - loss: 0.2339 - acc: 0.9152 - val_loss: 0.6390 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.82170 to 0.82510, saving model to densenet_tr_04_tr_10_tr_05_1_-30-0.8251.hdf5\n",
      "Epoch 31/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2252 - acc: 0.9199 - val_loss: 0.7020 - val_acc: 0.8121\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82510\n",
      "Epoch 32/40\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 0.2234 - acc: 0.9196 - val_loss: 0.6622 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82510\n",
      "Epoch 33/40\n",
      "50000/50000 [==============================] - 281s 6ms/step - loss: 0.2243 - acc: 0.9197 - val_loss: 0.6708 - val_acc: 0.8211\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.82510\n",
      "Epoch 34/40\n",
      "13056/50000 [======>.......................] - ETA: 3:19 - loss: 0.2210 - acc: 0.9180"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ea1b00ecd069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 callbacks=callbacks_list)  \n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= len(x_train)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6m6Mgwh1G6l8",
    "outputId": "bc3aee71-5d64-4ecd-fcf4-c2b368ec89f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9873 - acc: 0.6482 - val_loss: 1.5091 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.62840\n",
      "Epoch 2/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9468 - acc: 0.6603 - val_loss: 1.4757 - val_acc: 0.5832\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.62840\n",
      "Epoch 3/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9335 - acc: 0.6675 - val_loss: 1.5189 - val_acc: 0.5756\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.62840\n",
      "Epoch 4/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9362 - acc: 0.6618 - val_loss: 1.5160 - val_acc: 0.5780\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.62840\n",
      "Epoch 5/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9352 - acc: 0.6661 - val_loss: 1.5022 - val_acc: 0.5812\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.62840\n",
      "Epoch 6/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9294 - acc: 0.6671 - val_loss: 1.4843 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.62840\n",
      "Epoch 7/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9276 - acc: 0.6683 - val_loss: 1.5184 - val_acc: 0.5780\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.62840\n",
      "Epoch 8/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9230 - acc: 0.6712 - val_loss: 1.5050 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.62840\n",
      "Epoch 9/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9212 - acc: 0.6700 - val_loss: 1.5036 - val_acc: 0.5816\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.62840\n",
      "Epoch 10/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9229 - acc: 0.6703 - val_loss: 1.4953 - val_acc: 0.5832\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.62840\n",
      "Epoch 11/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9183 - acc: 0.6735 - val_loss: 1.5134 - val_acc: 0.5788\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62840\n",
      "Epoch 12/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9265 - acc: 0.6677 - val_loss: 1.4734 - val_acc: 0.5848\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62840\n",
      "Epoch 13/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9178 - acc: 0.6700 - val_loss: 1.5031 - val_acc: 0.5808\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62840\n",
      "Epoch 14/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9167 - acc: 0.6704 - val_loss: 1.4838 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62840\n",
      "Epoch 15/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9108 - acc: 0.6752 - val_loss: 1.5227 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62840\n",
      "Epoch 16/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9149 - acc: 0.6757 - val_loss: 1.4906 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.62840\n",
      "Epoch 17/100\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 0.9008 - acc: 0.6777 - val_loss: 1.4869 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62840\n",
      "Epoch 18/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9127 - acc: 0.6742 - val_loss: 1.5294 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.62840\n",
      "Epoch 19/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9091 - acc: 0.6733 - val_loss: 1.4884 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62840\n",
      "Epoch 20/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8999 - acc: 0.6761 - val_loss: 1.5125 - val_acc: 0.5808\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.62840\n",
      "Epoch 21/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9121 - acc: 0.6751 - val_loss: 1.5307 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.62840\n",
      "Epoch 22/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9055 - acc: 0.6756 - val_loss: 1.4970 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.62840\n",
      "Epoch 23/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9050 - acc: 0.6763 - val_loss: 1.5105 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.62840\n",
      "Epoch 24/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9036 - acc: 0.6762 - val_loss: 1.5369 - val_acc: 0.5780\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.62840\n",
      "Epoch 25/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9089 - acc: 0.6723 - val_loss: 1.4915 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.62840\n",
      "Epoch 26/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9122 - acc: 0.6753 - val_loss: 1.5283 - val_acc: 0.5788\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.62840\n",
      "Epoch 27/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9130 - acc: 0.6772 - val_loss: 1.5351 - val_acc: 0.5776\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.62840\n",
      "Epoch 28/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9026 - acc: 0.6770 - val_loss: 1.5213 - val_acc: 0.5780\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.62840\n",
      "Epoch 29/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9101 - acc: 0.6753 - val_loss: 1.5051 - val_acc: 0.5812\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.62840\n",
      "Epoch 30/100\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 0.8945 - acc: 0.6794 - val_loss: 1.5373 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.62840\n",
      "Epoch 31/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.8940 - acc: 0.6827 - val_loss: 1.5068 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.62840\n",
      "Epoch 32/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8955 - acc: 0.6799 - val_loss: 1.5233 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.62840\n",
      "Epoch 33/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9026 - acc: 0.6786 - val_loss: 1.5129 - val_acc: 0.5804\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62840\n",
      "Epoch 34/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9036 - acc: 0.6777 - val_loss: 1.5289 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.62840\n",
      "Epoch 35/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9001 - acc: 0.6782 - val_loss: 1.5132 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.62840\n",
      "Epoch 36/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8955 - acc: 0.6787 - val_loss: 1.5169 - val_acc: 0.5788\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.62840\n",
      "Epoch 37/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9000 - acc: 0.6762 - val_loss: 1.4964 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.62840\n",
      "Epoch 38/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9023 - acc: 0.6775 - val_loss: 1.4929 - val_acc: 0.5828\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.62840\n",
      "Epoch 39/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9011 - acc: 0.6737 - val_loss: 1.5184 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.62840\n",
      "Epoch 40/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8975 - acc: 0.6812 - val_loss: 1.5506 - val_acc: 0.5740\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.62840\n",
      "Epoch 41/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8964 - acc: 0.6795 - val_loss: 1.5242 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.62840\n",
      "Epoch 42/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9019 - acc: 0.6756 - val_loss: 1.5413 - val_acc: 0.5760\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.62840\n",
      "Epoch 43/100\n",
      "391/390 [==============================] - 77s 198ms/step - loss: 0.9016 - acc: 0.6790 - val_loss: 1.5168 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.62840\n",
      "Epoch 44/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8923 - acc: 0.6808 - val_loss: 1.5159 - val_acc: 0.5784\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.62840\n",
      "Epoch 45/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8935 - acc: 0.6801 - val_loss: 1.5399 - val_acc: 0.5784\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.62840\n",
      "Epoch 46/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8941 - acc: 0.6793 - val_loss: 1.5120 - val_acc: 0.5784\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.62840\n",
      "Epoch 47/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8964 - acc: 0.6788 - val_loss: 1.5351 - val_acc: 0.5760\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.62840\n",
      "Epoch 48/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8988 - acc: 0.6771 - val_loss: 1.5455 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.62840\n",
      "Epoch 49/100\n",
      "391/390 [==============================] - 77s 196ms/step - loss: 0.9005 - acc: 0.6793 - val_loss: 1.5358 - val_acc: 0.5776\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.62840\n",
      "Epoch 50/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8982 - acc: 0.6798 - val_loss: 1.5271 - val_acc: 0.5776\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.62840\n",
      "Epoch 51/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8924 - acc: 0.6785 - val_loss: 1.5059 - val_acc: 0.5816\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.62840\n",
      "Epoch 52/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.8997 - acc: 0.6797 - val_loss: 1.5230 - val_acc: 0.5804\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.62840\n",
      "Epoch 53/100\n",
      "391/390 [==============================] - 77s 197ms/step - loss: 0.9017 - acc: 0.6772 - val_loss: 1.5339 - val_acc: 0.5784\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.62840\n",
      "Epoch 54/100\n",
      "115/390 [=======>......................] - ETA: 52s - loss: 0.8971 - acc: 0.6739"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                    batch_size=batch_size),\n",
    "                    steps_per_epoch= len(x_train)/batch_size,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSMOqu70G6l_",
    "outputId": "f1f97535-70c4-4e98-9dcc-a156368420e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1562 [==============================] - 255s 163ms/step - loss: 0.5237 - acc: 0.8196 - val_loss: 0.6589 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.82030\n",
      "Epoch 2/10\n",
      "1563/1562 [==============================] - 256s 164ms/step - loss: 0.5276 - acc: 0.8189 - val_loss: 0.6505 - val_acc: 0.8163\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.82030\n",
      "Epoch 3/10\n",
      "1563/1562 [==============================] - 256s 164ms/step - loss: 0.5225 - acc: 0.8208 - val_loss: 0.6414 - val_acc: 0.8199\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82030\n",
      "Epoch 4/10\n",
      "1563/1562 [==============================] - 256s 164ms/step - loss: 0.5222 - acc: 0.8207 - val_loss: 0.6389 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82030\n",
      "Epoch 5/10\n",
      "1563/1562 [==============================] - 256s 164ms/step - loss: 0.5204 - acc: 0.8206 - val_loss: 0.6248 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.82030 to 0.82190, saving model to densenet_tr_04_tr_02_-05-0.8219.hdf5\n",
      "Epoch 6/10\n",
      "1563/1562 [==============================] - 256s 164ms/step - loss: 0.5187 - acc: 0.8205 - val_loss: 0.6550 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82190\n",
      "Epoch 7/10\n",
      "1563/1562 [==============================] - 255s 163ms/step - loss: 0.5156 - acc: 0.8224 - val_loss: 0.6295 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82190\n",
      "Epoch 8/10\n",
      "1563/1562 [==============================] - 256s 164ms/step - loss: 0.5222 - acc: 0.8195 - val_loss: 0.6292 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.82190 to 0.82220, saving model to densenet_tr_04_tr_02_-08-0.8222.hdf5\n",
      "Epoch 9/10\n",
      "1563/1562 [==============================] - 256s 164ms/step - loss: 0.5163 - acc: 0.8225 - val_loss: 0.5990 - val_acc: 0.8287\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.82220 to 0.82870, saving model to densenet_tr_04_tr_02_-09-0.8287.hdf5\n",
      "Epoch 10/10\n",
      "1563/1562 [==============================] - 257s 164ms/step - loss: 0.5129 - acc: 0.8238 - val_loss: 0.5985 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcbc32a2e8>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                    batch_size=batch_size),\n",
    "                    steps_per_epoch= len(x_train)/batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEQ6qu5BG6mC",
    "outputId": "d3d5b175-fbc8-4624-a476-6ff6ae9ec189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "0.0\n",
      "0.0\n",
      "False\n",
      "0.001\n",
      "0.0\n",
      "0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)\n",
    "\t   \n",
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.001)\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTi7d5RDG6mM",
    "outputId": "f92692b1-e390-4c5c-bbb4-8664132c1ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10s 1ms/step\n",
      "Test loss: 0.5989693383455277\n",
      "Test accuracy: 0.8287\n"
     ]
    }
   ],
   "source": [
    "model = load_model('densenet_tr_04_tr_02_-09-0.8287.hdf5')\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2UeD_6KG6mR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onvf93F-G6mU",
    "outputId": "ba633c58-eb7c-4c1d-e296-f44a15990493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1562 [==============================] - 274s 175ms/step - loss: 0.5161 - acc: 0.8219 - val_loss: 0.6120 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.82870\n",
      "Epoch 2/20\n",
      "1563/1562 [==============================] - 257s 165ms/step - loss: 0.5163 - acc: 0.8214 - val_loss: 0.6226 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.82870\n",
      "Epoch 3/20\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5158 - acc: 0.8217 - val_loss: 0.6149 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82870\n",
      "Epoch 4/20\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5113 - acc: 0.8234 - val_loss: 0.5930 - val_acc: 0.8318\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.82870 to 0.83180, saving model to densenet_tr_04_tr_02_-04-0.8318.hdf5\n",
      "Epoch 5/20\n",
      "1563/1562 [==============================] - 259s 166ms/step - loss: 0.5120 - acc: 0.8245 - val_loss: 0.6026 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.83180\n",
      "Epoch 6/20\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5171 - acc: 0.8221 - val_loss: 0.6563 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.83180\n",
      "Epoch 7/20\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5119 - acc: 0.8233 - val_loss: 0.6223 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83180\n",
      "Epoch 8/20\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5087 - acc: 0.8239 - val_loss: 0.6296 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83180\n",
      "Epoch 9/20\n",
      "1110/1562 [====================>.........] - ETA: 1:12 - loss: 0.5098 - acc: 0.8238"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-931771152009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                    batch_size=batch_size),\n",
    "                    steps_per_epoch= len(x_train)/batch_size,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeR7qi_iG6mX",
    "outputId": "e12e99a9-c267-4c81-fe24-6d4c4809681a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "Test loss: 0.5930212475657463\n",
      "Test accuracy: 0.8318\n"
     ]
    }
   ],
   "source": [
    "#densenet_tr_04-26-0.82.hdf5\n",
    "model = load_model('densenet_tr_04_tr_02_-04-0.8318.hdf5')\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_W4l0s3G6ma",
    "outputId": "e3460c04-8738-45f1-8ac4-1bae56fa04fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-04\n",
      "0.9\n",
      "1e-04\n",
      "False\n",
      "1e-04\n",
      "0.9\n",
      "1e-04\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)\n",
    " \n",
    "import keras.backend as K\n",
    "k.set_value(model.optimizer.lr, 0.0001)\n",
    "k.set_value(model.optimizer.momentum, 0.9)\n",
    "k.set_value(model.optimizer.decay, 0.0001)\n",
    "model.optimizer.nesterov= True\n",
    "\n",
    "print(K.eval(model.optimizer.lr))\n",
    "print(K.eval(model.optimizer.momentum))\n",
    "print(K.eval(model.optimizer.decay))\n",
    "print(model.optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A_Ktx95G6mh",
    "outputId": "863c7823-cf52-4383-b546-fb73b8033e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1562 [==============================] - 276s 177ms/step - loss: 0.5099 - acc: 0.8240 - val_loss: 0.6481 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.83180\n",
      "Epoch 2/5\n",
      "1563/1562 [==============================] - 260s 166ms/step - loss: 0.5094 - acc: 0.8238 - val_loss: 0.6091 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.83180\n",
      "Epoch 3/5\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5118 - acc: 0.8218 - val_loss: 0.6109 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.83180\n",
      "Epoch 4/5\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5105 - acc: 0.8240 - val_loss: 0.6249 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.83180\n",
      "Epoch 5/5\n",
      "1563/1562 [==============================] - 258s 165ms/step - loss: 0.5074 - acc: 0.8249 - val_loss: 0.6098 - val_acc: 0.8290\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.83180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc48e22390>"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                    batch_size=batch_size),\n",
    "                    steps_per_epoch= len(x_train)/batch_size,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAnw_xNlG6mj",
    "outputId": "f679acb4-169a-4d23-acd0-b1b540596108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 421s 538ms/step - loss: 0.9375 - acc: 0.6674 - val_loss: 2.1078 - val_acc: 0.5434\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.65110\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 421s 538ms/step - loss: 0.9306 - acc: 0.6690 - val_loss: 1.4070 - val_acc: 0.6178\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.65110\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 421s 538ms/step - loss: 0.9196 - acc: 0.6729 - val_loss: 1.3154 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.65110\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 421s 538ms/step - loss: 0.9123 - acc: 0.6767 - val_loss: 1.2812 - val_acc: 0.6265\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.65110\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 421s 538ms/step - loss: 0.8984 - acc: 0.6816 - val_loss: 1.3735 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.65110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb93e0ff9e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                    batch_size=batch_size),\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwOoDMefG6mm",
    "outputId": "002e30cf-7bb5-420c-bc7c-86bb1c1d8d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.5481 - acc: 0.8068 - val_loss: 0.8859 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.73290, saving model to densenet-01-0.73.hdf5\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.5417 - acc: 0.8087 - val_loss: 0.7878 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.73290 to 0.75890, saving model to densenet-02-0.76.hdf5\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.5372 - acc: 0.8082 - val_loss: 0.7391 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75890 to 0.77700, saving model to densenet-03-0.78.hdf5\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.5322 - acc: 0.8104 - val_loss: 0.7139 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77700 to 0.77740, saving model to densenet-04-0.78.hdf5\n",
      "Epoch 5/50\n",
      "21248/50000 [===========>..................] - ETA: 1:58 - loss: 0.5212 - acc: 0.8171"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU0meEE3G6mr",
    "outputId": "f6793d3d-6c44-4eb8-fcb1-1112b3c540a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3575 - acc: 0.8739 - val_loss: 0.7500 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.81130\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3542 - acc: 0.8761 - val_loss: 0.7343 - val_acc: 0.7993\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.81130\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3531 - acc: 0.8749 - val_loss: 0.6764 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.81130 to 0.81160, saving model to densenet-03-0.81.hdf5\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3472 - acc: 0.8772 - val_loss: 0.7236 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.81160\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3487 - acc: 0.8770 - val_loss: 0.7087 - val_acc: 0.8135\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.81160 to 0.81350, saving model to densenet-05-0.81.hdf5\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3414 - acc: 0.8804 - val_loss: 0.8661 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81350\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3404 - acc: 0.8796 - val_loss: 0.7279 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81350\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3416 - acc: 0.8811 - val_loss: 0.6423 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.81350 to 0.82200, saving model to densenet-08-0.82.hdf5\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3355 - acc: 0.8823 - val_loss: 0.6961 - val_acc: 0.8107\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82200\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3362 - acc: 0.8803 - val_loss: 0.8177 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82200\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 218s 4ms/step - loss: 0.3345 - acc: 0.8820 - val_loss: 0.9932 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.82200\n",
      "Epoch 12/50\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8828"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWYRrcp7G6mv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DNST_CIFAR10_Conv_09.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
