{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This experiment was done to make sure that the model Model-2 936K achives 92 %\n",
    "\n",
    "#  It is seen that Model-2 936K params achieves 92.10 with Adam()\n",
    "\n",
    "- 4 Denseblocks with layer [14,14,14,14], growth rate= 12, compression = 0.5, dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K70hAckqg0EA",
    "outputId": "a0dba4c3-f402-4223-ea89-3be41ca275fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "#!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "756bdmamG6f1",
    "outputId": "2f1c4951-b47b-40a0-ab02-1c97fd7c1a95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight \n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhIk-iu4G6f-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time, pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBoa2F25G6gE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "#l = 40\n",
    "#num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWRR6JyzG6gT"
   },
   "outputs": [],
   "source": [
    "do_sub_sampling_of_input = False\n",
    "do_data_augmentation = True\n",
    "do_data_append       = False   #2X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "16a221e4-1075-49d1-9d76-8091d240f280"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_hYUAbsG6ge"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "class_name = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "00iNYJLkG6gj",
    "outputId": "4911e408-9a52-4009-eefd-44e5242fb219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6], dtype=uint8), 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], img_height, img_width, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS5X5srxG6gp"
   },
   "outputs": [],
   "source": [
    "def draw_img(i, x_train, y_train, class_name):\n",
    "    im = x_train[i]\n",
    "    c = y_train[i]\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"Class %d (%s)\" % (int(c), class_name[int(c)]))\n",
    "    plt.axis('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "mjF2KvIQG6gw",
    "outputId": "6c7a984d-4db5-4d5b-d110-b740edd41a02"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_img(7, x_train, y_train, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to dispaly and analyze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions\n",
    "\n",
    "plot_confusion_matrix(): helps us to plot the confusion matrix. \n",
    "It is taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html. \n",
    "The same page has some sample examples on how to use this function \n",
    "cm: Confusion matrix calcualted using confusion_matrix() from sklearn.metrics <br//> classes: a list of labels for the classes we are plotting\n",
    "normalize=False: True means we will plot nomalized values \n",
    "title='Confusion matrix': set the tiltle of the plot \n",
    "cmap : leave it as it is \n",
    "Example Usage:plot_confusion_matrix(cm, classes=Facial_Expressions, normalize=True, title='Test Data - Using Simple Average Ensembling ')\n",
    "\n",
    "plot_histogram(): helps to plot the histogram of a list \n",
    "lst_data: the list whose histogtam we want to plot , \n",
    "class_labels: a list of labels for the classes we are plotting \n",
    "ylabel='None': set the y label of the plot, x label is always frequency \n",
    "title='None': set the tiltle of the plot -lst_data, class_labels, ylabel='None', title='None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm) #to print in text if needed\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_histogram(lst_data, class_labels, ylabel='None', title='None'):\n",
    "    data = pd.Series(lst_data)\n",
    "    distribution = data.value_counts(sort=False)\n",
    "    y_pos = np.arange(len(class_labels))\n",
    "    \n",
    "    plt.bar(y_pos, distribution, align='center', alpha=0.8)\n",
    "    plt.xticks(y_pos, class_labels)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = ['airplane', 'automobile', 'bird','cat', 'deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(list(y_train), y_classes, ylabel='Frequency',title='CIFAR 10 Class Distribution in Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEICAYAAABI7RO5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xe4HlW59/HvD4KEECAQIkJCDE2agEAUkCKK8koHqYoQfcOJKE2RI+DhlRyPHkFF2hGU3kQpgkSsFFHAQ0moAUQiNSFAKCEQSiC53z/W/Zhhu8szu2/y+1zXvvbMmjVrrSnP3DNr5plHEYGZmVmzFuvrBpiZ2cDiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwWL8l6TuSLujrdlRJ+qOk/buprG0lPVAZny5p2+4oO8t7WNLW3VVepdxuWwc2MDlw9FOSPidpsqRXJc2U9DtJW+W0iZIuqeQNSXMz76uSZrcoa83Mc3qL9EEt5p0u6QeS2twvJB0haYqkeZLOaWX69nnAek3SjZJGd7CcB2R5c3M5fyPpo82up+7Syrp4XtL1kvaq5ouI7SPiZ02WNaa9fBFxU0Ss3/XWg6RLJE1sUf7aEXFzd5TfotwO10FLkhav7J+vSlog6fXK+L6dbU9+TvZrZ/oHc3s06pop6RpJH6tRx6GSft/ZNr7bOHD0Q5KOBE4B/htYCRgNnAHs1s5sG0XE0Pwb1mLaOOBFYD9JS7Qy7/oRMRT4BHBA5m/LDODbwAWttHsl4ErgWGA4cA9waVsFSfoG8EPgv4ARwPuBs2h/OXtaY12sA1wC/ETSf3R3JZIGdXeZ/VlEzK/sn0OBp4EdKmmX9XAT3sx6lwE2BW4Ffidp7x6u990pIvzXj/6A5YBXgb3byTMRuKQyHsCabeQV8DgwAXge2L0ybVDOO6aSdhVwahPtPAE4p0XaV4C/VMaXBd5srW3A8sBrwB7t1PEd4IIcXowSlJ4BZgM3AetW8u4MPAS8AkwHvpbp7wV+m/O8WG1fi7r+ZV1k+n7A68CwHL8F+EIOfwD4C/ByrttLM/2vWdbc3JZ7Ap/M7fDNXIbzG2mVuqYDR+dyvAScCyyZ0w4Cbmqtvbne3wLmZX1XV8rbNocHA6cBMynB/0fAe3Jao23fAGZRDuoHtrNdquvgIODPwMm5jh8Ftm9i//ln21os00TgsVyfFwPL5rShwOW5DV8CbqN8Vk4B5gNv5LKf2EpdHwTeaONz9Fhl/D9zPbwC3E8JbAAfpuzHb2cd0zN9T+C+zP8EcHRfHz96689XHP3PFpQP+dXdVN62lKuWXwBX0M7VhKR1gS2BaZ2sa33g3sZIRMyhHARa647ZknKguKZG+dcCawHvA6ZSDiwN5wPjI2IZYEPKwQzg3ykHsxE533E16gP4FbAk5eDR0neB31CC4Cjgx5m+Tf5fP8rZ9C9zfBTlADiacrBvzf7ApyjLuT7l6q1dEXEGcBnw31nfHq1k+xYwlrJuNqas/2rZo4ClgFWAg4EzJS3bUd3po5QD7XBKADm3yflaOpqyv24BrJppJ+X/L1EC5SqUbXk4MC8ivgrcTQlkQyPi6Br1XQWMkdSo6yFgc2AYJbD+QtIKEXEn8HXg+qxjVOafA+xLCWB7AsdI+mTNZR6QHDj6n+HA8xHxds357pI0O/9Oq6SPA36TB/FLgR0lDW8x732S5gIPAtcBP+1k24dSzr6rXqZ0D7Q0HHguIhY0U3BELIiICyLilYh4g3K2uKmkpTPLW8B6kpaJiBcj4q5K+irA6IiYFxF/qbNAWdeLwAqtTH6Lcsa/ckS8ERG3dlDc28DEbMfrbeQ5LSKmR8TzlK7Kz9Zpbzv2z7pnRcRzlO7GAyrT3wC+ExFvRcQkyhn2B5os+x8RcV5EzAcuBEZJWrETbTyYctb+TK6fb1Ou+KCs6xHA6hHxdkTc0c46bNbT+X8FgIj4RdY9PyLOp1x9bdzWzBFxXUQ8lPvmZOCXQNP3TQYyB47+5wVgxU70gW8SEcPy73CAPKjuCTRuZN5C6SZpeTDakHJw/xzlbG9pOudVSvdU1bKUS/mWXgDe296N+Kq8ufp9SY9KmsPCq6LGAWoPYFfgSUk3Sdos00+gdCPcIOkfkv69xvIgaTDlwPJiK5O/DiwBTJZ0v6T27g0BPBsR8zrI81Rl+AlK0OsOq2R51bJHVsafzwN/w2uUE4FmPNNiPmrMC5Ttm+35Y+MECLgTWELSMMq9r78CV0t6Kp+46+rxq7H8L2YbJuR2bNQ/hoX7V2tt3kbSX/JBipeBz7eX/93EgaP/+V/K2d7u3VDWnpQP8FmSnqH0b69EK91Vedb0c2Ay0NmbwQ8AGzVGJC0DrJbpLd1KOQPftcmyDwR2pNzAXw5Ys1ENQETcHhG7Uu5pXEvpmiMi5kTE1yJiDGWdHl3naZqc503KQewdImJmRBwUESsDh1DW82qULpXWNPMq6lUrw6NZeFY8FxhSmfa+mmU/TXn4oFr2jCba0ysyaM0EtqmcAA2LiMERMTuv6I6LiLWBj1NOcho3tjv7iu89gCci4ilJ61G6p8YDK0R5wORxcv9qo44rKA9QjIyI5XJYreR713Hg6Gci4mVKf/SPJe0uaYikJSTtIOn7NYsbB5wNbAB8KP+2oXTxrNvGPCcAB0sa0drEfNR0MLA4sLikwXm2COVS/UPZ7sHA8cDkiPiXeyYR8RLlZuSZknaVtFQu506STmil6mUoB/AXKAfQ71batJTK48vLRsRblCucBTltF0lrSBKl22x+Y1p7JA2XdABwOvC9iJjdSp59JDXOWmdTDi7z8yD4ArB6R/W04lBJI7M78VjKvQso9442lLSBpKUo67bq2Q7q+znwLUkr5rb9f5QDXX/yE+DExjqVtJKknXP4U5LWzauMOZSTjsZ27GjZ30HS+yR9nfIwQOOeyNAsbxawmKRDKVccDc8Coxs9AdmOpSnbeZ7Ko/J71l/kgcmBox+KiJOAIyk3cmdRui8OpdyobYrK9ye2BU7JftvG3x3A9bRxkzwi7qZc9RzVRtETKU8ZHQV8IYePzXmfBfYBvk958mUTyplhW8t5IuWDO5HyAXwK+HIby3k+5az5acoVzF9bTB8HPJHdWOMp3QYAawM3UrrRbqU8MdbedxsekPQq8AjwReCwiPh2G3k3A+7M+0NXAYdExJM57Xjg0uz2+Ew79bX0c8r2+QfwMOU+BxHxYA7flOkt79WcA2wk6SVJV7ZS7n9Sgs9UypNAtwPfq9Gu3vA9ynL9ObfjLSy8x7Aq8GvKScG9lIcqGg8dnASMz3Xd1jItmd/hmJvzfwzYOfIx4PxcnEu50f40pWvvnsr8v6Vcoc2S9ETemzsYOJVyQnIk5am/RYIi/ENOZmbWPF9xmJlZLQ4cZmZWiwOHmZnV4sBhZma1vCtftLbiiivGmDFj+roZZmYDypQpU56PiFYfxa96VwaOMWPGMHny5L5uhpnZgCLpiY5zuavKzMxqcuAwM7NaHDjMzKwWBw4zM6vFgcPMzGpx4DAzs1p6LHBIOk/Sc5KmVtJWkHSdpEfy//KZLkmnSZom6T5Jm1TmGZf5H2nih3LMzKyH9eQVxwXAp1ukHQPcEBFrATfkOMAOlN9YXguYAJwJJdBQXk+9GfAR4PhGsDEzs77RY4Ejf9u55c9t7kb5TWLy/+6V9IuiuA0YJmll4P8A1+VvSL9E+T3slsHIzMx6UW9/c3yliJiZw89QfsYUym//Vn9reXqmtZX+LyRNoFytMHr06C41cpfTb+nS/B359WFbLZJ1t1e/63bdrrtn6+5OfXZzPMovSHXbr0hFxFkRMTYixo4Y0eGrVszMrJN6O3A8m11Q5P/nMn0G5achG0ZlWlvpZmbWR3o7cExi4W9dj6P8bnAj/cB8umpz4OXs0voDsL2k5fOm+PaZZmZmfaTH7nFI+jmwLbCipOmUp6NOAC6XNB54Atgns/8W2BGYBrwGfBEgIl6U9F/AnZnv2xHR8oa7mZn1oh4LHBHx2TYmbddK3gAOaaOc84DzurFpZmbWBf7muJmZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlZLnwQOSV+T9ICkqZJ+LmmwpNUk3S5pmqTLJL0n8y6Z49Ny+pi+aLOZmRW9HjgkjQQOB8ZGxAeBxYH9gBOBkyNiTeAlYHzOMh54KdNPznxmZtZH+qqrahCwlKRBwBBgJvAJ4MqcfiGwew7vluPk9O0kqRfbamZmFb0eOCJiBvBD4ElKwHgZmALMjoi3M9t0YGQOjwSeynnfzvzDW5YraYKkyZImz5o1q2cXwsxsEdYXXVXLU64iVgNWAZYGPt3VciPirIgYGxFjR4wY0dXizMysDX3RVfVJ4LGImBURbwFXAVsCw7LrCmAUMCOHZwCrAuT05YAXerfJZmbW0BeB40lgc0lD8l7FdsCDwJ+AvTLPOOCaHJ6U4+T0GyMierG9ZmZW0Rf3OG6n3OS+C7g/23AWcDRwpKRplHsY5+Ys5wLDM/1I4JjebrOZmS00qOMs3S8ijgeOb5H8KPCRVvK+AezdG+0yM7OO+ZvjZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1NBU4JG3Q0w0xM7OBodkrjjMk3SHpK5KW69EWmZlZv9ZU4IiIrYH9gVWBKZIulfSpHm2ZmZn1S03f44iIR4DjgKOBjwGnSfqbpM/0VOPMzKz/afYex4aSTgYeAj4B7BIR6+bwyT3YPjMz62eaveI4HbgL2CgiDomIuwAi4mnKVUgtkoZJujKvWB6StIWkFSRdJ+mR/L985pWk0yRNk3SfpE3q1mdmZt2n2cCxE3BpRLwOIGkxSUMAIuLiTtR7KvD7iFgH2IhyJXMMcENErAXckOMAOwBr5d8E4MxO1GdmZt2k2cBxPbBUZXxIptWWT2VtA5wLEBHzImI2sBtwYWa7ENg9h3cDLoriNmCYpJU7U7eZmXVds4FjcES82hjJ4SGdrHM1YBZwvqS7JZ0jaWlgpYiYmXmeAVbK4ZHAU5X5p2eamZn1gWYDx9zqvQVJmwKvd7LOQcAmwJkRsTEwl4XdUgBERABRp1BJEyRNljR51qxZnWyamZl1pNnA8VXgCkk3S7oFuAw4tJN1TgemR8TtOX4lJZA82+iCyv/P5fQZlO+PNIzKtHeIiLMiYmxEjB0xYkQnm2ZmZh1p9guAdwLrAF8GDgbWjYgpnakwIp4BnpK0diZtBzwITALGZdo44JocngQcmE9XbQ68XOnSMjOzXjaoRt4PA2Nynk0kEREXdbLew4CfSXoP8CjwRUoQu1zSeOAJYJ/M+1tgR2Aa8FrmNTOzPtJU4JB0MbAGcA8wP5MD6FTgiIh7gLGtTNqulbwBHNKZeszMrPs1e8UxFlgvD+JmZrYIa/bm+FTgfT3ZEDMzGxiaveJYEXhQ0h3Am43EiNi1R1plZmb9VrOBY2JPNsLMzAaOpgJHRPxZ0vuBtSLi+nxP1eI92zQzM+uPmn2t+r9Rvqj300waCfyqpxplZmb9V7M3xw8BtgTmwD9/1Om9PdUoMzPrv5oNHG9GxLzGiKRB1HyXlJmZvTs0Gzj+LOmbwFL5W+NXAL/uuWaZmVl/1WzgOIbyKvT7gS9RXgNS+5f/zMxs4Gv2qaoFwNn5Z2Zmi7Bm31X1GK3c04iI1bu9RWZm1q/VeVdVw2Bgb2CF7m+OmZn1d83+HscLlb8ZEXEKsFMPt83MzPqhZruqNqmMLka5AqnzWx5mZvYu0ezB/6TK8NvA4yz8oSUzM1uENPtU1cd7uiFmZjYwNNtVdWR70yPiR93THDMz6+/qPFX1YWBSju8C3AE80hONMjOz/qvZwDEK2CQiXgGQNBH4TUR8vqcaZmZm/VOzrxxZCZhXGZ+XaWZmtohp9orjIuAOSVfn+O7AhT3TJDMz68+afarqu5J+B2ydSV+MiLt7rllmZtZfNdtVBTAEmBMRpwLTJa3WQ20yM7N+rNmfjj0eOBo4NpOWAC7pqUaZmVn/1ewVxx7ArsBcgIh4GlimpxplZmb9V7OBY15EBPlqdUlL91yTzMysP2s2cFwu6afAMEn/BlyPf9TJzGyR1OxTVT/M3xqfA6wNfCsiruvRlpmZWb/UYeCQtDhwfb7o0MHCzGwR12FXVUTMBxZIWq4X2mNmZv1cs98cfxW4X9J15JNVABFxeI+0yszM+q1mA8dV+WdmZou4dgOHpNER8WREdPt7qfLeyWRgRkTsnN9E/wUwHJgCHBAR8yQtSXlX1qbAC8C+EfF4d7fHzMya09E9jl81BiT9spvrPgJ4qDJ+InByRKwJvASMz/TxwEuZfnLmMzOzPtJR4FBlePXuqlTSKGAn4JwcF/AJ4MrMciHlDbwAu7HwTbxXAttlfjMz6wMdBY5oY7irTgG+ASzI8eHA7Ih4O8enAyNzeCTwFEBOfznzv4OkCZImS5o8a9asbmyqmZlVdRQ4NpI0R9IrwIY5PEfSK5LmdKZCSTsDz0XElM7M35aIOCsixkbE2BEjRnRn0WZmVtHuzfGIWLwH6twS2FXSjsBgYFngVMrrTAblVcUoYEbmnwGsSnmV+yBgOcpNcjMz6wN1fo+jW0TEsRExKiLGAPsBN0bE/sCfgL0y2zjgmhyelOPk9BvzhYtmZtYHej1wtONo4EhJ0yj3MM7N9HOB4Zl+JHBMH7XPzMxo/guAPSIibgJuyuFHgY+0kucNYO9ebZiZmbWpP11xmJnZAODAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlZLrwcOSatK+pOkByU9IOmITF9B0nWSHsn/y2e6JJ0maZqk+yRt0tttNjOzhfriiuNt4OsRsR6wOXCIpPWAY4AbImIt4IYcB9gBWCv/JgBn9n6TzcysodcDR0TMjIi7cvgV4CFgJLAbcGFmuxDYPYd3Ay6K4jZgmKSVe7nZZmaW+vQeh6QxwMbA7cBKETEzJz0DrJTDI4GnKrNNz7SWZU2QNFnS5FmzZvVYm83MFnV9FjgkDQV+CXw1IuZUp0VEAFGnvIg4KyLGRsTYESNGdGNLzcysqk8Ch6QlKEHjZxFxVSY/2+iCyv/PZfoMYNXK7KMyzczM+kBfPFUl4FzgoYj4UWXSJGBcDo8DrqmkH5hPV20OvFzp0jIzs142qA/q3BI4ALhf0j2Z9k3gBOBySeOBJ4B9ctpvgR2BacBrwBd7t7lmZlbV64EjIm4B1Mbk7VrJH8AhPdooMzNrmr85bmZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrU4cJiZWS0OHGZmVosDh5mZ1eLAYWZmtThwmJlZLQ4cZmZWiwOHmZnV4sBhZma1OHCYmVktDhxmZlaLA4eZmdXiwGFmZrUMmMAh6dOSHpY0TdIxfd0eM7NF1YAIHJIWB34M7ACsB3xW0np92yozs0XTgAgcwEeAaRHxaETMA34B7NbHbTIzWyQpIvq6DR2StBfw6Yg4KMcPADaLiEMreSYAE3J0beDhXmziisDzvVif63bdrtt194T3R8SIjjIN6o2W9IaIOAs4qy/qljQ5Isa6btftul33u6Xu9gyUrqoZwKqV8VGZZmZmvWygBI47gbUkrSbpPcB+wKQ+bpOZ2SJpQHRVRcTbkg4F/gAsDpwXEQ/0cbOq+qSLzHW7btftuvvCgLg5bmZm/cdA6aoyM7N+woHDzMxqceCokPRbScNqznNBfs+kK/Xu3tPfhJc0RtLUNqad06hf0uOSVmwrfzVvB/VtK+narre8OVnfR7uxvImSjuqu8gZCGyQdLukhST/r4Xra3Bf7g8ZnoJX0Xbv6uiNJwyR9pStlVMrq1c9YlQNHRUTsGBGzq2kqeno97U55lUqfiIiDIuLBruTN18L0pW2Bbgsc3UHSgHj4pOIrwKciYv9GQn9bhr5sT0RMiogTuljMMMp6fof+tp47ssgGDkm/kjRF0gP5rfOWZ9sPS7oImAqsKulVSSdn/hsk/cu3KyV9S9KdkqZmWY3yH5Z0oqT5kv4uaWtJe0m6MM/uPg9cKukpSWtI+pCk2yTdJ+lqSctn+TdlGybnmeGHJV0l6RFJ36m048hsw1RJX600cZCkn+W8V0oaUim3tS8ZrSDpeUlvSHpU0tBq3lwnJ0m6F9hC5UWUf5N0F/CZbtpOB+Z6uFfSxZJ2kXS7pLslXS9pJUljgIOBr0m6R9LWnazrP3L73EJ5+wC5PX6f2/JmSetk+ghJv8ztfaekLTN9YrbzVuDibmpDW/vDhzPtHkk/6MpZvKSfAKsDv5P0cnUZJA2WdL6k+3O9fzznGSLpckkPZrtub2M/as3iks7Oz8cfJS3VwX5/iqTJwBGS9s59+15Jf8k8i+c6uDPn/1KTy720pN9kWVMl7ZuTDpN0Vy5zY5t/QdL/5PAFkn6Sn8W/S9q5yeU+AVgjt9mduU9NAh5UiysxSUdJmpjDa+b+fm+2a40Wy/Hh3DbvSO8xEbFI/gEr5P+lKMFhOPA45Sv+Y4AFwOaV/AHsn8PfAv4nhy8A9qqWmcOXAbtk+XMpL2l8FdgRuB7YC/grcFyWsR8wGVgNuA/4WJbzbeCUHL4JODGHjwCeBlYGlgSm5zJsCtwPLA0MBR4ANs5lCmDLnP884KhKuWNzuLEOtsv822T6Q8DPW+QNYJ8cHgw8BawFCLgcuLaL22h94O/Aio31CyzPwqcBDwJOyuGJjeXpZF2N9TYEWBaYBhwF3ACslXk2A27M4UuBrXJ4NPBQpR1TgKW6sQ1t7Q9TgS1y+ARgahfXd2Pbv2MZgK9THoEHWAd4Mrf3UcBPM/2DwNuNfaODesZk3g/l+OWUk6f29vszKvPfD4zM4WH5fwJwXA4vSX6WmmjLnsDZlfHlcj0cluNfAc7J4S/wzs/97ykn32tRPn+Dm1z2qTm8LeXYsFrLaTl+FDAxh28H9qh81obk/NdSrrSnAKO7sv3r/A2oy6NudrikPXJ4VcrGr3oiIm6rjC+gBAOAS4CrWinz45K+Qdmo7we2AZ6j7Mj3Zp4plB0EYBXgQMqH9WOZthHlw/DnHL8QuKJSR+OLj/cDD0TETABJj+ZybAVcHRFzM/0qYOuc76mIuLWyDIcDP2xlOQC2BOYDp0mCEoQ2Bp6p5JkP/DKH1wEei4hHst5LWPjusM76BHBFRDwPEBEvStoAuEzSysB7gMe6WEfD1pT19hpAngUOpnwor8h1AGVbAnwSWK+SvqykoTk8KSJe76Y2LE0r+4PKvbhlIuJ/M/1SoNmz3mZUl2Er4HSAiPibpCeAD2T6qZk+VdJ9Ncp/LCLuyeEpwBq0v99fVhm+FbhA0uUs/BxuD2yohfcbl6N8pjvaP+4HTpJ0IuVE5+bcpo1yp9D21fPlEbEAeCQ/f+sA97SRty13RES7bZS0DCVQXg0QEW9kOsC6lO96bB8RT9esu9MWycAhaVvKB3+LiHhN0k2Ug0TV3A6KeccXYCQNBs4AxlI+BBcBF0fEf0iaTTkLD8rBdlClvsOAz1J22islLddBvW/m/wWV4cZ4R9uz5Zd22vsSj4C5EfEhAEmfyLYuX8nzRkTM76DO7nY68KOImJTbcWIP1rUYMLuxDlqZtnnjQ9yQH+aO9p2BoKeXobrvzqf0/bfnn+2JiIMlbQbsBEyRtCllfz0sIv5QpxER8XdJm1B6Ar4j6YYW7Wt8XludvYPxZlTX89u88/ZBy2NSa2Zmvo0pPRC9YlG9x7Ec8FIGjXWAzZuYZzFK9xLA54BbWkxvbOTngZUoH4S3svxlc9qzLLyy2YOyob9M2XmWkfQBys7zkhb20x8ANM7CmnEzsHv2Py+d9dyc00ZL2qKdZai6lXIWvWOOf5HS7dWWvwFjKn2sn63R5rbcCOwtaTiApBUo267xnrJxlbyvAMt0oa6/UNbbUnmGtwvwGvCYpL2zfknaKPP/kRJIyWmtBZfuaMNcWtkfojzE8UoeQKF0dfaUm4H9AXIfHU15+/StwD6Zvh6wQRfqeJkm93tJa0TE7RHxLWAW5Ur7D8CXJS3RaGfu/+2StArwWkRcAvwA2KRGm/eWtFju86vT3Bu529tPnwXeK2m4pCXJK8iIeAWYLmn3bPOSyvuTwGxKAP1enkj1ikXyioPSN3mwpIcoG/u2DvJD+QB/RNJxlO6nfasTI2K2pLMp/c7PAnOAQ4AP5TDAMZQuhZUpZwqvUILHvsB44C3K5f844Ce5czxKOWg3JSLuknQBcEcmnRMRd6vcQH4YOETSecCDwJntFDUt23ZJtuN1ytNfW7VR7xsqDxn8RtJrlINNVw7kRMQDkr4L/FnSfOBuyhXGFZJeogSW1TL7r4ErJe1GOfO8ubUy26nrLkmXUboUn6O8Hw3KAfPM3O5LUH4L5l5KN9+Ps3tmEOWgf3CnF7b9NrS1P4wHzpa0gHKQfbk8kkpxAAABBElEQVQr9bfjDMo6uJ9yYvOFiHhT0hnAhZIepJw4PNDFNjS73/9AUuNe2g2U9XUfpQv4LpXLvlmU/bUjG2R5Cyifvy8DVzbZ3icpn7NlgYNbXn22JiJekHRr3gR/nXKsaEx7S9K3s8wZlHXacADw05z+FrB3Zb5n8+b87yT934i4vcn2d5pfOdIkSa9GxNCOc5r1DklDI+LVHD4GWDkijujF+hcHlsiThjUoD32sHeXH1t7V8uTs2ohoNsi8qyyqVxxm7wY7STqW8jl+gvLUT28aAvwpu4cEfGVRCBrmKw4zM6tpUb05bmZmneTAYWZmtThwmJlZLQ4cZmZWiwOHmZnV8v8BtcmNw8Q2EwUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(list(y_test), y_classes, ylabel='Frequency',title='CIFAR 10 Class Distribution in Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmdwlu8pG6g3"
   },
   "outputs": [],
   "source": [
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JGd4ezgG6g7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "z_arXgOSG6hA",
    "outputId": "8960acf7-335b-4c96-9ae9-7fe9b9ad56fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  62,  63],\n",
       "       [ 43,  46,  45],\n",
       "       [ 50,  48,  43],\n",
       "       [ 68,  54,  42],\n",
       "       [ 98,  73,  52],\n",
       "       [119,  91,  63],\n",
       "       [139, 107,  75],\n",
       "       [145, 110,  80],\n",
       "       [149, 117,  89],\n",
       "       [149, 120,  93],\n",
       "       [131, 103,  77],\n",
       "       [125,  99,  76],\n",
       "       [142, 115,  91],\n",
       "       [144, 112,  86],\n",
       "       [137, 105,  79],\n",
       "       [129,  97,  71],\n",
       "       [137, 106,  79],\n",
       "       [134, 106,  76],\n",
       "       [124,  97,  64],\n",
       "       [139, 113,  78],\n",
       "       [139, 112,  75],\n",
       "       [133, 105,  69],\n",
       "       [136, 105,  74],\n",
       "       [139, 108,  77],\n",
       "       [152, 120,  89],\n",
       "       [163, 131, 100],\n",
       "       [168, 136, 108],\n",
       "       [159, 129, 102],\n",
       "       [158, 130, 104],\n",
       "       [158, 132, 108],\n",
       "       [152, 125, 102],\n",
       "       [148, 124, 103]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train[0][0:32][0:32][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "jd2usuXDG6hH",
    "outputId": "a0716d82-94e5-4f1b-ba28-39b1cdced01b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "T372AU6RG6hN",
    "outputId": "4739d668-5818-4f70-e6d3-3491edcb274c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Mean :  [[[[125.3069  122.95015 113.866  ]]]]\n",
      "Channel Std :  [[[[62.99325  62.088604 66.70501 ]]]]\n",
      "Channel Mean1 :  [[[[126.02428 123.70843 114.85442]]]]\n",
      "Channel Std1 :  [[[[62.896416 61.937508 66.70607 ]]]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "mean  = np.mean(x_train, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "std   = np.std(x_train, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "\n",
    "mean1  = np.mean(x_test, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "std1   = np.std(x_test, axis=(0, 1, 2), keepdims=True).astype('float32')\n",
    "\n",
    "\n",
    "print(\"Channel Mean : \", mean)\n",
    "print(\"Channel Std : \", std)\n",
    "print(\"Channel Mean1 : \", mean1)\n",
    "print(\"Channel Std1 : \", std1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = (x_train - mean) / (std)\n",
    "x_test  = (x_test - mean1) / (std1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOCMAYM1G6hT"
   },
   "outputs": [],
   "source": [
    "#x_train = (x_train - 127.5)/255.0\n",
    "#x_test  = (x_test  - 127.5)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZF74U6zcG6hX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.0526032e+00, -9.8166406e-01, -7.6255137e-01],\n",
       "        [-1.3065987e+00, -1.2393603e+00, -1.0323962e+00],\n",
       "        [-1.1954757e+00, -1.2071482e+00, -1.0623789e+00],\n",
       "        ...,\n",
       "        [ 5.1899368e-01,  1.4575703e-01, -8.7939382e-02],\n",
       "        [ 4.2374539e-01,  3.3014923e-02, -1.7788765e-01],\n",
       "        [ 3.6024651e-01,  1.6908908e-02, -1.6289628e-01]],\n",
       "\n",
       "       [[-1.7352160e+00, -1.6581167e+00, -1.4071807e+00],\n",
       "        [-1.9892114e+00, -1.9802370e+00, -1.7070082e+00],\n",
       "        [-1.7034665e+00, -1.8513888e+00, -1.7070082e+00],\n",
       "        ...,\n",
       "        [-3.6621384e-02, -5.6290764e-01, -8.8248241e-01],\n",
       "        [-1.0012025e-01, -6.4343774e-01, -9.5743930e-01],\n",
       "        [-5.2496098e-02, -5.7901365e-01, -8.5249966e-01]],\n",
       "\n",
       "       [[-1.5923436e+00, -1.5936927e+00, -1.3921893e+00],\n",
       "        [-1.7352160e+00, -1.8674948e+00, -1.7070082e+00],\n",
       "        [-1.2113504e+00, -1.5453745e+00, -1.5870771e+00],\n",
       "        ...,\n",
       "        [-1.1599497e-01, -6.2733167e-01, -9.5743930e-01],\n",
       "        [-8.4245533e-02, -6.2733167e-01, -9.5743930e-01],\n",
       "        [-2.5886741e-01, -8.0449790e-01, -1.0773703e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.3127295e+00,  7.5778562e-01, -2.6783592e-01],\n",
       "        [ 1.2016065e+00,  4.8398334e-01, -1.1973014e+00],\n",
       "        [ 1.1539823e+00,  6.1283147e-01, -1.3172324e+00],\n",
       "        ...,\n",
       "        [ 5.5074310e-01,  1.6186304e-01, -6.5761173e-01],\n",
       "        [-1.1002274e+00, -1.4809505e+00, -1.6020685e+00],\n",
       "        [-1.1478515e+00, -1.4326324e+00, -1.4071807e+00]],\n",
       "\n",
       "       [[ 8.6823744e-01,  2.5849915e-01, -2.6783592e-01],\n",
       "        [ 7.5711441e-01,  8.0289232e-04, -1.0773703e+00],\n",
       "        [ 9.6348572e-01,  3.3902922e-01, -1.2572669e+00],\n",
       "        ...,\n",
       "        [ 9.3173629e-01,  4.0345326e-01, -2.9781866e-01],\n",
       "        [-4.4936401e-01, -9.8166406e-01, -1.1973014e+00],\n",
       "        [-6.7161006e-01, -1.1266181e+00, -1.1973014e+00]],\n",
       "\n",
       "       [[ 8.2061332e-01,  3.3902922e-01,  3.1991642e-02],\n",
       "        [ 6.7774087e-01,  9.7438984e-02, -2.9781866e-01],\n",
       "        [ 8.5236275e-01,  3.0681717e-01, -4.0275833e-01],\n",
       "        ...,\n",
       "        [ 1.4397272e+00,  9.8326981e-01,  3.9178470e-01],\n",
       "        [ 4.0787068e-01, -7.9727180e-02, -4.4773245e-01],\n",
       "        [-3.6621384e-02, -4.9848357e-01, -6.2762898e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "acEl5EjAG6hd",
    "outputId": "9efce3d3-3074-4f2e-b660-4c6b2f0caf20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XuGiXp6rG6hk",
    "outputId": "acc28bb5-470e-48c3-8e6c-354db61c19f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb0EH5H_G6hq"
   },
   "outputs": [],
   "source": [
    "if do_sub_sampling_of_input:\n",
    "    x_, x_train, x_, y_train    = cross_validation.train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
    "    x_, x_test,  y_, y_test    = cross_validation.train_test_split(x_test, y_test, test_size=0.25, random_state=0)\n",
    "    print(\"After SubSampling\")\n",
    "    print(x_train.shape, x_test.shape)\n",
    "    print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jz7L_trG6hv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if do_data_append :\n",
    "    print(\"Doing Data Appending\")\n",
    "    x_train = np.append(x_train, x_train,axis=0)\n",
    "    y_train = np.append(y_train, y_train,axis=0)\n",
    "#print(np.append(x_train, x_train,axis=0).shape)\n",
    "#print(np.append(y_train, y_train,axis=0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MkHX4hNLG6h0",
    "outputId": "25cf1428-1767-461c-ced2-577e40a2dec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "(50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMuvM67DG6h8"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npxonLr5G6h-"
   },
   "outputs": [],
   "source": [
    "keras.utils.Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,    # randomly flip images\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Tk4q4iYG6iD"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SeparableConv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction densenet -BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, growth_rate, dropout_rate = 0.2, l = 0):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        #Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        #Conv2D_1_1 = Conv2D(int(num_filter*4*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "        Conv2D_1_1 = Conv2D(int(growth_rate*4), (1,1), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_1_1 = Dropout(dropout_rate)(Conv2D_1_1)\n",
    "        BatchNorm_1_1 = BatchNormalization()(Conv2D_1_1)\n",
    "        relu_1_1 = Activation('relu')(BatchNorm_1_1)\n",
    "        \n",
    "        Conv2D_3_3 = Conv2D(int(growth_rate), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        #Conv2D_3_3 = SeparableConv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu_1_1)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eEu8gikG6iP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    num_channels = int(input.shape[-1]) #assuming it is tensor\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_channels*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    \n",
    "    #Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    #flat = Dropout(0.25)(flat)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbzC-GOZG6ie"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "#num_filter = 12\n",
    "num_filter = growth_rate = 12\n",
    "\n",
    "dropout_rate = 0.2\n",
    "#compression = 0.751\n",
    "compression = 0.5\n",
    "l = 16\n",
    "dense_l= [8, 16, 20, 12]\n",
    "dense_l= [12, 12, 12, 12]\n",
    "dense_l= [14, 14, 14, 14]\n",
    "\n",
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(2*num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, growth_rate, dropout_rate, dense_l[0])\n",
    "First_Transition = add_transition(First_Block, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, growth_rate, dropout_rate, dense_l[1])\n",
    "Second_Transition = add_transition(Second_Block, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, growth_rate, dropout_rate, dense_l[2])\n",
    "Third_Transition = add_transition(Third_Block, dropout_rate)\n",
    "\n",
    "Last_Block = add_denseblock(Third_Transition,  growth_rate, dropout_rate, dense_l[3])\n",
    "output = output_layer(Last_Block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing and Verifying the Densenet-BC configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "p9uA_iR8G6io",
    "outputId": "87a7b724-7e86-416d-e7e9-6526b4bc860b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(?, 32, 32, 24)\n",
      "(?, 32, 32, 192) 192\n",
      "(?, 16, 16, 96)\n",
      "(?, 16, 16, 264)\n",
      "(?, 8, 8, 132)\n",
      "(?, 8, 8, 300)\n",
      "(?, 4, 4, 150)\n",
      "(?, 4, 4, 318)\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "print(First_Conv2D.shape)\n",
    "print(First_Block.shape, First_Block.shape[-1])\n",
    "print(First_Transition.shape)\n",
    "\n",
    "print(Second_Block.shape)\n",
    "print(Second_Transition.shape)\n",
    "\n",
    "print(Third_Block.shape)\n",
    "print(Third_Transition.shape)\n",
    "\n",
    "print(Last_Block.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 19618
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "7bc989ab-6517-466c-8120-25c63379a4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_817 (Conv2D)             (None, 32, 32, 24)   648         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_817 (BatchN (None, 32, 32, 24)   96          conv2d_817[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_817 (Activation)     (None, 32, 32, 24)   0           batch_normalization_817[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_818 (Conv2D)             (None, 32, 32, 48)   1152        activation_817[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_809 (Dropout)           (None, 32, 32, 48)   0           conv2d_818[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_818 (BatchN (None, 32, 32, 48)   192         dropout_809[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_818 (Activation)     (None, 32, 32, 48)   0           batch_normalization_818[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_819 (Conv2D)             (None, 32, 32, 12)   5184        activation_818[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_810 (Dropout)           (None, 32, 32, 12)   0           conv2d_819[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_393 (Concatenate)   (None, 32, 32, 36)   0           conv2d_817[0][0]                 \n",
      "                                                                 dropout_810[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_819 (BatchN (None, 32, 32, 36)   144         concatenate_393[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_819 (Activation)     (None, 32, 32, 36)   0           batch_normalization_819[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_820 (Conv2D)             (None, 32, 32, 48)   1728        activation_819[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_811 (Dropout)           (None, 32, 32, 48)   0           conv2d_820[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_820 (BatchN (None, 32, 32, 48)   192         dropout_811[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_820 (Activation)     (None, 32, 32, 48)   0           batch_normalization_820[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_821 (Conv2D)             (None, 32, 32, 12)   5184        activation_820[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_812 (Dropout)           (None, 32, 32, 12)   0           conv2d_821[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_394 (Concatenate)   (None, 32, 32, 48)   0           concatenate_393[0][0]            \n",
      "                                                                 dropout_812[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_821 (BatchN (None, 32, 32, 48)   192         concatenate_394[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_821 (Activation)     (None, 32, 32, 48)   0           batch_normalization_821[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_822 (Conv2D)             (None, 32, 32, 48)   2304        activation_821[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_813 (Dropout)           (None, 32, 32, 48)   0           conv2d_822[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_822 (BatchN (None, 32, 32, 48)   192         dropout_813[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_822 (Activation)     (None, 32, 32, 48)   0           batch_normalization_822[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_823 (Conv2D)             (None, 32, 32, 12)   5184        activation_822[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_814 (Dropout)           (None, 32, 32, 12)   0           conv2d_823[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_395 (Concatenate)   (None, 32, 32, 60)   0           concatenate_394[0][0]            \n",
      "                                                                 dropout_814[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_823 (BatchN (None, 32, 32, 60)   240         concatenate_395[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_823 (Activation)     (None, 32, 32, 60)   0           batch_normalization_823[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_824 (Conv2D)             (None, 32, 32, 48)   2880        activation_823[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_815 (Dropout)           (None, 32, 32, 48)   0           conv2d_824[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_824 (BatchN (None, 32, 32, 48)   192         dropout_815[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_824 (Activation)     (None, 32, 32, 48)   0           batch_normalization_824[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_825 (Conv2D)             (None, 32, 32, 12)   5184        activation_824[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_816 (Dropout)           (None, 32, 32, 12)   0           conv2d_825[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_396 (Concatenate)   (None, 32, 32, 72)   0           concatenate_395[0][0]            \n",
      "                                                                 dropout_816[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_825 (BatchN (None, 32, 32, 72)   288         concatenate_396[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_825 (Activation)     (None, 32, 32, 72)   0           batch_normalization_825[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_826 (Conv2D)             (None, 32, 32, 48)   3456        activation_825[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_817 (Dropout)           (None, 32, 32, 48)   0           conv2d_826[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_826 (BatchN (None, 32, 32, 48)   192         dropout_817[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_826 (Activation)     (None, 32, 32, 48)   0           batch_normalization_826[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_827 (Conv2D)             (None, 32, 32, 12)   5184        activation_826[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_818 (Dropout)           (None, 32, 32, 12)   0           conv2d_827[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_397 (Concatenate)   (None, 32, 32, 84)   0           concatenate_396[0][0]            \n",
      "                                                                 dropout_818[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_827 (BatchN (None, 32, 32, 84)   336         concatenate_397[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_827 (Activation)     (None, 32, 32, 84)   0           batch_normalization_827[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_828 (Conv2D)             (None, 32, 32, 48)   4032        activation_827[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_819 (Dropout)           (None, 32, 32, 48)   0           conv2d_828[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_828 (BatchN (None, 32, 32, 48)   192         dropout_819[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_828 (Activation)     (None, 32, 32, 48)   0           batch_normalization_828[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_829 (Conv2D)             (None, 32, 32, 12)   5184        activation_828[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_820 (Dropout)           (None, 32, 32, 12)   0           conv2d_829[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_398 (Concatenate)   (None, 32, 32, 96)   0           concatenate_397[0][0]            \n",
      "                                                                 dropout_820[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_829 (BatchN (None, 32, 32, 96)   384         concatenate_398[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_829 (Activation)     (None, 32, 32, 96)   0           batch_normalization_829[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_830 (Conv2D)             (None, 32, 32, 48)   4608        activation_829[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_821 (Dropout)           (None, 32, 32, 48)   0           conv2d_830[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_830 (BatchN (None, 32, 32, 48)   192         dropout_821[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_830 (Activation)     (None, 32, 32, 48)   0           batch_normalization_830[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_831 (Conv2D)             (None, 32, 32, 12)   5184        activation_830[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_822 (Dropout)           (None, 32, 32, 12)   0           conv2d_831[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_399 (Concatenate)   (None, 32, 32, 108)  0           concatenate_398[0][0]            \n",
      "                                                                 dropout_822[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_831 (BatchN (None, 32, 32, 108)  432         concatenate_399[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_831 (Activation)     (None, 32, 32, 108)  0           batch_normalization_831[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_832 (Conv2D)             (None, 32, 32, 48)   5184        activation_831[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_823 (Dropout)           (None, 32, 32, 48)   0           conv2d_832[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_832 (BatchN (None, 32, 32, 48)   192         dropout_823[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_832 (Activation)     (None, 32, 32, 48)   0           batch_normalization_832[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_833 (Conv2D)             (None, 32, 32, 12)   5184        activation_832[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_824 (Dropout)           (None, 32, 32, 12)   0           conv2d_833[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_400 (Concatenate)   (None, 32, 32, 120)  0           concatenate_399[0][0]            \n",
      "                                                                 dropout_824[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_833 (BatchN (None, 32, 32, 120)  480         concatenate_400[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_833 (Activation)     (None, 32, 32, 120)  0           batch_normalization_833[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_834 (Conv2D)             (None, 32, 32, 48)   5760        activation_833[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_825 (Dropout)           (None, 32, 32, 48)   0           conv2d_834[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_834 (BatchN (None, 32, 32, 48)   192         dropout_825[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_834 (Activation)     (None, 32, 32, 48)   0           batch_normalization_834[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_835 (Conv2D)             (None, 32, 32, 12)   5184        activation_834[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_826 (Dropout)           (None, 32, 32, 12)   0           conv2d_835[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_401 (Concatenate)   (None, 32, 32, 132)  0           concatenate_400[0][0]            \n",
      "                                                                 dropout_826[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_835 (BatchN (None, 32, 32, 132)  528         concatenate_401[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_835 (Activation)     (None, 32, 32, 132)  0           batch_normalization_835[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_836 (Conv2D)             (None, 32, 32, 48)   6336        activation_835[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_827 (Dropout)           (None, 32, 32, 48)   0           conv2d_836[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_836 (BatchN (None, 32, 32, 48)   192         dropout_827[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_836 (Activation)     (None, 32, 32, 48)   0           batch_normalization_836[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_837 (Conv2D)             (None, 32, 32, 12)   5184        activation_836[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_828 (Dropout)           (None, 32, 32, 12)   0           conv2d_837[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_402 (Concatenate)   (None, 32, 32, 144)  0           concatenate_401[0][0]            \n",
      "                                                                 dropout_828[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_837 (BatchN (None, 32, 32, 144)  576         concatenate_402[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_837 (Activation)     (None, 32, 32, 144)  0           batch_normalization_837[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_838 (Conv2D)             (None, 32, 32, 48)   6912        activation_837[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_829 (Dropout)           (None, 32, 32, 48)   0           conv2d_838[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_838 (BatchN (None, 32, 32, 48)   192         dropout_829[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_838 (Activation)     (None, 32, 32, 48)   0           batch_normalization_838[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_839 (Conv2D)             (None, 32, 32, 12)   5184        activation_838[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_830 (Dropout)           (None, 32, 32, 12)   0           conv2d_839[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_403 (Concatenate)   (None, 32, 32, 156)  0           concatenate_402[0][0]            \n",
      "                                                                 dropout_830[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_839 (BatchN (None, 32, 32, 156)  624         concatenate_403[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_839 (Activation)     (None, 32, 32, 156)  0           batch_normalization_839[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_840 (Conv2D)             (None, 32, 32, 48)   7488        activation_839[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_831 (Dropout)           (None, 32, 32, 48)   0           conv2d_840[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_840 (BatchN (None, 32, 32, 48)   192         dropout_831[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_840 (Activation)     (None, 32, 32, 48)   0           batch_normalization_840[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_841 (Conv2D)             (None, 32, 32, 12)   5184        activation_840[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_832 (Dropout)           (None, 32, 32, 12)   0           conv2d_841[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_404 (Concatenate)   (None, 32, 32, 168)  0           concatenate_403[0][0]            \n",
      "                                                                 dropout_832[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_841 (BatchN (None, 32, 32, 168)  672         concatenate_404[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_841 (Activation)     (None, 32, 32, 168)  0           batch_normalization_841[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_842 (Conv2D)             (None, 32, 32, 48)   8064        activation_841[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_833 (Dropout)           (None, 32, 32, 48)   0           conv2d_842[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_842 (BatchN (None, 32, 32, 48)   192         dropout_833[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_842 (Activation)     (None, 32, 32, 48)   0           batch_normalization_842[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_843 (Conv2D)             (None, 32, 32, 12)   5184        activation_842[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_834 (Dropout)           (None, 32, 32, 12)   0           conv2d_843[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_405 (Concatenate)   (None, 32, 32, 180)  0           concatenate_404[0][0]            \n",
      "                                                                 dropout_834[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_843 (BatchN (None, 32, 32, 180)  720         concatenate_405[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_843 (Activation)     (None, 32, 32, 180)  0           batch_normalization_843[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_844 (Conv2D)             (None, 32, 32, 48)   8640        activation_843[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_835 (Dropout)           (None, 32, 32, 48)   0           conv2d_844[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_844 (BatchN (None, 32, 32, 48)   192         dropout_835[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_844 (Activation)     (None, 32, 32, 48)   0           batch_normalization_844[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_845 (Conv2D)             (None, 32, 32, 12)   5184        activation_844[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_836 (Dropout)           (None, 32, 32, 12)   0           conv2d_845[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_406 (Concatenate)   (None, 32, 32, 192)  0           concatenate_405[0][0]            \n",
      "                                                                 dropout_836[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_845 (BatchN (None, 32, 32, 192)  768         concatenate_406[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_845 (Activation)     (None, 32, 32, 192)  0           batch_normalization_845[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_846 (Conv2D)             (None, 32, 32, 96)   18432       activation_845[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_837 (Dropout)           (None, 32, 32, 96)   0           conv2d_846[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_33 (AveragePo (None, 16, 16, 96)   0           dropout_837[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_846 (BatchN (None, 16, 16, 96)   384         average_pooling2d_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_846 (Activation)     (None, 16, 16, 96)   0           batch_normalization_846[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_847 (Conv2D)             (None, 16, 16, 48)   4608        activation_846[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_838 (Dropout)           (None, 16, 16, 48)   0           conv2d_847[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_847 (BatchN (None, 16, 16, 48)   192         dropout_838[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_847 (Activation)     (None, 16, 16, 48)   0           batch_normalization_847[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_848 (Conv2D)             (None, 16, 16, 12)   5184        activation_847[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_839 (Dropout)           (None, 16, 16, 12)   0           conv2d_848[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_407 (Concatenate)   (None, 16, 16, 108)  0           average_pooling2d_33[0][0]       \n",
      "                                                                 dropout_839[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_848 (BatchN (None, 16, 16, 108)  432         concatenate_407[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_848 (Activation)     (None, 16, 16, 108)  0           batch_normalization_848[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_849 (Conv2D)             (None, 16, 16, 48)   5184        activation_848[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_840 (Dropout)           (None, 16, 16, 48)   0           conv2d_849[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_849 (BatchN (None, 16, 16, 48)   192         dropout_840[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_849 (Activation)     (None, 16, 16, 48)   0           batch_normalization_849[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_850 (Conv2D)             (None, 16, 16, 12)   5184        activation_849[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_841 (Dropout)           (None, 16, 16, 12)   0           conv2d_850[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_408 (Concatenate)   (None, 16, 16, 120)  0           concatenate_407[0][0]            \n",
      "                                                                 dropout_841[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_850 (BatchN (None, 16, 16, 120)  480         concatenate_408[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_850 (Activation)     (None, 16, 16, 120)  0           batch_normalization_850[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_851 (Conv2D)             (None, 16, 16, 48)   5760        activation_850[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_842 (Dropout)           (None, 16, 16, 48)   0           conv2d_851[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_851 (BatchN (None, 16, 16, 48)   192         dropout_842[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_851 (Activation)     (None, 16, 16, 48)   0           batch_normalization_851[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_852 (Conv2D)             (None, 16, 16, 12)   5184        activation_851[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_843 (Dropout)           (None, 16, 16, 12)   0           conv2d_852[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_409 (Concatenate)   (None, 16, 16, 132)  0           concatenate_408[0][0]            \n",
      "                                                                 dropout_843[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_852 (BatchN (None, 16, 16, 132)  528         concatenate_409[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_852 (Activation)     (None, 16, 16, 132)  0           batch_normalization_852[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_853 (Conv2D)             (None, 16, 16, 48)   6336        activation_852[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_844 (Dropout)           (None, 16, 16, 48)   0           conv2d_853[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_853 (BatchN (None, 16, 16, 48)   192         dropout_844[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_853 (Activation)     (None, 16, 16, 48)   0           batch_normalization_853[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_854 (Conv2D)             (None, 16, 16, 12)   5184        activation_853[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_845 (Dropout)           (None, 16, 16, 12)   0           conv2d_854[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_410 (Concatenate)   (None, 16, 16, 144)  0           concatenate_409[0][0]            \n",
      "                                                                 dropout_845[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_854 (BatchN (None, 16, 16, 144)  576         concatenate_410[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_854 (Activation)     (None, 16, 16, 144)  0           batch_normalization_854[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_855 (Conv2D)             (None, 16, 16, 48)   6912        activation_854[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_846 (Dropout)           (None, 16, 16, 48)   0           conv2d_855[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_855 (BatchN (None, 16, 16, 48)   192         dropout_846[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_855 (Activation)     (None, 16, 16, 48)   0           batch_normalization_855[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_856 (Conv2D)             (None, 16, 16, 12)   5184        activation_855[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_847 (Dropout)           (None, 16, 16, 12)   0           conv2d_856[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_411 (Concatenate)   (None, 16, 16, 156)  0           concatenate_410[0][0]            \n",
      "                                                                 dropout_847[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_856 (BatchN (None, 16, 16, 156)  624         concatenate_411[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_856 (Activation)     (None, 16, 16, 156)  0           batch_normalization_856[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_857 (Conv2D)             (None, 16, 16, 48)   7488        activation_856[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_848 (Dropout)           (None, 16, 16, 48)   0           conv2d_857[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_857 (BatchN (None, 16, 16, 48)   192         dropout_848[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_857 (Activation)     (None, 16, 16, 48)   0           batch_normalization_857[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_858 (Conv2D)             (None, 16, 16, 12)   5184        activation_857[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_849 (Dropout)           (None, 16, 16, 12)   0           conv2d_858[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_412 (Concatenate)   (None, 16, 16, 168)  0           concatenate_411[0][0]            \n",
      "                                                                 dropout_849[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_858 (BatchN (None, 16, 16, 168)  672         concatenate_412[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_858 (Activation)     (None, 16, 16, 168)  0           batch_normalization_858[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_859 (Conv2D)             (None, 16, 16, 48)   8064        activation_858[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_850 (Dropout)           (None, 16, 16, 48)   0           conv2d_859[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_859 (BatchN (None, 16, 16, 48)   192         dropout_850[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_859 (Activation)     (None, 16, 16, 48)   0           batch_normalization_859[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_860 (Conv2D)             (None, 16, 16, 12)   5184        activation_859[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_851 (Dropout)           (None, 16, 16, 12)   0           conv2d_860[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_413 (Concatenate)   (None, 16, 16, 180)  0           concatenate_412[0][0]            \n",
      "                                                                 dropout_851[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_860 (BatchN (None, 16, 16, 180)  720         concatenate_413[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_860 (Activation)     (None, 16, 16, 180)  0           batch_normalization_860[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_861 (Conv2D)             (None, 16, 16, 48)   8640        activation_860[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_852 (Dropout)           (None, 16, 16, 48)   0           conv2d_861[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_861 (BatchN (None, 16, 16, 48)   192         dropout_852[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_861 (Activation)     (None, 16, 16, 48)   0           batch_normalization_861[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_862 (Conv2D)             (None, 16, 16, 12)   5184        activation_861[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_853 (Dropout)           (None, 16, 16, 12)   0           conv2d_862[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_414 (Concatenate)   (None, 16, 16, 192)  0           concatenate_413[0][0]            \n",
      "                                                                 dropout_853[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_862 (BatchN (None, 16, 16, 192)  768         concatenate_414[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_862 (Activation)     (None, 16, 16, 192)  0           batch_normalization_862[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_863 (Conv2D)             (None, 16, 16, 48)   9216        activation_862[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_854 (Dropout)           (None, 16, 16, 48)   0           conv2d_863[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_863 (BatchN (None, 16, 16, 48)   192         dropout_854[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_863 (Activation)     (None, 16, 16, 48)   0           batch_normalization_863[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_864 (Conv2D)             (None, 16, 16, 12)   5184        activation_863[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_855 (Dropout)           (None, 16, 16, 12)   0           conv2d_864[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_415 (Concatenate)   (None, 16, 16, 204)  0           concatenate_414[0][0]            \n",
      "                                                                 dropout_855[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_864 (BatchN (None, 16, 16, 204)  816         concatenate_415[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_864 (Activation)     (None, 16, 16, 204)  0           batch_normalization_864[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_865 (Conv2D)             (None, 16, 16, 48)   9792        activation_864[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_856 (Dropout)           (None, 16, 16, 48)   0           conv2d_865[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_865 (BatchN (None, 16, 16, 48)   192         dropout_856[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_865 (Activation)     (None, 16, 16, 48)   0           batch_normalization_865[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_866 (Conv2D)             (None, 16, 16, 12)   5184        activation_865[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_857 (Dropout)           (None, 16, 16, 12)   0           conv2d_866[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_416 (Concatenate)   (None, 16, 16, 216)  0           concatenate_415[0][0]            \n",
      "                                                                 dropout_857[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_866 (BatchN (None, 16, 16, 216)  864         concatenate_416[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_866 (Activation)     (None, 16, 16, 216)  0           batch_normalization_866[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_867 (Conv2D)             (None, 16, 16, 48)   10368       activation_866[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_858 (Dropout)           (None, 16, 16, 48)   0           conv2d_867[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_867 (BatchN (None, 16, 16, 48)   192         dropout_858[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_867 (Activation)     (None, 16, 16, 48)   0           batch_normalization_867[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_868 (Conv2D)             (None, 16, 16, 12)   5184        activation_867[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_859 (Dropout)           (None, 16, 16, 12)   0           conv2d_868[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_417 (Concatenate)   (None, 16, 16, 228)  0           concatenate_416[0][0]            \n",
      "                                                                 dropout_859[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_868 (BatchN (None, 16, 16, 228)  912         concatenate_417[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_868 (Activation)     (None, 16, 16, 228)  0           batch_normalization_868[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_869 (Conv2D)             (None, 16, 16, 48)   10944       activation_868[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_860 (Dropout)           (None, 16, 16, 48)   0           conv2d_869[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_869 (BatchN (None, 16, 16, 48)   192         dropout_860[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_869 (Activation)     (None, 16, 16, 48)   0           batch_normalization_869[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_870 (Conv2D)             (None, 16, 16, 12)   5184        activation_869[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_861 (Dropout)           (None, 16, 16, 12)   0           conv2d_870[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_418 (Concatenate)   (None, 16, 16, 240)  0           concatenate_417[0][0]            \n",
      "                                                                 dropout_861[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_870 (BatchN (None, 16, 16, 240)  960         concatenate_418[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_870 (Activation)     (None, 16, 16, 240)  0           batch_normalization_870[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_871 (Conv2D)             (None, 16, 16, 48)   11520       activation_870[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_862 (Dropout)           (None, 16, 16, 48)   0           conv2d_871[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_871 (BatchN (None, 16, 16, 48)   192         dropout_862[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_871 (Activation)     (None, 16, 16, 48)   0           batch_normalization_871[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_872 (Conv2D)             (None, 16, 16, 12)   5184        activation_871[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_863 (Dropout)           (None, 16, 16, 12)   0           conv2d_872[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_419 (Concatenate)   (None, 16, 16, 252)  0           concatenate_418[0][0]            \n",
      "                                                                 dropout_863[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_872 (BatchN (None, 16, 16, 252)  1008        concatenate_419[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_872 (Activation)     (None, 16, 16, 252)  0           batch_normalization_872[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_873 (Conv2D)             (None, 16, 16, 48)   12096       activation_872[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_864 (Dropout)           (None, 16, 16, 48)   0           conv2d_873[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_873 (BatchN (None, 16, 16, 48)   192         dropout_864[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_873 (Activation)     (None, 16, 16, 48)   0           batch_normalization_873[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_874 (Conv2D)             (None, 16, 16, 12)   5184        activation_873[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_865 (Dropout)           (None, 16, 16, 12)   0           conv2d_874[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_420 (Concatenate)   (None, 16, 16, 264)  0           concatenate_419[0][0]            \n",
      "                                                                 dropout_865[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_874 (BatchN (None, 16, 16, 264)  1056        concatenate_420[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_874 (Activation)     (None, 16, 16, 264)  0           batch_normalization_874[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_875 (Conv2D)             (None, 16, 16, 132)  34848       activation_874[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_866 (Dropout)           (None, 16, 16, 132)  0           conv2d_875[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_34 (AveragePo (None, 8, 8, 132)    0           dropout_866[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_875 (BatchN (None, 8, 8, 132)    528         average_pooling2d_34[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_875 (Activation)     (None, 8, 8, 132)    0           batch_normalization_875[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_876 (Conv2D)             (None, 8, 8, 48)     6336        activation_875[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_867 (Dropout)           (None, 8, 8, 48)     0           conv2d_876[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_876 (BatchN (None, 8, 8, 48)     192         dropout_867[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_876 (Activation)     (None, 8, 8, 48)     0           batch_normalization_876[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_877 (Conv2D)             (None, 8, 8, 12)     5184        activation_876[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_868 (Dropout)           (None, 8, 8, 12)     0           conv2d_877[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_421 (Concatenate)   (None, 8, 8, 144)    0           average_pooling2d_34[0][0]       \n",
      "                                                                 dropout_868[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_877 (BatchN (None, 8, 8, 144)    576         concatenate_421[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_877 (Activation)     (None, 8, 8, 144)    0           batch_normalization_877[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_878 (Conv2D)             (None, 8, 8, 48)     6912        activation_877[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_869 (Dropout)           (None, 8, 8, 48)     0           conv2d_878[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_878 (BatchN (None, 8, 8, 48)     192         dropout_869[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_878 (Activation)     (None, 8, 8, 48)     0           batch_normalization_878[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_879 (Conv2D)             (None, 8, 8, 12)     5184        activation_878[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_870 (Dropout)           (None, 8, 8, 12)     0           conv2d_879[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_422 (Concatenate)   (None, 8, 8, 156)    0           concatenate_421[0][0]            \n",
      "                                                                 dropout_870[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_879 (BatchN (None, 8, 8, 156)    624         concatenate_422[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_879 (Activation)     (None, 8, 8, 156)    0           batch_normalization_879[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_880 (Conv2D)             (None, 8, 8, 48)     7488        activation_879[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_871 (Dropout)           (None, 8, 8, 48)     0           conv2d_880[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_880 (BatchN (None, 8, 8, 48)     192         dropout_871[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_880 (Activation)     (None, 8, 8, 48)     0           batch_normalization_880[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_881 (Conv2D)             (None, 8, 8, 12)     5184        activation_880[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_872 (Dropout)           (None, 8, 8, 12)     0           conv2d_881[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_423 (Concatenate)   (None, 8, 8, 168)    0           concatenate_422[0][0]            \n",
      "                                                                 dropout_872[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_881 (BatchN (None, 8, 8, 168)    672         concatenate_423[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_881 (Activation)     (None, 8, 8, 168)    0           batch_normalization_881[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_882 (Conv2D)             (None, 8, 8, 48)     8064        activation_881[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_873 (Dropout)           (None, 8, 8, 48)     0           conv2d_882[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_882 (BatchN (None, 8, 8, 48)     192         dropout_873[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_882 (Activation)     (None, 8, 8, 48)     0           batch_normalization_882[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_883 (Conv2D)             (None, 8, 8, 12)     5184        activation_882[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_874 (Dropout)           (None, 8, 8, 12)     0           conv2d_883[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_424 (Concatenate)   (None, 8, 8, 180)    0           concatenate_423[0][0]            \n",
      "                                                                 dropout_874[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_883 (BatchN (None, 8, 8, 180)    720         concatenate_424[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_883 (Activation)     (None, 8, 8, 180)    0           batch_normalization_883[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_884 (Conv2D)             (None, 8, 8, 48)     8640        activation_883[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_875 (Dropout)           (None, 8, 8, 48)     0           conv2d_884[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_884 (BatchN (None, 8, 8, 48)     192         dropout_875[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_884 (Activation)     (None, 8, 8, 48)     0           batch_normalization_884[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_885 (Conv2D)             (None, 8, 8, 12)     5184        activation_884[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_876 (Dropout)           (None, 8, 8, 12)     0           conv2d_885[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_425 (Concatenate)   (None, 8, 8, 192)    0           concatenate_424[0][0]            \n",
      "                                                                 dropout_876[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_885 (BatchN (None, 8, 8, 192)    768         concatenate_425[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_885 (Activation)     (None, 8, 8, 192)    0           batch_normalization_885[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_886 (Conv2D)             (None, 8, 8, 48)     9216        activation_885[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_877 (Dropout)           (None, 8, 8, 48)     0           conv2d_886[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_886 (BatchN (None, 8, 8, 48)     192         dropout_877[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_886 (Activation)     (None, 8, 8, 48)     0           batch_normalization_886[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_887 (Conv2D)             (None, 8, 8, 12)     5184        activation_886[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_878 (Dropout)           (None, 8, 8, 12)     0           conv2d_887[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_426 (Concatenate)   (None, 8, 8, 204)    0           concatenate_425[0][0]            \n",
      "                                                                 dropout_878[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_887 (BatchN (None, 8, 8, 204)    816         concatenate_426[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_887 (Activation)     (None, 8, 8, 204)    0           batch_normalization_887[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_888 (Conv2D)             (None, 8, 8, 48)     9792        activation_887[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_879 (Dropout)           (None, 8, 8, 48)     0           conv2d_888[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_888 (BatchN (None, 8, 8, 48)     192         dropout_879[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_888 (Activation)     (None, 8, 8, 48)     0           batch_normalization_888[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_889 (Conv2D)             (None, 8, 8, 12)     5184        activation_888[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_880 (Dropout)           (None, 8, 8, 12)     0           conv2d_889[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_427 (Concatenate)   (None, 8, 8, 216)    0           concatenate_426[0][0]            \n",
      "                                                                 dropout_880[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_889 (BatchN (None, 8, 8, 216)    864         concatenate_427[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_889 (Activation)     (None, 8, 8, 216)    0           batch_normalization_889[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_890 (Conv2D)             (None, 8, 8, 48)     10368       activation_889[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_881 (Dropout)           (None, 8, 8, 48)     0           conv2d_890[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_890 (BatchN (None, 8, 8, 48)     192         dropout_881[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_890 (Activation)     (None, 8, 8, 48)     0           batch_normalization_890[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_891 (Conv2D)             (None, 8, 8, 12)     5184        activation_890[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_882 (Dropout)           (None, 8, 8, 12)     0           conv2d_891[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_428 (Concatenate)   (None, 8, 8, 228)    0           concatenate_427[0][0]            \n",
      "                                                                 dropout_882[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_891 (BatchN (None, 8, 8, 228)    912         concatenate_428[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_891 (Activation)     (None, 8, 8, 228)    0           batch_normalization_891[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_892 (Conv2D)             (None, 8, 8, 48)     10944       activation_891[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_883 (Dropout)           (None, 8, 8, 48)     0           conv2d_892[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_892 (BatchN (None, 8, 8, 48)     192         dropout_883[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_892 (Activation)     (None, 8, 8, 48)     0           batch_normalization_892[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_893 (Conv2D)             (None, 8, 8, 12)     5184        activation_892[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_884 (Dropout)           (None, 8, 8, 12)     0           conv2d_893[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_429 (Concatenate)   (None, 8, 8, 240)    0           concatenate_428[0][0]            \n",
      "                                                                 dropout_884[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_893 (BatchN (None, 8, 8, 240)    960         concatenate_429[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_893 (Activation)     (None, 8, 8, 240)    0           batch_normalization_893[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_894 (Conv2D)             (None, 8, 8, 48)     11520       activation_893[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_885 (Dropout)           (None, 8, 8, 48)     0           conv2d_894[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_894 (BatchN (None, 8, 8, 48)     192         dropout_885[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_894 (Activation)     (None, 8, 8, 48)     0           batch_normalization_894[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_895 (Conv2D)             (None, 8, 8, 12)     5184        activation_894[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_886 (Dropout)           (None, 8, 8, 12)     0           conv2d_895[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_430 (Concatenate)   (None, 8, 8, 252)    0           concatenate_429[0][0]            \n",
      "                                                                 dropout_886[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_895 (BatchN (None, 8, 8, 252)    1008        concatenate_430[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_895 (Activation)     (None, 8, 8, 252)    0           batch_normalization_895[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_896 (Conv2D)             (None, 8, 8, 48)     12096       activation_895[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_887 (Dropout)           (None, 8, 8, 48)     0           conv2d_896[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_896 (BatchN (None, 8, 8, 48)     192         dropout_887[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_896 (Activation)     (None, 8, 8, 48)     0           batch_normalization_896[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_897 (Conv2D)             (None, 8, 8, 12)     5184        activation_896[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_888 (Dropout)           (None, 8, 8, 12)     0           conv2d_897[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_431 (Concatenate)   (None, 8, 8, 264)    0           concatenate_430[0][0]            \n",
      "                                                                 dropout_888[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_897 (BatchN (None, 8, 8, 264)    1056        concatenate_431[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_897 (Activation)     (None, 8, 8, 264)    0           batch_normalization_897[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_898 (Conv2D)             (None, 8, 8, 48)     12672       activation_897[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_889 (Dropout)           (None, 8, 8, 48)     0           conv2d_898[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_898 (BatchN (None, 8, 8, 48)     192         dropout_889[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_898 (Activation)     (None, 8, 8, 48)     0           batch_normalization_898[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_899 (Conv2D)             (None, 8, 8, 12)     5184        activation_898[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_890 (Dropout)           (None, 8, 8, 12)     0           conv2d_899[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_432 (Concatenate)   (None, 8, 8, 276)    0           concatenate_431[0][0]            \n",
      "                                                                 dropout_890[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_899 (BatchN (None, 8, 8, 276)    1104        concatenate_432[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_899 (Activation)     (None, 8, 8, 276)    0           batch_normalization_899[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_900 (Conv2D)             (None, 8, 8, 48)     13248       activation_899[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_891 (Dropout)           (None, 8, 8, 48)     0           conv2d_900[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_900 (BatchN (None, 8, 8, 48)     192         dropout_891[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_900 (Activation)     (None, 8, 8, 48)     0           batch_normalization_900[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_901 (Conv2D)             (None, 8, 8, 12)     5184        activation_900[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_892 (Dropout)           (None, 8, 8, 12)     0           conv2d_901[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_433 (Concatenate)   (None, 8, 8, 288)    0           concatenate_432[0][0]            \n",
      "                                                                 dropout_892[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_901 (BatchN (None, 8, 8, 288)    1152        concatenate_433[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_901 (Activation)     (None, 8, 8, 288)    0           batch_normalization_901[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_902 (Conv2D)             (None, 8, 8, 48)     13824       activation_901[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_893 (Dropout)           (None, 8, 8, 48)     0           conv2d_902[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_902 (BatchN (None, 8, 8, 48)     192         dropout_893[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_902 (Activation)     (None, 8, 8, 48)     0           batch_normalization_902[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_903 (Conv2D)             (None, 8, 8, 12)     5184        activation_902[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_894 (Dropout)           (None, 8, 8, 12)     0           conv2d_903[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_434 (Concatenate)   (None, 8, 8, 300)    0           concatenate_433[0][0]            \n",
      "                                                                 dropout_894[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_903 (BatchN (None, 8, 8, 300)    1200        concatenate_434[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_903 (Activation)     (None, 8, 8, 300)    0           batch_normalization_903[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_904 (Conv2D)             (None, 8, 8, 150)    45000       activation_903[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_895 (Dropout)           (None, 8, 8, 150)    0           conv2d_904[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_35 (AveragePo (None, 4, 4, 150)    0           dropout_895[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_904 (BatchN (None, 4, 4, 150)    600         average_pooling2d_35[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_904 (Activation)     (None, 4, 4, 150)    0           batch_normalization_904[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_905 (Conv2D)             (None, 4, 4, 48)     7200        activation_904[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_896 (Dropout)           (None, 4, 4, 48)     0           conv2d_905[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_905 (BatchN (None, 4, 4, 48)     192         dropout_896[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_905 (Activation)     (None, 4, 4, 48)     0           batch_normalization_905[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_906 (Conv2D)             (None, 4, 4, 12)     5184        activation_905[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_897 (Dropout)           (None, 4, 4, 12)     0           conv2d_906[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_435 (Concatenate)   (None, 4, 4, 162)    0           average_pooling2d_35[0][0]       \n",
      "                                                                 dropout_897[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_906 (BatchN (None, 4, 4, 162)    648         concatenate_435[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_906 (Activation)     (None, 4, 4, 162)    0           batch_normalization_906[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_907 (Conv2D)             (None, 4, 4, 48)     7776        activation_906[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_898 (Dropout)           (None, 4, 4, 48)     0           conv2d_907[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_907 (BatchN (None, 4, 4, 48)     192         dropout_898[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_907 (Activation)     (None, 4, 4, 48)     0           batch_normalization_907[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_908 (Conv2D)             (None, 4, 4, 12)     5184        activation_907[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_899 (Dropout)           (None, 4, 4, 12)     0           conv2d_908[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_436 (Concatenate)   (None, 4, 4, 174)    0           concatenate_435[0][0]            \n",
      "                                                                 dropout_899[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_908 (BatchN (None, 4, 4, 174)    696         concatenate_436[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_908 (Activation)     (None, 4, 4, 174)    0           batch_normalization_908[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_909 (Conv2D)             (None, 4, 4, 48)     8352        activation_908[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_900 (Dropout)           (None, 4, 4, 48)     0           conv2d_909[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_909 (BatchN (None, 4, 4, 48)     192         dropout_900[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_909 (Activation)     (None, 4, 4, 48)     0           batch_normalization_909[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_910 (Conv2D)             (None, 4, 4, 12)     5184        activation_909[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_901 (Dropout)           (None, 4, 4, 12)     0           conv2d_910[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_437 (Concatenate)   (None, 4, 4, 186)    0           concatenate_436[0][0]            \n",
      "                                                                 dropout_901[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_910 (BatchN (None, 4, 4, 186)    744         concatenate_437[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_910 (Activation)     (None, 4, 4, 186)    0           batch_normalization_910[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_911 (Conv2D)             (None, 4, 4, 48)     8928        activation_910[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_902 (Dropout)           (None, 4, 4, 48)     0           conv2d_911[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_911 (BatchN (None, 4, 4, 48)     192         dropout_902[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_911 (Activation)     (None, 4, 4, 48)     0           batch_normalization_911[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_912 (Conv2D)             (None, 4, 4, 12)     5184        activation_911[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_903 (Dropout)           (None, 4, 4, 12)     0           conv2d_912[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_438 (Concatenate)   (None, 4, 4, 198)    0           concatenate_437[0][0]            \n",
      "                                                                 dropout_903[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_912 (BatchN (None, 4, 4, 198)    792         concatenate_438[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_912 (Activation)     (None, 4, 4, 198)    0           batch_normalization_912[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_913 (Conv2D)             (None, 4, 4, 48)     9504        activation_912[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_904 (Dropout)           (None, 4, 4, 48)     0           conv2d_913[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_913 (BatchN (None, 4, 4, 48)     192         dropout_904[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_913 (Activation)     (None, 4, 4, 48)     0           batch_normalization_913[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_914 (Conv2D)             (None, 4, 4, 12)     5184        activation_913[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_905 (Dropout)           (None, 4, 4, 12)     0           conv2d_914[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_439 (Concatenate)   (None, 4, 4, 210)    0           concatenate_438[0][0]            \n",
      "                                                                 dropout_905[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_914 (BatchN (None, 4, 4, 210)    840         concatenate_439[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_914 (Activation)     (None, 4, 4, 210)    0           batch_normalization_914[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_915 (Conv2D)             (None, 4, 4, 48)     10080       activation_914[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_906 (Dropout)           (None, 4, 4, 48)     0           conv2d_915[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_915 (BatchN (None, 4, 4, 48)     192         dropout_906[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_915 (Activation)     (None, 4, 4, 48)     0           batch_normalization_915[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_916 (Conv2D)             (None, 4, 4, 12)     5184        activation_915[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_907 (Dropout)           (None, 4, 4, 12)     0           conv2d_916[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_440 (Concatenate)   (None, 4, 4, 222)    0           concatenate_439[0][0]            \n",
      "                                                                 dropout_907[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_916 (BatchN (None, 4, 4, 222)    888         concatenate_440[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_916 (Activation)     (None, 4, 4, 222)    0           batch_normalization_916[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_917 (Conv2D)             (None, 4, 4, 48)     10656       activation_916[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_908 (Dropout)           (None, 4, 4, 48)     0           conv2d_917[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_917 (BatchN (None, 4, 4, 48)     192         dropout_908[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_917 (Activation)     (None, 4, 4, 48)     0           batch_normalization_917[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_918 (Conv2D)             (None, 4, 4, 12)     5184        activation_917[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_909 (Dropout)           (None, 4, 4, 12)     0           conv2d_918[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_441 (Concatenate)   (None, 4, 4, 234)    0           concatenate_440[0][0]            \n",
      "                                                                 dropout_909[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_918 (BatchN (None, 4, 4, 234)    936         concatenate_441[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_918 (Activation)     (None, 4, 4, 234)    0           batch_normalization_918[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_919 (Conv2D)             (None, 4, 4, 48)     11232       activation_918[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_910 (Dropout)           (None, 4, 4, 48)     0           conv2d_919[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_919 (BatchN (None, 4, 4, 48)     192         dropout_910[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_919 (Activation)     (None, 4, 4, 48)     0           batch_normalization_919[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_920 (Conv2D)             (None, 4, 4, 12)     5184        activation_919[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_911 (Dropout)           (None, 4, 4, 12)     0           conv2d_920[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_442 (Concatenate)   (None, 4, 4, 246)    0           concatenate_441[0][0]            \n",
      "                                                                 dropout_911[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_920 (BatchN (None, 4, 4, 246)    984         concatenate_442[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_920 (Activation)     (None, 4, 4, 246)    0           batch_normalization_920[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_921 (Conv2D)             (None, 4, 4, 48)     11808       activation_920[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_912 (Dropout)           (None, 4, 4, 48)     0           conv2d_921[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_921 (BatchN (None, 4, 4, 48)     192         dropout_912[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_921 (Activation)     (None, 4, 4, 48)     0           batch_normalization_921[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_922 (Conv2D)             (None, 4, 4, 12)     5184        activation_921[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_913 (Dropout)           (None, 4, 4, 12)     0           conv2d_922[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_443 (Concatenate)   (None, 4, 4, 258)    0           concatenate_442[0][0]            \n",
      "                                                                 dropout_913[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_922 (BatchN (None, 4, 4, 258)    1032        concatenate_443[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_922 (Activation)     (None, 4, 4, 258)    0           batch_normalization_922[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_923 (Conv2D)             (None, 4, 4, 48)     12384       activation_922[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_914 (Dropout)           (None, 4, 4, 48)     0           conv2d_923[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_923 (BatchN (None, 4, 4, 48)     192         dropout_914[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_923 (Activation)     (None, 4, 4, 48)     0           batch_normalization_923[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_924 (Conv2D)             (None, 4, 4, 12)     5184        activation_923[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_915 (Dropout)           (None, 4, 4, 12)     0           conv2d_924[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_444 (Concatenate)   (None, 4, 4, 270)    0           concatenate_443[0][0]            \n",
      "                                                                 dropout_915[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_924 (BatchN (None, 4, 4, 270)    1080        concatenate_444[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_924 (Activation)     (None, 4, 4, 270)    0           batch_normalization_924[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_925 (Conv2D)             (None, 4, 4, 48)     12960       activation_924[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_916 (Dropout)           (None, 4, 4, 48)     0           conv2d_925[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_925 (BatchN (None, 4, 4, 48)     192         dropout_916[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_925 (Activation)     (None, 4, 4, 48)     0           batch_normalization_925[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_926 (Conv2D)             (None, 4, 4, 12)     5184        activation_925[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_917 (Dropout)           (None, 4, 4, 12)     0           conv2d_926[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_445 (Concatenate)   (None, 4, 4, 282)    0           concatenate_444[0][0]            \n",
      "                                                                 dropout_917[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_926 (BatchN (None, 4, 4, 282)    1128        concatenate_445[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_926 (Activation)     (None, 4, 4, 282)    0           batch_normalization_926[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_927 (Conv2D)             (None, 4, 4, 48)     13536       activation_926[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_918 (Dropout)           (None, 4, 4, 48)     0           conv2d_927[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_927 (BatchN (None, 4, 4, 48)     192         dropout_918[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_927 (Activation)     (None, 4, 4, 48)     0           batch_normalization_927[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_928 (Conv2D)             (None, 4, 4, 12)     5184        activation_927[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_919 (Dropout)           (None, 4, 4, 12)     0           conv2d_928[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_446 (Concatenate)   (None, 4, 4, 294)    0           concatenate_445[0][0]            \n",
      "                                                                 dropout_919[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_928 (BatchN (None, 4, 4, 294)    1176        concatenate_446[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_928 (Activation)     (None, 4, 4, 294)    0           batch_normalization_928[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_929 (Conv2D)             (None, 4, 4, 48)     14112       activation_928[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_920 (Dropout)           (None, 4, 4, 48)     0           conv2d_929[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_929 (BatchN (None, 4, 4, 48)     192         dropout_920[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_929 (Activation)     (None, 4, 4, 48)     0           batch_normalization_929[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_930 (Conv2D)             (None, 4, 4, 12)     5184        activation_929[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_921 (Dropout)           (None, 4, 4, 12)     0           conv2d_930[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_447 (Concatenate)   (None, 4, 4, 306)    0           concatenate_446[0][0]            \n",
      "                                                                 dropout_921[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_930 (BatchN (None, 4, 4, 306)    1224        concatenate_447[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_930 (Activation)     (None, 4, 4, 306)    0           batch_normalization_930[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_931 (Conv2D)             (None, 4, 4, 48)     14688       activation_930[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_922 (Dropout)           (None, 4, 4, 48)     0           conv2d_931[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_931 (BatchN (None, 4, 4, 48)     192         dropout_922[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_931 (Activation)     (None, 4, 4, 48)     0           batch_normalization_931[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_932 (Conv2D)             (None, 4, 4, 12)     5184        activation_931[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_923 (Dropout)           (None, 4, 4, 12)     0           conv2d_932[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_448 (Concatenate)   (None, 4, 4, 318)    0           concatenate_447[0][0]            \n",
      "                                                                 dropout_923[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_932 (BatchN (None, 4, 4, 318)    1272        concatenate_448[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_932 (Activation)     (None, 4, 4, 318)    0           batch_normalization_932[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_36 (AveragePo (None, 2, 2, 318)    0           activation_932[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1272)         0           average_pooling2d_36[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 10)           12730       flatten_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 936,802\n",
      "Trainable params: 909,286\n",
      "Non-trainable params: 27,516\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f68LtcYHG6i4"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 90\n",
    "decay = learning_rate/epochs\n",
    "decay = 0.0001\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=learning_rate, decay=decay, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Oj8RSwyG6i9"
   },
   "outputs": [],
   "source": [
    "#batch_size = 64\n",
    "#clr_triangular = CyclicLR(mode='triangular', base_lr = 0.1, max_lr = 0.2, step_size = (len(x_train)* 2 * 4)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HT7ZwHzG6jG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crhGk7kEhXAz"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcWydmIVhZGr"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 30.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7dl5K84G6jl"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay1(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epoch_drop_01 = 40\n",
    "    epoch_drop_02 = epoch_drop_01 + 40\n",
    "    epoch_drop_03 = epoch_drop_02 + 40\n",
    "    \n",
    "    if (epoch < epoch_drop_01):\n",
    "        lrate = initial_lrate\n",
    "    elif (epoch < epoch_drop_02):\n",
    "        lrate = initial_lrate * drop\n",
    "    else:\n",
    "        lrate = initial_lrate * drop * drop\n",
    "\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDig-b71G6jq"
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(self, epoch, logs=None):\n",
    "    print(\"epoch: \", epoch,\"learning rate for\", K.eval(self.model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zjHHVfPG6jw"
   },
   "outputs": [],
   "source": [
    "lrate = LearningRateScheduler(step_decay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aS6q4X1G6j0"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.001)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience= 4, min_delta=0.003, verbose=1, cooldown=0, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJgRsh_2G6j7"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZZuotjhG6kA"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#filepath = file_prefix + r\".best.hdf5\"\n",
    "filepath = \"DNST_CIFAR10_Conv_09_10-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE3lF6EH1r_L"
   },
   "outputs": [],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "#model.save_weights(\"densenet_tr_03-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai-yZ2ED5AK1"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "\n",
    "#files.download('DNST_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir_fg-p9G6kO"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6HtNUyQG6kS"
   },
   "outputs": [],
   "source": [
    "class AdamTracker_0(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"start , epoch = \", epoch,\", lr = \", K.eval(optimizer.lr),\", decay = \",K.eval(optimizer.decay),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BtYHxTCG6kV"
   },
   "outputs": [],
   "source": [
    "class AdamTracker_1(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"end, epoch = \", epoch,\", lr = \", K.eval(optimizer.lr),\", decay = \",K.eval(optimizer.decay),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMe9lOY9G6kb"
   },
   "outputs": [],
   "source": [
    "adam_lr_tracker_1 = AdamTracker_1()\n",
    "adam_lr_tracker_0 = AdamTracker_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2iue0UsLDzb"
   },
   "outputs": [],
   "source": [
    "class SGDLearningRateTracker(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        #lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        print(\"epoch = \", epoch,\", lr = \", K.eval(optimizer.lr), \", momentum = \",K.eval(optimizer.momentum),\n",
    "              \", decay = \",K.eval(optimizer.decay), \", Nestrov = \",optimizer.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-itoaFDG6kf"
   },
   "outputs": [],
   "source": [
    "sgd_lr_tracker = SGDLearningRateTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Og56VCRh5j8V"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint, reduce_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ki7pVU60G6ko"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [adam_lr_tracker_0, adam_lr_tracker_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBTjYaJ4G6kv"
   },
   "outputs": [],
   "source": [
    "#callbacks_list = [checkpoint, adam_lr_tracker_0, adam_lr_tracker_1, clr_triangular]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhuPjscpK0mm"
   },
   "outputs": [],
   "source": [
    "#callbacks_list = [checkpoint, sgd_lr_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bTKPS9HG6kx"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model('densenet_tr_03-46-0.71.hdf5')\n",
    "#score = model.evaluate(x_test, y_test, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OfdusR7aG6k1",
    "outputId": "f23665c5-6112-4dcc-d452-fae080e5ab70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-LiiCB9G6k8"
   },
   "source": [
    "## Call the model with the datagen, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WKzwh45G6k8"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZjsbZONG6lJ"
   },
   "outputs": [],
   "source": [
    "load_model_from_back = False\n",
    "\n",
    "if load_model_from_back:\n",
    "    model = load_model('DNST_CIFAR10_Conv_04.hdf5')\n",
    "    score = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4114
    },
    "colab_type": "code",
    "id": "ODPSQd8dG6lM",
    "outputId": "0b061b74-5653-4c53-932e-04eb30c3664e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 255s 325ms/step - loss: 1.5895 - acc: 0.4175 - val_loss: 1.8471 - val_acc: 0.4456\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/80\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 153s 196ms/step - loss: 1.1656 - acc: 0.5818 - val_loss: 2.1541 - val_acc: 0.4715\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/80\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.9620 - acc: 0.6581 - val_loss: 1.8339 - val_acc: 0.5589\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/80\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.8401 - acc: 0.7039 - val_loss: 0.9109 - val_acc: 0.7188\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/80\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.7599 - acc: 0.7332 - val_loss: 1.3607 - val_acc: 0.6396\n",
      "end, epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "Epoch 6/80\n",
      "start , epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 196ms/step - loss: 0.6966 - acc: 0.7557 - val_loss: 1.0374 - val_acc: 0.7191\n",
      "end, epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "Epoch 7/80\n",
      "start , epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.6461 - acc: 0.7740 - val_loss: 0.8341 - val_acc: 0.7556\n",
      "end, epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "Epoch 8/80\n",
      "start , epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 198ms/step - loss: 0.6104 - acc: 0.7859 - val_loss: 1.3093 - val_acc: 0.6838\n",
      "end, epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "Epoch 9/80\n",
      "start , epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 199ms/step - loss: 0.5799 - acc: 0.7977 - val_loss: 0.7490 - val_acc: 0.7749\n",
      "end, epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "Epoch 10/80\n",
      "start , epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.5484 - acc: 0.8092 - val_loss: 0.6561 - val_acc: 0.8022\n",
      "end, epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "Epoch 11/80\n",
      "start , epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.5194 - acc: 0.8197 - val_loss: 0.6436 - val_acc: 0.8031\n",
      "end, epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "Epoch 12/80\n",
      "start , epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.4985 - acc: 0.8267 - val_loss: 0.6051 - val_acc: 0.8180\n",
      "end, epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "Epoch 13/80\n",
      "start , epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.4821 - acc: 0.8310 - val_loss: 0.6776 - val_acc: 0.8026\n",
      "end, epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "Epoch 14/80\n",
      "start , epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 196ms/step - loss: 0.4600 - acc: 0.8398 - val_loss: 0.6333 - val_acc: 0.8177\n",
      "end, epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "Epoch 15/80\n",
      "start , epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.4466 - acc: 0.8452 - val_loss: 0.5805 - val_acc: 0.8297\n",
      "end, epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "Epoch 16/80\n",
      "start , epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 198ms/step - loss: 0.4293 - acc: 0.8508 - val_loss: 0.6629 - val_acc: 0.8244\n",
      "end, epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "Epoch 17/80\n",
      "start , epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 199ms/step - loss: 0.4176 - acc: 0.8562 - val_loss: 0.5257 - val_acc: 0.8464\n",
      "end, epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "Epoch 18/80\n",
      "start , epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.4081 - acc: 0.8585 - val_loss: 0.5173 - val_acc: 0.8464\n",
      "end, epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "Epoch 19/80\n",
      "start , epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 156s 199ms/step - loss: 0.3953 - acc: 0.8623 - val_loss: 0.5576 - val_acc: 0.8391\n",
      "end, epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "Epoch 20/80\n",
      "start , epoch =  19 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.3862 - acc: 0.8656 - val_loss: 0.4915 - val_acc: 0.8549\n",
      "end, epoch =  19 , lr =  0.001 , decay =  0.0\n",
      "Epoch 21/80\n",
      "start , epoch =  20 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.3711 - acc: 0.8702 - val_loss: 0.5241 - val_acc: 0.8470\n",
      "end, epoch =  20 , lr =  0.001 , decay =  0.0\n",
      "Epoch 22/80\n",
      "start , epoch =  21 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.3658 - acc: 0.8718 - val_loss: 0.5989 - val_acc: 0.8325\n",
      "end, epoch =  21 , lr =  0.001 , decay =  0.0\n",
      "Epoch 23/80\n",
      "start , epoch =  22 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.3546 - acc: 0.8772 - val_loss: 0.4977 - val_acc: 0.8603\n",
      "end, epoch =  22 , lr =  0.001 , decay =  0.0\n",
      "Epoch 24/80\n",
      "start , epoch =  23 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.3492 - acc: 0.8784 - val_loss: 0.4716 - val_acc: 0.8664\n",
      "end, epoch =  23 , lr =  0.001 , decay =  0.0\n",
      "Epoch 25/80\n",
      "start , epoch =  24 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.3415 - acc: 0.8821 - val_loss: 0.5095 - val_acc: 0.8541\n",
      "end, epoch =  24 , lr =  0.001 , decay =  0.0\n",
      "Epoch 26/80\n",
      "start , epoch =  25 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 159s 203ms/step - loss: 0.3342 - acc: 0.8826 - val_loss: 0.5515 - val_acc: 0.8503\n",
      "end, epoch =  25 , lr =  0.001 , decay =  0.0\n",
      "Epoch 27/80\n",
      "start , epoch =  26 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 159s 204ms/step - loss: 0.3277 - acc: 0.8855 - val_loss: 0.3904 - val_acc: 0.8826\n",
      "end, epoch =  26 , lr =  0.001 , decay =  0.0\n",
      "Epoch 28/80\n",
      "start , epoch =  27 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.3199 - acc: 0.8867 - val_loss: 0.4481 - val_acc: 0.8664\n",
      "end, epoch =  27 , lr =  0.001 , decay =  0.0\n",
      "Epoch 29/80\n",
      "start , epoch =  28 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.3138 - acc: 0.8898 - val_loss: 0.4807 - val_acc: 0.8554\n",
      "end, epoch =  28 , lr =  0.001 , decay =  0.0\n",
      "Epoch 30/80\n",
      "start , epoch =  29 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.3104 - acc: 0.8896 - val_loss: 0.4951 - val_acc: 0.8611\n",
      "end, epoch =  29 , lr =  0.001 , decay =  0.0\n",
      "Epoch 31/80\n",
      "start , epoch =  30 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.3031 - acc: 0.8939 - val_loss: 0.4199 - val_acc: 0.8774\n",
      "end, epoch =  30 , lr =  0.001 , decay =  0.0\n",
      "Epoch 32/80\n",
      "start , epoch =  31 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.3001 - acc: 0.8962 - val_loss: 0.4523 - val_acc: 0.8730\n",
      "end, epoch =  31 , lr =  0.001 , decay =  0.0\n",
      "Epoch 33/80\n",
      "start , epoch =  32 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.2934 - acc: 0.8977 - val_loss: 0.4250 - val_acc: 0.8795\n",
      "end, epoch =  32 , lr =  0.001 , decay =  0.0\n",
      "Epoch 34/80\n",
      "start , epoch =  33 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 196ms/step - loss: 0.2862 - acc: 0.8998 - val_loss: 0.4398 - val_acc: 0.8795\n",
      "end, epoch =  33 , lr =  0.001 , decay =  0.0\n",
      "Epoch 35/80\n",
      "start , epoch =  34 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.2834 - acc: 0.9025 - val_loss: 0.5076 - val_acc: 0.8609\n",
      "end, epoch =  34 , lr =  0.001 , decay =  0.0\n",
      "Epoch 36/80\n",
      "start , epoch =  35 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 153s 196ms/step - loss: 0.2778 - acc: 0.9025 - val_loss: 0.4479 - val_acc: 0.8793\n",
      "end, epoch =  35 , lr =  0.001 , decay =  0.0\n",
      "Epoch 37/80\n",
      "start , epoch =  36 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.2750 - acc: 0.9049 - val_loss: 0.4807 - val_acc: 0.8707\n",
      "end, epoch =  36 , lr =  0.001 , decay =  0.0\n",
      "Epoch 38/80\n",
      "start , epoch =  37 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.2709 - acc: 0.9046 - val_loss: 0.3826 - val_acc: 0.8887\n",
      "end, epoch =  37 , lr =  0.001 , decay =  0.0\n",
      "Epoch 39/80\n",
      "start , epoch =  38 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 199ms/step - loss: 0.2679 - acc: 0.9043 - val_loss: 0.4020 - val_acc: 0.8841\n",
      "end, epoch =  38 , lr =  0.001 , decay =  0.0\n",
      "Epoch 40/80\n",
      "start , epoch =  39 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 196ms/step - loss: 0.2609 - acc: 0.9093 - val_loss: 0.4782 - val_acc: 0.8758\n",
      "end, epoch =  39 , lr =  0.001 , decay =  0.0\n",
      "Epoch 41/80\n",
      "start , epoch =  40 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.2575 - acc: 0.9085 - val_loss: 0.4289 - val_acc: 0.8837\n",
      "end, epoch =  40 , lr =  0.001 , decay =  0.0\n",
      "Epoch 42/80\n",
      "start , epoch =  41 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.2543 - acc: 0.9102 - val_loss: 0.3844 - val_acc: 0.8882\n",
      "end, epoch =  41 , lr =  0.001 , decay =  0.0\n",
      "Epoch 43/80\n",
      "start , epoch =  42 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 156s 199ms/step - loss: 0.2469 - acc: 0.9136 - val_loss: 0.4497 - val_acc: 0.8850\n",
      "end, epoch =  42 , lr =  0.001 , decay =  0.0\n",
      "Epoch 44/80\n",
      "start , epoch =  43 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.2477 - acc: 0.9128 - val_loss: 0.4100 - val_acc: 0.8906\n",
      "end, epoch =  43 , lr =  0.001 , decay =  0.0\n",
      "Epoch 45/80\n",
      "start , epoch =  44 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.2458 - acc: 0.9131 - val_loss: 0.3904 - val_acc: 0.8888\n",
      "end, epoch =  44 , lr =  0.001 , decay =  0.0\n",
      "Epoch 46/80\n",
      "start , epoch =  45 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.2413 - acc: 0.9144 - val_loss: 0.4636 - val_acc: 0.8798\n",
      "end, epoch =  45 , lr =  0.001 , decay =  0.0\n",
      "Epoch 47/80\n",
      "start , epoch =  46 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.2384 - acc: 0.9159 - val_loss: 0.3637 - val_acc: 0.8924\n",
      "end, epoch =  46 , lr =  0.001 , decay =  0.0\n",
      "Epoch 48/80\n",
      "start , epoch =  47 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 154s 197ms/step - loss: 0.2340 - acc: 0.9180 - val_loss: 0.3662 - val_acc: 0.8965\n",
      "end, epoch =  47 , lr =  0.001 , decay =  0.0\n",
      "Epoch 49/80\n",
      "start , epoch =  48 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 155s 198ms/step - loss: 0.2263 - acc: 0.9200 - val_loss: 0.4307 - val_acc: 0.8849\n",
      "end, epoch =  48 , lr =  0.001 , decay =  0.0\n",
      "Epoch 50/80\n",
      "start , epoch =  49 , lr =  0.001 , decay =  0.0\n",
      "173/781 [=====>........................] - ETA: 1:50 - loss: 0.2325 - acc: 0.9191"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-095345c19fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 1.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### above cell stopped early to try batch_size increace( inspired by the paper) \n",
    "\n",
    "  - paper \"DON’T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE\" (https://openreview.net/pdf?id=B1Yy1BxCZ)\n",
    "  - increase batch size (effectively reduces lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 173s 221ms/step - loss: 0.1853 - acc: 0.9348 - val_loss: 0.3797 - val_acc: 0.8985\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/20\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 172s 220ms/step - loss: 0.1813 - acc: 0.9359 - val_loss: 0.3987 - val_acc: 0.8923\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/20\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 170s 218ms/step - loss: 0.1791 - acc: 0.9368 - val_loss: 0.3801 - val_acc: 0.8968\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/20\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "782/781 [==============================] - 172s 220ms/step - loss: 0.1778 - acc: 0.9370 - val_loss: 0.3772 - val_acc: 0.9042\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/20\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "565/781 [====================>.........] - ETA: 46s - loss: 0.1745 - acc: 0.9377"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-2e9bdd915c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     model.fit(x_train, y_train,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 2.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### above cell stopped early to try batch_size increace( inspired by the paper) \n",
    "\n",
    "  - paper \"DON’T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE\" (https://openreview.net/pdf?id=B1Yy1BxCZ)\n",
    "  - max batch_size supported is 192 in GPU\n",
    "  - increase batch size (effectively reduces lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 143s 271ms/step - loss: 0.1504 - acc: 0.9469 - val_loss: 0.3445 - val_acc: 0.9101\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/20\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1496 - acc: 0.9468 - val_loss: 0.3525 - val_acc: 0.9115\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/20\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1490 - acc: 0.9466 - val_loss: 0.3939 - val_acc: 0.9006\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/20\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1492 - acc: 0.9472 - val_loss: 0.4071 - val_acc: 0.8994\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/20\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1456 - acc: 0.9487 - val_loss: 0.4198 - val_acc: 0.8955\n",
      "end, epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "Epoch 6/20\n",
      "start , epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1407 - acc: 0.9494 - val_loss: 0.3613 - val_acc: 0.9086\n",
      "end, epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "Epoch 7/20\n",
      "start , epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1390 - acc: 0.9502 - val_loss: 0.3485 - val_acc: 0.9166\n",
      "end, epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "Epoch 8/20\n",
      "start , epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1392 - acc: 0.9505 - val_loss: 0.3462 - val_acc: 0.9141\n",
      "end, epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "Epoch 9/20\n",
      "start , epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1361 - acc: 0.9508 - val_loss: 0.3875 - val_acc: 0.9071\n",
      "end, epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "Epoch 10/20\n",
      "start , epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1374 - acc: 0.9511 - val_loss: 0.4703 - val_acc: 0.8941\n",
      "end, epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "Epoch 11/20\n",
      "start , epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1319 - acc: 0.9525 - val_loss: 0.3517 - val_acc: 0.9126\n",
      "end, epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "Epoch 12/20\n",
      "start , epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1297 - acc: 0.9537 - val_loss: 0.3785 - val_acc: 0.9129\n",
      "end, epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "Epoch 13/20\n",
      "start , epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1293 - acc: 0.9546 - val_loss: 0.4505 - val_acc: 0.8950\n",
      "end, epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "Epoch 14/20\n",
      "start , epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1274 - acc: 0.9547 - val_loss: 0.3712 - val_acc: 0.9117\n",
      "end, epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "Epoch 15/20\n",
      "start , epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1252 - acc: 0.9550 - val_loss: 0.3930 - val_acc: 0.9081\n",
      "end, epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "Epoch 16/20\n",
      "start , epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1219 - acc: 0.9562 - val_loss: 0.4269 - val_acc: 0.9027\n",
      "end, epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "Epoch 17/20\n",
      "start , epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1220 - acc: 0.9560 - val_loss: 0.4175 - val_acc: 0.9059\n",
      "end, epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "Epoch 18/20\n",
      "start , epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1222 - acc: 0.9567 - val_loss: 0.3750 - val_acc: 0.9137\n",
      "end, epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "Epoch 19/20\n",
      "start , epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1167 - acc: 0.9579 - val_loss: 0.3826 - val_acc: 0.9147\n",
      "end, epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "Epoch 20/20\n",
      "start , epoch =  19 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1176 - acc: 0.9577 - val_loss: 0.4250 - val_acc: 0.9095\n",
      "end, epoch =  19 , lr =  0.001 , decay =  0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 190\n",
    "epochs = 20\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 2.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "start , epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1133 - acc: 0.9596 - val_loss: 0.3938 - val_acc: 0.9109\n",
      "end, epoch =  0 , lr =  0.001 , decay =  0.0\n",
      "Epoch 2/20\n",
      "start , epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1155 - acc: 0.9588 - val_loss: 0.3899 - val_acc: 0.9108\n",
      "end, epoch =  1 , lr =  0.001 , decay =  0.0\n",
      "Epoch 3/20\n",
      "start , epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1110 - acc: 0.9604 - val_loss: 0.4234 - val_acc: 0.9085\n",
      "end, epoch =  2 , lr =  0.001 , decay =  0.0\n",
      "Epoch 4/20\n",
      "start , epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1108 - acc: 0.9600 - val_loss: 0.3743 - val_acc: 0.9144\n",
      "end, epoch =  3 , lr =  0.001 , decay =  0.0\n",
      "Epoch 5/20\n",
      "start , epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1089 - acc: 0.9606 - val_loss: 0.4073 - val_acc: 0.9121\n",
      "end, epoch =  4 , lr =  0.001 , decay =  0.0\n",
      "Epoch 6/20\n",
      "start , epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1062 - acc: 0.9618 - val_loss: 0.4523 - val_acc: 0.9041\n",
      "end, epoch =  5 , lr =  0.001 , decay =  0.0\n",
      "Epoch 7/20\n",
      "start , epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1083 - acc: 0.9616 - val_loss: 0.3759 - val_acc: 0.9155\n",
      "end, epoch =  6 , lr =  0.001 , decay =  0.0\n",
      "Epoch 8/20\n",
      "start , epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1044 - acc: 0.9630 - val_loss: 0.4134 - val_acc: 0.9105\n",
      "end, epoch =  7 , lr =  0.001 , decay =  0.0\n",
      "Epoch 9/20\n",
      "start , epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.1048 - acc: 0.9625 - val_loss: 0.3635 - val_acc: 0.9210\n",
      "end, epoch =  8 , lr =  0.001 , decay =  0.0\n",
      "Epoch 10/20\n",
      "start , epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.1011 - acc: 0.9638 - val_loss: 0.3794 - val_acc: 0.9177\n",
      "end, epoch =  9 , lr =  0.001 , decay =  0.0\n",
      "Epoch 11/20\n",
      "start , epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.1017 - acc: 0.9633 - val_loss: 0.4528 - val_acc: 0.9047\n",
      "end, epoch =  10 , lr =  0.001 , decay =  0.0\n",
      "Epoch 12/20\n",
      "start , epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.0999 - acc: 0.9648 - val_loss: 0.4364 - val_acc: 0.9077\n",
      "end, epoch =  11 , lr =  0.001 , decay =  0.0\n",
      "Epoch 13/20\n",
      "start , epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 270ms/step - loss: 0.0989 - acc: 0.9639 - val_loss: 0.4290 - val_acc: 0.9071\n",
      "end, epoch =  12 , lr =  0.001 , decay =  0.0\n",
      "Epoch 14/20\n",
      "start , epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.0986 - acc: 0.9648 - val_loss: 0.3866 - val_acc: 0.9185\n",
      "end, epoch =  13 , lr =  0.001 , decay =  0.0\n",
      "Epoch 15/20\n",
      "start , epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.0995 - acc: 0.9645 - val_loss: 0.4332 - val_acc: 0.9086\n",
      "end, epoch =  14 , lr =  0.001 , decay =  0.0\n",
      "Epoch 16/20\n",
      "start , epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.0959 - acc: 0.9652 - val_loss: 0.4299 - val_acc: 0.9109\n",
      "end, epoch =  15 , lr =  0.001 , decay =  0.0\n",
      "Epoch 17/20\n",
      "start , epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.0985 - acc: 0.9647 - val_loss: 0.4761 - val_acc: 0.9014\n",
      "end, epoch =  16 , lr =  0.001 , decay =  0.0\n",
      "Epoch 18/20\n",
      "start , epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 268ms/step - loss: 0.0944 - acc: 0.9663 - val_loss: 0.4014 - val_acc: 0.9184\n",
      "end, epoch =  17 , lr =  0.001 , decay =  0.0\n",
      "Epoch 19/20\n",
      "start , epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 142s 269ms/step - loss: 0.0950 - acc: 0.9662 - val_loss: 0.3941 - val_acc: 0.9163\n",
      "end, epoch =  18 , lr =  0.001 , decay =  0.0\n",
      "Epoch 20/20\n",
      "start , epoch =  19 , lr =  0.001 , decay =  0.0\n",
      "527/526 [==============================] - 141s 267ms/step - loss: 0.0896 - acc: 0.9677 - val_loss: 0.4334 - val_acc: 0.9110\n",
      "end, epoch =  19 , lr =  0.001 , decay =  0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 190\n",
    "epochs = 20\n",
    "if do_data_augmentation:\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch= (len(x_train)* 2.0)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can observe that the model has achived 92.10% accuracy (84 epochs, slightly faster than Model-1 with 982K params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DNST_CIFAR10_Conv_09.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
